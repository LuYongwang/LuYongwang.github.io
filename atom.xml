<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L丶lulu‘s Blog</title>
  
  <subtitle>敬往事一杯酒,再爱我也不回头🌹</subtitle>
  <link href="https://blog.yongwang.lu/atom.xml" rel="self"/>
  
  <link href="https://blog.yongwang.lu/"/>
  <updated>2023-01-05T08:40:00.000Z</updated>
  <id>https://blog.yongwang.lu/</id>
  
  <author>
    <name>L丶lulu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes（k8s）7、健康检查详解</title>
    <link href="https://blog.yongwang.lu/post/bb9b1e8e.html"/>
    <id>https://blog.yongwang.lu/post/bb9b1e8e.html</id>
    <published>2023-01-05T08:40:00.000Z</published>
    <updated>2023-01-05T08:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><blockquote><p>Kubernetes中的健康检查主要使用 就绪性探针（<code>readinessProbes</code>）和 存活性探针（<code>livenessProbes</code>） 来实现，service即为负载均衡，k8s保证 service 后面的 pod 都可用，是k8s中自愈能力的主要手段，主要基于这两种探测机制，可以实现如下需求：</p></blockquote><ul><li><p>异常实例自动剔除，并重启新实例</p></li><li><p>多种类型探针检测，保证异常pod不接入流量</p></li><li><p>不停机部署，更安全的滚动升级</p></li></ul><p><img src="https://res.yongwang.lu/img/20230322232410.png" alt="图片"><br>官方文档：<a href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</a>  </p><h3 id="1）k8s中的探针种类"><a href="#1）k8s中的探针种类" class="headerlink" title="1）k8s中的探针种类"></a>1）k8s中的探针种类</h3><h4 id="1、就绪检查（readinessProbe，就绪探针）"><a href="#1、就绪检查（readinessProbe，就绪探针）" class="headerlink" title="1、就绪检查（readinessProbe，就绪探针）"></a>1、就绪检查（readinessProbe，就绪探针）</h4><blockquote><p><code>readiness probes</code>准备就绪检查，通过readiness是否准备接受流量，准备完毕加入到<code>Endpoint</code>，否则剔除。如果容器不提供就绪探针，则<strong>默认状态为 Success</strong>。</p></blockquote><h4 id="2、存活检查（livenessProbe，存活探针）"><a href="#2、存活检查（livenessProbe，存活探针）" class="headerlink" title="2、存活检查（livenessProbe，存活探针）"></a>2、存活检查（livenessProbe，存活探针）</h4><blockquote><p><code>liveness probes</code>在线检查机制，检查应用是否可用，如死锁，无法响应，异常时将根据<code>restartPolicy</code>来设置 Pod 状态会自动重启容器，如果容器不提供存活探针，则<strong>默认状态为 Success</strong>。</p></blockquote><p><code>restartPolicy</code>有三个可选值：</p><ul><li><p><code>Always</code>：当容器终止退出后，总是重启容器，<strong>默认策略</strong>。</p></li><li><p><code>OnFailure</code>：当容器异常退出（退出状态码非0）时，才重启容器。</p></li><li><p><code>Never</code>：当容器终止退出，从不重启容器。</p></li></ul><h4 id="3、启动检查（startupProbe，启动探针，1-17-版本新增）"><a href="#3、启动检查（startupProbe，启动探针，1-17-版本新增）" class="headerlink" title="3、启动检查（startupProbe，启动探针，1.17 版本新增）"></a>3、启动检查（startupProbe，启动探针，1.17 版本新增）</h4><ul><li><p><code>startupProbes</code> 启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被前面的探针kill掉。</p></li><li><p>判断容器内的应用程序是否已启动，主要针对于不能确定具体启动时间的应用。如果匹配了 <code>startupProbes</code> 探测，则在 <code>startupProbes</code> 状态为 Success 之前，其他所有探针都处于无效状态，直<strong>到它成功后其他探针才起作用</strong>。</p></li><li><p>如果 <code>startupProbe</code> 失败，kubelet 将杀死容器，容器将根据 <code>restartPolicy</code> 来重启。如果容器没有配置 <code>startupProbe</code>，则<strong>默认状态为 Success</strong>。其实一般主要是设置上面两种即可。</p></li></ul><p><strong>就绪、存活两种探针的区别：</strong></p><blockquote><p><strong>readinessProbe</strong> 和 <strong>livenessProbe</strong> 可以使用相同探测方式，只是对 Pod 的处置方式不同。</p></blockquote><ul><li><p><strong>livenessProbe</strong> 当检测失败后，将杀死容器并根据 Pod 的<strong>重启策略来决定作出对应的措施</strong>。</p></li><li><p><strong>readinessProbe</strong> 当检测失败后，将 Pod 的 IP:Port 从对应的 <code>EndPoint</code> 列表中删除。</p></li></ul><h3 id="2）k8s中的三种探测方式"><a href="#2）k8s中的三种探测方式" class="headerlink" title="2）k8s中的三种探测方式"></a>2）k8s中的三种探测方式</h3><blockquote><p>每种探测机制支持三种健康检查方法，分别是命令行exec，httpGet和tcpSocket，其中exec通用性最强，适用与大部分场景，tcpSocket适用于TCP业务，httpGet适用于web业务。</p></blockquote><ul><li><p><code>exec</code>（自定义健康检查）：在容器中执行指定的命令，如果执行成功，退出码为 0 则探测成功。</p></li><li><p><code>httpGet</code>：通过容器的IP地址、端口号及路径调用 HTTP Get方法，如果响应的状态码大于等于200且小于400，则认为容器 健康。</p></li><li><p><code>tcpSocket</code>：通过容器的 IP 地址和端口号执行 TCP 检 查，如果能够建立 TCP 连接，则表明容器健康。</p></li></ul><p>探针探测结果有以下值：</p><ul><li><p><code>Success</code>：表示通过检测。</p></li><li><p><code>Failure</code>：表示未通过检测。</p></li><li><p><code>Unknown</code>：表示检测没有正常进行。</p></li></ul><h2 id="二、readinessProbe（就绪性探针）"><a href="#二、readinessProbe（就绪性探针）" class="headerlink" title="二、readinessProbe（就绪性探针）"></a>二、readinessProbe（就绪性探针）</h2><ul><li><p><strong>readiness probe</strong> 就绪性探针，用于判断容器内的程序是否存活（或者说是否健康），只有程序(服务)正常， 容器开始对外提供网络访问（启动完成并就绪）；</p></li><li><p>容器启动后按照<code>readiness probe</code>配置进行探测，无问题后结果为成功即状态为 <code>Success</code>；</p></li><li><p>pod的<code>READY</code>状态为 true，从0&#x2F;1变为1&#x2F;1。如果失败继续为0&#x2F;1，状态为 false；</p></li><li><p>若<strong>未配置就绪探针</strong>，则**默认状态容器启动后为<code>Success</code>**。对于此pod、此pod关联的<code>Service</code>资源、<code>EndPoint</code> 的关系也将基于 Pod 的 <code>Ready</code> 状态进行设置；</p></li><li><p>如果 Pod 运行过程中 <strong>Ready 状态变为 false</strong>，则系统自动<strong>从 <code>Service</code>资源 关联的 <code>EndPoint</code>列表中去除此pod</strong>，届时service资源接收到GET请求后，<code>kube-proxy</code>将一定不会把流量引入此pod中，通过这种机制就能防止将流量转发到不可用的 Pod 上。</p></li><li><p>如果 <strong>Pod 恢复为 Ready 状态</strong>。将再<strong>会被加回 <code>Endpoint</code> 列表</strong>。<code>kube-proxy</code>也将有概率通过负载机制会引入流量到此pod中。</p></li></ul><h2 id="三、livenessProbe（存活性探针）"><a href="#三、livenessProbe（存活性探针）" class="headerlink" title="三、livenessProbe（存活性探针）"></a>三、livenessProbe（存活性探针）</h2><ul><li><p><strong>liveness probe</strong>存活性探针，用于判断容器是不是健康，<strong>如果不满足健康条件</strong>，<strong>那么 Kubelet 将根据 Pod 中设置的 <code>restartPolicy</code> （重启策略）来判断，Pod 是否要进行重启操作</strong>；</p></li><li><p>LivenessProbe按照配置去探测 ( <strong>进程、或者端口、或者命令执行后是否成功等等</strong>)，来判断容器是不是正常；</p></li><li><p>如果探测不到，代表容器不健康（可以配置连续多少次失败才记为不健康），则 kubelet 会杀掉该容器，并根据容器的重启策略做相应的处理；</p></li><li><p>如果<strong>未配置存活探针</strong>，则<strong>默认容器启动为通过（Success）状态</strong>。即探针返回的值永远是 <code>Success</code>。即Success后pod状态是<code>RUNING</code>。</p></li></ul><h2 id="四、实战演示"><a href="#四、实战演示" class="headerlink" title="四、实战演示"></a>四、实战演示</h2><p>常用的探针可选参数：</p><table><thead><tr><th>参数名称</th><th>默认值</th><th>最小值</th><th>描述</th></tr></thead><tbody><tr><td>initialDelaySeconds</td><td>0秒</td><td>0秒</td><td>探测延迟时长，容器启动后多久开始进行第一次探测工作。</td></tr><tr><td>periodSeconds</td><td>10秒</td><td>1秒</td><td>探测频度，频率过高会对pod带来较大的额外开销，频率过低则无法及时反映容器产生的错误。</td></tr><tr><td>timeoutSeconds</td><td>1秒</td><td>1秒</td><td>探测的超时时长。</td></tr><tr><td>failureThreshold</td><td>3</td><td>1</td><td>处于成功状态时，探测连续失败几次可被认为失败。</td></tr><tr><td>successThreshold</td><td>1</td><td>1</td><td>处于失败状态时，探测连续成功几次，被认为成功。</td></tr></tbody></table><h3 id="1）exec方式"><a href="#1）exec方式" class="headerlink" title="1）exec方式"></a>1）exec方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;exec-liveness.yaml&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Pod</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    test: liveness</span></span><br><span class="line"><span class="string">  name: liveness-exec</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  # 为了测试方便，指定调度机器</span></span><br><span class="line"><span class="string">  nodeName: local-168-182-110</span></span><br><span class="line"><span class="string">  containers:</span></span><br><span class="line"><span class="string">  - name: liveness</span></span><br><span class="line"><span class="string">    image: registry.aliyuncs.com/google_containers/busybox</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">    - /bin/sh</span></span><br><span class="line"><span class="string">    - -c</span></span><br><span class="line"><span class="string">    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600</span></span><br><span class="line"><span class="string">    livenessProbe:</span></span><br><span class="line"><span class="string">      exec:</span></span><br><span class="line"><span class="string">        command:</span></span><br><span class="line"><span class="string">        - cat</span></span><br><span class="line"><span class="string">        - /tmp/healthy</span></span><br><span class="line"><span class="string">      initialDelaySeconds: 5</span></span><br><span class="line"><span class="string">      periodSeconds: 5</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>解释：</p><ul><li><p><code>initialDelaySeconds</code> 字段告诉 kubelet 在执行第一次探测前应该<strong>等待 5 秒</strong>。</p></li><li><p><code>periodSeconds</code> 字段指定了 kubelet 应该<strong>每 5 秒执行一次存活探测</strong>。</p></li><li><p>kubelet 在容器内执行命令 <strong>cat &#x2F;tmp&#x2F;healthy 来进行探测</strong>。</p></li><li><p>如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。</p></li><li><p>如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。</p></li><li><p>当容器启动时，执行如下的命令：</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/bin/sh -c <span class="string">&quot;touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600&quot;</span></span><br></pre></td></tr></table></figure><ul><li>这个容器生命的前 30 秒，&#x2F;tmp&#x2F;healthy 文件是存在的。 所以在这最开始的 30 秒内，执行命令 cat &#x2F;tmp&#x2F;healthy 会返回成功代码。 30 秒之后，执行命令 cat &#x2F;tmp&#x2F;healthy 就会返回失败代码。</li></ul><p>创建 Pod：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最好先拉取镜像，如果是使用docker，就换成docker就行</span></span><br><span class="line">crictl pull registry.aliyuncs.com/google_containers/busybox</span><br><span class="line"></span><br><span class="line">kubectl apply -f exec-liveness.yaml</span><br></pre></td></tr></table></figure><p>【问题】<code>ERRO[0000] unable to determine image API version: rpc error: code = Unavailable desc = connection error: desc = “transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory”</code><br>【解决】原因：未配置endpoints</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crictl config runtime-endpoint unix:///run/containerd/containerd.sock</span><br><span class="line">crictl config image-endpoint unix:///run/containerd/containerd.sock</span><br></pre></td></tr></table></figure><p>查看</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod liveness-exec</span><br></pre></td></tr></table></figure><p>【现象】30s之后检查失败后就重启pod了，又正常了。</p><h3 id="2）httpGet-方式"><a href="#2）httpGet-方式" class="headerlink" title="2）httpGet 方式"></a>2）httpGet 方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;http-liveness.yaml&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Pod</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: liveness-httpget</span></span><br><span class="line"><span class="string">  namespace: default</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  # 为了测试方便，指定调度机器</span></span><br><span class="line"><span class="string">  nodeName: local-168-182-110</span></span><br><span class="line"><span class="string">  containers:</span></span><br><span class="line"><span class="string">  - name: liveness-httpget-container</span></span><br><span class="line"><span class="string">    image: nginx</span></span><br><span class="line"><span class="string">    imagePullPolicy: IfNotPresent</span></span><br><span class="line"><span class="string">    ports:</span></span><br><span class="line"><span class="string">    - name: nginx</span></span><br><span class="line"><span class="string">      containerPort: 80</span></span><br><span class="line"><span class="string">    livenessProbe:</span></span><br><span class="line"><span class="string">      httpGet:</span></span><br><span class="line"><span class="string">        port: nginx</span></span><br><span class="line"><span class="string">        path: /index.html</span></span><br><span class="line"><span class="string">      initialDelaySeconds: 1</span></span><br><span class="line"><span class="string">      periodSeconds: 3</span></span><br><span class="line"><span class="string">      timeoutSeconds: 10</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>解释：</p><ul><li><p><code>initialDelaySeconds</code>字段告诉 kubelet 在执行第一次探测前应该<strong>等待 1 秒</strong>。</p></li><li><p><code>periodSeconds</code> 字段指定了 kubelet <strong>每隔 3 秒</strong>执行一次存活探测。</p></li><li><p>kubelet 会向容器内运行的服务（服务在监听 80 端口）发送一个 HTTP GET 请求来执行探测。</p></li><li><p>如果服务器上<code>/index.html</code>路径下的处理程序返回成功代码，则 kubelet 认为容器是健康存活的。</p></li><li><p>如果处理程序返回失败代码，则 kubelet 会杀死这个容器并将其重启。</p></li><li><p>返回<strong>大于或等于 <code>200</code></strong> 并且<strong>小于 <code>400</code></strong> 的任何代码都<strong>标示成功</strong>，其它返回代码都标示失败。</p></li></ul><p>执行并查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crictl pull nginx</span><br><span class="line">kubectl apply -f http-liveness.yaml</span><br><span class="line">kubectl describe pod liveness-httpget</span><br></pre></td></tr></table></figure><p>删除 Pod 的 index.html 文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it liveness-httpget -- <span class="built_in">rm</span> -rf /usr/share/nginx/html/index.html</span><br><span class="line"><span class="comment"># 再查看</span></span><br><span class="line">kubectl describe pod liveness-httpget</span><br><span class="line">kubectl get pod liveness-httpget</span><br></pre></td></tr></table></figure><ul><li><p>重启原因是 HTTP 探测得到的状态返回码是 404，<code>Liveness probe failed: HTTP probe failed with statuscode: 404</code>。</p></li><li><p>重启完成后，不会再次重启，因为重新拉取的镜像中包含了 index.html 文件。</p></li></ul><p>HTTP Probes 允许针对 httpGet 配置额外的字段：</p><ul><li><p><code>host</code>：连接使用的主机名，默认是 Pod 的 IP。也可以在 HTTP 头中设置 “Host” 来代替。</p></li><li><p><code>scheme</code> ：用于设置连接主机的方式（HTTP 还是 HTTPS）。默认是 “HTTP”。</p></li><li><p><code>path</code>：访问 HTTP 服务的路径。默认值为 “&#x2F;“。</p></li><li><p><code>httpHeaders</code>：请求中自定义的 HTTP 头。HTTP 头字段允许重复。</p></li><li><p><code>port</code>：访问容器的端口号或者端口名。如果数字必须在 1～65535 之间。</p></li></ul><p>你可以通过为探测设置 .httpHeaders 来重载默认的头部字段值；例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">httpHeaders:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Accept</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">application/json</span></span><br><span class="line"></span><br><span class="line"><span class="attr">startupProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">httpHeaders:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">User-Agent</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">MyUserAgent</span></span><br></pre></td></tr></table></figure><h3 id="3）tcpSocket-方式"><a href="#3）tcpSocket-方式" class="headerlink" title="3）tcpSocket 方式"></a>3）tcpSocket 方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; tcp-liveness-readiness.yaml &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Pod</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: liveness-readiness-tcpsocket</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    app: liveness-readiness-tcpsocket</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  # 为了测试方便，指定调度机器</span></span><br><span class="line"><span class="string">  nodeName: local-168-182-110</span></span><br><span class="line"><span class="string">  containers:</span></span><br><span class="line"><span class="string">  - name: liveness-readiness-tcpsocket</span></span><br><span class="line"><span class="string">    image: nginx</span></span><br><span class="line"><span class="string">    ports:</span></span><br><span class="line"><span class="string">    - containerPort: 80</span></span><br><span class="line"><span class="string">    readinessProbe:</span></span><br><span class="line"><span class="string">      tcpSocket:</span></span><br><span class="line"><span class="string">        port: 80</span></span><br><span class="line"><span class="string">      initialDelaySeconds: 5</span></span><br><span class="line"><span class="string">      periodSeconds: 10</span></span><br><span class="line"><span class="string">    livenessProbe:</span></span><br><span class="line"><span class="string">      tcpSocket:</span></span><br><span class="line"><span class="string">        port: 80</span></span><br><span class="line"><span class="string">      initialDelaySeconds: 15</span></span><br><span class="line"><span class="string">      periodSeconds: 20</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>解释：</p><ul><li><p>kubelet 会在容器启动 5 秒后发送第一个<strong>就绪探测（livenessProbe）</strong>。</p></li><li><p>探测器会尝试连接 goproxy 容器的 80 端口。 如果探测成功，这个 Pod 会被标记为就绪状态，kubelet 将继续每隔 10 秒运行一次检测。</p></li><li><p>除了就绪探测，这个配置包括了一个<strong>存活探测（livenessProbe）</strong>。</p></li><li><p>kubelet 会在容器启动 <strong>15 秒后进行第一次存活探测（livenessProbe）</strong>。</p></li><li><p>与就绪探测类似，活跃探测器会尝试连接 goproxy 容器的 80 端口。 如果存活探测失败，容器会被重新启动。<br>执行</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f tcp-liveness-readiness.yaml</span><br><span class="line">kubectl get pod liveness-readiness-tcpsocket</span><br><span class="line">kubectl describe pod liveness-readiness-tcpsocket</span><br></pre></td></tr></table></figure><h3 id="4）使用命名端口"><a href="#4）使用命名端口" class="headerlink" title="4）使用命名端口"></a>4）使用命名端口</h3><blockquote><p>对于 HTTP 或者 TCP 存活检测可以使用命名的 port。</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ports:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">hostPort:</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">    <span class="attr">port:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure><p><strong>完整版配置</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ports:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">hostPort:</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># readinessProbe，就绪探针</span></span><br><span class="line"><span class="attr">readinessProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">    <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">  <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">  <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment">#  超时时间</span></span><br><span class="line">  <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">  <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">  <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># livenessProbe，存活探针</span></span><br><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">    <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">  <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">  <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment">#  超时时间</span></span><br><span class="line">  <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">  <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">  <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># startupProbe，启动探针</span></span><br><span class="line"><span class="attr">startupProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">    <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">  <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">  <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment">#  超时时间</span></span><br><span class="line">  <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">  <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">  <span class="attr">successThreshold:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>一般使用控制器去创建管理pod，对k8s 控制器不清晰的小伙伴，可以参考我之前的文章：Kubernetes（k8s）Deployment、StatefulSet、DaemonSet、Job、CronJob五种控制器详解</p><p>下面是一个完整版的示例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deployment-probe</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">     <span class="attr">app:</span> <span class="string">deployment-probe</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">deployment-probe</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.17.1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># readinessProbe，就绪探针</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment">#  超时时间</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">          <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># livenessProbe，存活探针</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment">#  超时时间</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">          <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># startupProbe，启动探针</span></span><br><span class="line">        <span class="attr">startupProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment">#  超时时间</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">          <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>执行查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crictl pull nginx:1.17.1</span><br><span class="line">kubectl apply -f deployment-probe.yaml</span><br><span class="line">kubectl get pod,deploy</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）6、kube-proxy、Service详解</title>
    <link href="https://blog.yongwang.lu/post/9f7e260b.html"/>
    <id>https://blog.yongwang.lu/post/9f7e260b.html</id>
    <published>2023-01-05T07:40:00.000Z</published>
    <updated>2023-01-05T07:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、kube-proxy简介"><a href="#一、kube-proxy简介" class="headerlink" title="一、kube-proxy简介"></a>一、kube-proxy简介</h2><blockquote><p><strong>kube-proxy负责为Service提供cluster内部的服务发现和负载均衡</strong>，它运行在每个Node计算节点上，负责Pod网络代理, 它会定时从etcd服务获取到service信息来做相应的策略，维护网络规则和四层负载均衡工作。在K8s集群中微服务的负载均衡是由Kube-proxy实现的，它是K8s集群内部的负载均衡器，也是一个分布式代理服务器，在K8s的每个节点上都有一个，这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。</p></blockquote><blockquote><p><strong>service是一组pod的服务抽象，相当于一组pod的LB</strong>，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP。<strong>kube-proxy的作用主要是负责service的实现</strong>，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。</p></blockquote><p>简单来说:</p><ul><li><p>kube-proxy其实就是管理service的访问入口，包括集群内Pod到Service的访问和集群外访问service。</p></li><li><p>kube-proxy管理sevice的Endpoints，该service对外暴露一个Virtual IP，也成为Cluster IP, 集群内通过访问这个Cluster IP:Port就能访问到集群内对应的serivce下的Pod。</p></li><li><p>service是通过Selector选择的一组Pods的服务抽象，其实就是一个微服务，提供了服务的LB和反向代理的能力，而kube-proxy的主要作用就是负责service的实现。</p></li><li><p>service另外一个重要作用是，一个服务后端的Pods可能会随着生存灭亡而发生IP的改变，service的出现，给服务提供了一个固定的IP，而无视后端Endpoint的变化。</p></li></ul><h2 id="二、Service-简介"><a href="#二、Service-简介" class="headerlink" title="二、Service 简介"></a>二、Service 简介</h2><blockquote><p>Kubernetes Service定义了这样一种抽象： Service是一种可以访问 Pod逻辑分组的策略， Service通常是通过 Label Selector访问 Pod组。</p></blockquote><blockquote><p>Service能够提供负载均衡的能力，但是在使用上有以下限制：<strong>只提供 4 层负载均衡能力</strong>，而没有 7 层功能，但有时我们可能需要更多的匹配规则来转发请求，这点上 4 层负载均衡是不支持的。</p></blockquote><h2 id="三、Service-类型"><a href="#三、Service-类型" class="headerlink" title="三、Service 类型"></a>三、Service 类型</h2><p>Service在 K8s中有以下四种类型：</p><h3 id="1）ClusterIp（集群内部使用）"><a href="#1）ClusterIp（集群内部使用）" class="headerlink" title="1）ClusterIp（集群内部使用）"></a>1）ClusterIp（集群内部使用）</h3><blockquote><p><strong>默认类型</strong>，自动分配一个仅Cluster内部可以访问的虚拟IP（VIP）。</p></blockquote><h3 id="2）NodePort（对外暴露应用）"><a href="#2）NodePort（对外暴露应用）" class="headerlink" title="2）NodePort（对外暴露应用）"></a>2）NodePort（对外暴露应用）</h3><blockquote><p>在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过NodeIP:NodePort访问来访问该服务。<br>端口范围：30000~32767</p></blockquote><h3 id="3）LoadBalancer（对外暴露应用，适用于公有云）"><a href="#3）LoadBalancer（对外暴露应用，适用于公有云）" class="headerlink" title="3）LoadBalancer（对外暴露应用，适用于公有云）"></a>3）LoadBalancer（对外暴露应用，适用于公有云）</h3><blockquote><p>在NodePort的基础上，借助Cloud Provider创建一个外部负载均衡器，并将请求转发到NodePort。</p></blockquote><h3 id="4）ExternalName"><a href="#4）ExternalName" class="headerlink" title="4）ExternalName"></a>4）ExternalName</h3><blockquote><p>创建一个dns别名指到service name上，主要是防止service name发生变化，要配合dns插件使用。通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容。这只有 Kubernetes 1.7或更高版本的kube-dns才支持（<strong>我这里是Kubernetes 1.22.1版本</strong>）。</p></blockquote><h2 id="四、Service-工作流程"><a href="#四、Service-工作流程" class="headerlink" title="四、Service 工作流程"></a>四、Service 工作流程</h2><p><img src="https://res.yongwang.lu/img/20230322225854.png" alt="Service 工作流程"></p><ol><li><p>客户端访问节点时通过 iptables实现的</p></li><li><p>iptables规则是通过 kube-proxy写入的</p></li><li><p>apiserver通过监控 kube-proxy去进行对服务和端点的监控</p></li><li><p>kube-proxy通过 pod的标签（ lables）去判断这个断点信息是否写入到 Endpoints里</p></li></ol><h2 id="五、Endpoints简介"><a href="#五、Endpoints简介" class="headerlink" title="五、Endpoints简介"></a>五、Endpoints简介</h2><blockquote><p>endpoint是k8s集群中的一个资源对象，存储在etcd中，<strong>用来记录一个service对应的所有pod的访问地址</strong>。service配置selector，endpoint controller才会自动创建对应的endpoint对象；否则，不会生成endpoint对象。</p></blockquote><blockquote><p>【例如】k8s集群中创建一个名为hello的service，就会生成一个同名的endpoint对象，ENDPOINTS就是service关联的pod的ip地址和端口。</p></blockquote><h3 id="1）工作流程"><a href="#1）工作流程" class="headerlink" title="1）工作流程"></a>1）工作流程</h3><blockquote><p><strong>一个 Service 由一组 backend Pod 组成。这些 Pod 通过 endpoints 暴露出来</strong>。 Service Selector 将持续评估，结果被 POST 到一个名称为 Service-hello 的 Endpoint 对象上。 当 Pod 终止后，它会自动从 Endpoint 中移除，新的能够匹配上 Service Selector 的 Pod 将自动地被添加到 Endpoint 中。 检查该 Endpoint，注意到 IP 地址与创建的 Pod 是相同的。现在，能够从集群中任意节点上使用 curl 命令请求 hello Service <CLUSTER-IP>:<PORT> 。</p></blockquote><h3 id="2）示例"><a href="#2）示例" class="headerlink" title="2）示例"></a>2）示例</h3><p>1、deployment-hello.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ cat &lt;&lt; EOF &gt; deployment-hello.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">     run: hello</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: hello</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.17.1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>2、service-hello.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ cat &lt;&lt; EOF &gt; service-hello.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: service-hello</span><br><span class="line">  labels:</span><br><span class="line">  name: service-hello</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort      #这里代表是NodePort类型的,另外还有ingress,LoadBalancer</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80          </span><br><span class="line">    targetPort: 8080</span><br><span class="line">    protocol: TCP</span><br><span class="line">    nodePort: 31111   # 所有的节点都会开放此端口30000--32767，此端口供外部调用。</span><br><span class="line">  selector:</span><br><span class="line">    run: hello</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>3、查看验证</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f deployment-hello.yaml</span><br><span class="line">$ kubectl apply -f service-hello.yaml</span><br><span class="line"># 查看pod，如果本地没有镜像，可能等待的时候比较长，一定要等到所有pod都在运行中才行。</span><br><span class="line">$ kubectl get pod -o wide|grep hello-*</span><br><span class="line"># 查看service</span><br><span class="line">$ kubectl get service service-hello -o wide</span><br><span class="line"># 查看service详情</span><br><span class="line">$ kubectl describe service service-hello</span><br><span class="line"># 查看pointer</span><br><span class="line">$ kubectl get endpoints service-hello</span><br></pre></td></tr></table></figure><h2 id="六、Service-Endpoints与Pod的关系"><a href="#六、Service-Endpoints与Pod的关系" class="headerlink" title="六、Service, Endpoints与Pod的关系"></a>六、Service, Endpoints与Pod的关系</h2><p><img src="https://res.yongwang.lu/img/20230322230158.png" alt="Service, Endpoints与Pod的关系"></p><blockquote><p>Kube-proxy进程获取每个Service的Endpoints,实现Service的负载均衡功能。</p></blockquote><p>Service的负载均衡转发规则<br><img src="https://res.yongwang.lu/img/20230322230057.png" alt="Service的负载均衡转发规则 "></p><blockquote><p>访问Service的请求，不论是Cluster IP+TargetPort的方式；还是用Node节点IP+NodePort的方式，都被Node节点的Iptables规则重定向到Kube-proxy监听Service服务代理端口。kube-proxy接收到Service的访问请求后，根据负载策略，转发到后端的Pod。</p></blockquote><h2 id="七、Service的资源清单文件详解"><a href="#七、Service的资源清单文件详解" class="headerlink" title="七、Service的资源清单文件详解"></a>七、Service的资源清单文件详解</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">#元数据</span><br><span class="line">  name: string</span><br><span class="line">  #Service名称</span><br><span class="line">  namespace: string</span><br><span class="line">  #命名空间，不指定时默认为default命名空间</span><br><span class="line">  labels:</span><br><span class="line">  #自定义标签属性列表     </span><br><span class="line">    - name: string</span><br><span class="line">  annotations:</span><br><span class="line">  #自定义注解属性列表    </span><br><span class="line">    - name: string</span><br><span class="line">spec:</span><br><span class="line">#详细描述    </span><br><span class="line">  selector: []</span><br><span class="line">  #这里选择器一定要选择容器的标签，也就是pod的标签</span><br><span class="line">  #selector:</span><br><span class="line">  #  app: web</span><br><span class="line">  #Label Selector配置，选择具有指定label标签的pod作为管理范围</span><br><span class="line">  type: string</span><br><span class="line">  #service的类型，指定service的访问方式，默认ClusterIP</span><br><span class="line">  #ClusterIP：虚拟的服务ip地址，用于k8s集群内部的pod访问，在Node上kube-porxy通过设置的iptables规则进行转发</span><br><span class="line">  #NodePort：使用宿主机端口，能够访问各Node的外部客户端通过Node的IP和端口就能访问服务器</span><br><span class="line">  #LoadBalancer：使用外部负载均衡器完成到服务器的负载分发，</span><br><span class="line">  #需要在spec.status.loadBalancer字段指定外部负载均衡服务器的IP，并同时定义nodePort和clusterIP用于公有云环境。</span><br><span class="line">  clusterIP: string</span><br><span class="line">  #虚拟服务IP地址，当type=ClusterIP时，如不指定，则系统会自动进行分配，也可以手动指定。当type=loadBalancer，需要指定</span><br><span class="line">  sessionAffinity: string</span><br><span class="line">  #是否支持session，可选值为ClietIP，默认值为空</span><br><span class="line">  #ClientIP表示将同一个客户端(根据客户端IP地址决定)的访问请求都转发到同一个后端Pod</span><br><span class="line">  ports:</span><br><span class="line">  #service需要暴露的端口列表    </span><br><span class="line">  - name: string</span><br><span class="line">    #端口名称</span><br><span class="line">    protocol: string</span><br><span class="line">    #端口协议，支持TCP或UDP，默认TCP</span><br><span class="line">     port: int</span><br><span class="line">    #服务监听的端口号</span><br><span class="line">     targetPort: int</span><br><span class="line">    #需要转发到后端的端口号</span><br><span class="line">     nodePort: int</span><br><span class="line">    #当type=NodePort时，指定映射到物理机的端口号</span><br><span class="line">  status:</span><br><span class="line">  #当type=LoadBalancer时，设置外部负载均衡的地址，用于公有云环境    </span><br><span class="line">    loadBalancer:</span><br><span class="line">    #外部负载均衡器    </span><br><span class="line">      ingress:</span><br><span class="line">      #外部负载均衡器 </span><br><span class="line">      ip: string</span><br><span class="line">      #外部负载均衡器的IP地址</span><br><span class="line">      hostname: string</span><br><span class="line">     #外部负载均衡器的机主机</span><br></pre></td></tr></table></figure><h2 id="八、kubernetes中的四种port"><a href="#八、kubernetes中的四种port" class="headerlink" title="八、kubernetes中的四种port"></a>八、kubernetes中的四种port</h2><h3 id="1）nodePort"><a href="#1）nodePort" class="headerlink" title="1）nodePort"></a>1）nodePort</h3><blockquote><p><strong>nodePort是外部访问k8s集群中service的端口</strong>，通过nodeIP: nodePort可以从外部访问到某个service。</p></blockquote><h3 id="2）port"><a href="#2）port" class="headerlink" title="2）port"></a>2）port</h3><blockquote><p><strong>port是k8s集群内部访问service的端口</strong>，即通过clusterIP: port可以访问到某个service。</p></blockquote><h3 id="3）targetPort"><a href="#3）targetPort" class="headerlink" title="3）targetPort"></a>3）targetPort</h3><blockquote><p><strong>targetPort是pod的端口</strong>，从port和nodePort来的流量经过kube-proxy流入到后端pod的targetPort上，最后进入容器。</p></blockquote><h3 id="4）containerPort"><a href="#4）containerPort" class="headerlink" title="4）containerPort"></a>4）containerPort</h3><blockquote><p><strong>containerPort是pod内部容器的端口</strong>，targetPort映射到containerPort。</p></blockquote><p><img src="https://res.yongwang.lu/img/202303221413828.png" alt="图片"></p><h2 id="九、kubernetes服务发现"><a href="#九、kubernetes服务发现" class="headerlink" title="九、kubernetes服务发现"></a>九、kubernetes服务发现</h2><blockquote><p>Kubernetes提供了两种方式进行服务发现, 即<strong>环境变量和DNS</strong>, 简单说明如下:</p></blockquote><h3 id="1）环境变量"><a href="#1）环境变量" class="headerlink" title="1）环境变量"></a>1）环境变量</h3><blockquote><p>当你创建一个Pod的时候，kubelet会在该Pod中注入集群内所有Service的相关环境变量。</p></blockquote><blockquote><p>【注意】要想一个Pod中注入某个Service的环境变量，则<strong>必须Service要比该Pod先创建</strong>。这一点，几乎使得这种方式进行服务发现不可用。比如，一个ServiceName为redis-master的Service，对应的ClusterIP:Port为172.16.50.11:6379，则其对应的环境变量为:</p></blockquote><blockquote><p>REDIS_MASTER_SERVICE_HOST&#x3D;172.16.50.11<br>REDIS_MASTER_SERVICE_PORT&#x3D;6379<br>REDIS_MASTER_PORT&#x3D;tcp:&#x2F;&#x2F;172.16.50.11:6379<br>REDIS_MASTER_PORT_6379_TCP&#x3D;tcp:&#x2F;&#x2F;172.16.50.11:6379<br>REDIS_MASTER_PORT_6379_TCP_PROTO&#x3D;tcp<br>REDIS_MASTER_PORT_6379_TCP_PORT&#x3D;6379<br>REDIS_MASTER_PORT_6379_TCP_ADDR&#x3D;172.16.50.11</p></blockquote><h3 id="2-DNS"><a href="#2-DNS" class="headerlink" title="2) DNS"></a>2) DNS</h3><blockquote><p>这是k8s官方强烈推荐的方式!!! 可以通过cluster add-on方式轻松的创建KubeDNS来对集群内的Service进行服务发现。</p></blockquote><h2 id="十、Service代理模式"><a href="#十、Service代理模式" class="headerlink" title="十、Service代理模式"></a>十、Service代理模式</h2><blockquote><p>k8s群集中的每个节点都运行一个kube-proxy的组件，kube-proxy其实是一个代理层负责实现service。</p></blockquote><blockquote><p><strong>Kubernetes v1.2之前默认是userspace，v1.2之后默认是iptables模式</strong>，iptables模式性能和可靠性更好，但是iptables模式依赖健康检查，在没有健康检查的情况下如果一个pod不响应，iptables模式不会切换另一个pod上。</p></blockquote><h3 id="1）userspace模式"><a href="#1）userspace模式" class="headerlink" title="1）userspace模式"></a>1）userspace模式</h3><blockquote><p>客户端访问ServiceIP(clusterIP)请求会先从用户空间到内核中的iptables，然后回到用户空间kube-proxy，kube-proxy负责代理工作。</p></blockquote><p>缺点：</p><blockquote><p>可见，userspace这种mode最大的问题是，service的请求会先从用户空间进入内核iptables，然后再回到用户空间，由kube-proxy完成后端Endpoints的选择和代理工作，这样流量从用户空间进出内核带来的性能损耗是不可接受的。这也是k8s v1.0及之前版本中对kube-proxy质疑最大的一点，因此社区就开始研究iptables mode。</p></blockquote><p>详细工作流程：</p><blockquote><p>userspace这种模式下，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；对每个 Service，它都为其在本地节点开放一个端口，作为其服务代理端口；发往该端口的请求会采用一定的策略转发给与该服务对应的后端 Pod 实体。kube-proxy 同时会在本地节点设置 iptables 规则，配置一个 Virtual IP，把发往 Virtual IP 的请求重定向到与该 Virtual IP 对应的服务代理端口上。其工作流程大体如下:</p></blockquote><blockquote><p>【分析】该模式请求在到达 iptables 进行处理时就会进入内核，而 kube-proxy 监听则是在用户态, 请求就形成了从用户态到内核态再返回到用户态的传递过程, 一定程度降低了服务性能。</p></blockquote><h3 id="2）iptables模式（默认模式）"><a href="#2）iptables模式（默认模式）" class="headerlink" title="2）iptables模式（默认模式）"></a>2）iptables模式（默认模式）</h3><blockquote><p>该模式完全利用内核iptables来实现service的代理和LB, 这是K8s在<strong>v1.2及之后版本默认模式</strong>. 工作原理如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322225854.png" alt="iptables模式"></p><blockquote><p><strong>iptables mode因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗</strong>。另外，如果集群中存在上万的Service&#x2F;Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。这也导致目前大部分企业用k8s上生产时，都不会直接用kube-proxy作为服务代理，而是通过自己开发或者通过Ingress Controller来集成HAProxy, Nginx来代替kube-proxy。</p></blockquote><p>详细工作流程：</p><blockquote><p>iptables 模式与 userspace 相同，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；但它并不在本地节点开启反向代理服务，而是把反向代理全部交给 iptables 来实现；即 iptables 直接将对 VIP 的请求转发给后端 Pod，通过 iptables 设置转发策略。其工作流程大体如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322230525.png" alt="iptables-详细工作流程"></p><blockquote><p>【分析】 该模式相比 userspace 模式，克服了请求在用户态-内核态反复传递的问题，性能上有所提升，但使用 iptables NAT 来完成转发，存在不可忽视的性能损耗，而且在大规模场景下，iptables 规则的条目会十分巨大，性能上还要再打折扣。</p></blockquote><p>示例:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF &gt; mysql-service.yaml</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Service</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    name: mysql</span></span><br><span class="line"><span class="string">    role: service</span></span><br><span class="line"><span class="string">  name: mysql-service</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  ports:</span></span><br><span class="line"><span class="string">    - port: 3306</span></span><br><span class="line"><span class="string">      targetPort: 3306</span></span><br><span class="line"><span class="string">      nodePort: 30964</span></span><br><span class="line"><span class="string">  type: NodePort</span></span><br><span class="line"><span class="string">  selector:</span></span><br><span class="line"><span class="string">    mysql-service: &quot;true&quot;</span></span><br><span class="line"><span class="string">    name: mysql</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">$ kubectl apply -f mysql-service.yaml</span><br><span class="line">$ kubectl get svc</span><br></pre></td></tr></table></figure><h3 id="3）ipvs模型"><a href="#3）ipvs模型" class="headerlink" title="3）ipvs模型"></a>3）ipvs模型</h3><blockquote><p>在kubernetes 1.8以上的版本中，对于kube-proxy组件增加了除iptables模式和用户模式之外还支持ipvs模式。<strong>kube-proxy ipvs 是基于 NAT 实现的</strong>，通过ipvs的NAT模式，对访问k8s service的请求进行虚IP到POD IP的转发。当创建一个 service 后，kubernetes 会在每个节点上创建一个网卡，同时帮你将 Service IP(VIP) 绑定上，此时相当于每个 Node 都是一个 ds，而其他任何 Node 上的 Pod，甚至是宿主机服务(比如 kube-apiserver 的 6443)都可能成为 rs；</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322230705.png" alt="ipvs-service模型"></p><p>详细工作流程：</p><blockquote><p>与iptables、userspace 模式一样，kube-proxy 依然监听Service以及Endpoints对象的变化, 不过它并不创建反向代理, 也不创建大量的 iptables 规则, 而是通过netlink 创建ipvs规则，并使用k8s Service与Endpoints信息，对所在节点的ipvs规则进行定期同步; netlink 与 iptables 底层都是基于 netfilter 钩子，但是 netlink 由于采用了 hash table 而且直接工作在内核态，在性能上比 iptables 更优。其工作流程大体如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322230748.png" alt="ipvs工作流程"></p><blockquote><p>【分析】ipvs 是目前 kube-proxy 所支持的最新代理模式，相比使用 iptables，使用 ipvs 具有更高的性能。</p></blockquote><hr><h3 id="4）kube-proxy配置-ipvs模式（所有节点）"><a href="#4）kube-proxy配置-ipvs模式（所有节点）" class="headerlink" title="4）kube-proxy配置 ipvs模式（所有节点）"></a>4）kube-proxy配置 ipvs模式（所有节点）</h3><p>1、加载ip_vs相关内核模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ modprobe -- ip_vs</span><br><span class="line">$ modprobe -- ip_vs_sh</span><br><span class="line">$ modprobe -- ip_vs_rr</span><br><span class="line">$ modprobe -- ip_vs_wrr</span><br><span class="line">$ modprobe -- nf_conntrack_ipv4</span><br></pre></td></tr></table></figure><p>所有节点验证开启了ipvs：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lsmod |grep ip_vs</span><br></pre></td></tr></table></figure><p>2、安装ipvsadm工具</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install ipset ipvsadm -y</span><br></pre></td></tr></table></figure><p>3、编辑kube-proxy配置文件，mode修改成ipvs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl edit  configmap -n kube-system  kube-proxy</span><br></pre></td></tr></table></figure><p>4、重启kube-proxy<br>先查看之前的kube-proxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy</span><br></pre></td></tr></table></figure><p>删掉上面三个kube-proxy，重新拉起新的服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy |awk <span class="string">&#x27;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&#x27;</span></span><br></pre></td></tr></table></figure><p>再查看</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">title: Kubernetes（k8s）kube-proxy、Service详解</span><br><span class="line">abbrlink: 5475e8a5</span><br><span class="line">tags: kubernetes</span><br><span class="line">categories: kubernetes</span><br><span class="line">keywords: k8s</span><br><span class="line">description: kubernetes1.26</span><br><span class="line">top_img: &#x27;https://kubernetes.io/images/nav_logo2.svg&#x27;</span><br><span class="line">cover: &#x27;https://res.yongwang.lu/img/k8s_card.png&#x27;</span><br><span class="line">date: 2023-03-21 21:40:00</span><br><span class="line">updated: 2023-03-21 21:40:00</span><br><span class="line">comments:</span><br><span class="line">toc:</span><br><span class="line">toc_number:</span><br><span class="line">toc_style_simple:</span><br><span class="line">copyright:</span><br><span class="line">copyright_author:</span><br><span class="line">copyright_author_href:</span><br><span class="line">copyright_url:</span><br><span class="line">copyright_info:</span><br><span class="line">mathjax:</span><br><span class="line">katex:</span><br><span class="line">aplayer:</span><br><span class="line">highlight_shrink:</span><br><span class="line">aside:</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## 一、kube-proxy简介</span><br><span class="line"></span><br><span class="line">&gt; **kube-proxy负责为Service提供cluster内部的服务发现和负载均衡**，它运行在每个Node计算节点上，负责Pod网络代理, 它会定时从etcd服务获取到service信息来做相应的策略，维护网络规则和四层负载均衡工作。在K8s集群中微服务的负载均衡是由Kube-proxy实现的，它是K8s集群内部的负载均衡器，也是一个分布式代理服务器，在K8s的每个节点上都有一个，这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。</span><br><span class="line"></span><br><span class="line">&gt; **service是一组pod的服务抽象，相当于一组pod的LB**，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP。**kube-proxy的作用主要是负责service的实现**，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。</span><br><span class="line"></span><br><span class="line">简单来说:</span><br><span class="line"></span><br><span class="line">-   kube-proxy其实就是管理service的访问入口，包括集群内Pod到Service的访问和集群外访问service。</span><br><span class="line">    </span><br><span class="line">-   kube-proxy管理sevice的Endpoints，该service对外暴露一个Virtual IP，也成为Cluster IP, 集群内通过访问这个Cluster IP:Port就能访问到集群内对应的serivce下的Pod。</span><br><span class="line">    </span><br><span class="line">-   service是通过Selector选择的一组Pods的服务抽象，其实就是一个微服务，提供了服务的LB和反向代理的能力，而kube-proxy的主要作用就是负责service的实现。</span><br><span class="line">    </span><br><span class="line">-   service另外一个重要作用是，一个服务后端的Pods可能会随着生存灭亡而发生IP的改变，service的出现，给服务提供了一个固定的IP，而无视后端Endpoint的变化。</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">## 二、Service 简介</span><br><span class="line"></span><br><span class="line">&gt; Kubernetes Service定义了这样一种抽象： Service是一种可以访问 Pod逻辑分组的策略， Service通常是通过 Label Selector访问 Pod组。</span><br><span class="line"></span><br><span class="line">&gt; Service能够提供负载均衡的能力，但是在使用上有以下限制：**只提供 4 层负载均衡能力**，而没有 7 层功能，但有时我们可能需要更多的匹配规则来转发请求，这点上 4 层负载均衡是不支持的。</span><br><span class="line"></span><br><span class="line">## 三、Service 类型</span><br><span class="line"></span><br><span class="line">Service在 K8s中有以下四种类型：</span><br><span class="line"></span><br><span class="line">### 1）ClusterIp（集群内部使用）</span><br><span class="line"></span><br><span class="line">&gt; **默认类型**，自动分配一个仅Cluster内部可以访问的虚拟IP（VIP）。</span><br><span class="line"></span><br><span class="line">### 2）NodePort（对外暴露应用）</span><br><span class="line"></span><br><span class="line">&gt; 在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过NodeIP:NodePort访问来访问该服务。  </span><br><span class="line">&gt; 端口范围：30000~32767</span><br><span class="line"></span><br><span class="line">### 3）LoadBalancer（对外暴露应用，适用于公有云）</span><br><span class="line"></span><br><span class="line">&gt; 在NodePort的基础上，借助Cloud Provider创建一个外部负载均衡器，并将请求转发到NodePort。</span><br><span class="line"></span><br><span class="line">### 4）ExternalName</span><br><span class="line"></span><br><span class="line">&gt; 创建一个dns别名指到service name上，主要是防止service name发生变化，要配合dns插件使用。通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容。这只有 Kubernetes 1.7或更高版本的kube-dns才支持（**我这里是Kubernetes 1.22.1版本**）。</span><br><span class="line"></span><br><span class="line">## 四、Service 工作流程</span><br><span class="line"></span><br><span class="line">![Service 工作流程](https://res.yongwang.lu/img/20230322225854.png)</span><br><span class="line"></span><br><span class="line">1.  客户端访问节点时通过 iptables实现的</span><br><span class="line">    </span><br><span class="line">2.  iptables规则是通过 kube-proxy写入的</span><br><span class="line">    </span><br><span class="line">3.  apiserver通过监控 kube-proxy去进行对服务和端点的监控</span><br><span class="line">    </span><br><span class="line">4.  kube-proxy通过 pod的标签（ lables）去判断这个断点信息是否写入到 Endpoints里</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">## 五、Endpoints简介</span><br><span class="line"></span><br><span class="line">&gt; endpoint是k8s集群中的一个资源对象，存储在etcd中，**用来记录一个service对应的所有pod的访问地址**。service配置selector，endpoint controller才会自动创建对应的endpoint对象；否则，不会生成endpoint对象。</span><br><span class="line"></span><br><span class="line">&gt; 【例如】k8s集群中创建一个名为hello的service，就会生成一个同名的endpoint对象，ENDPOINTS就是service关联的pod的ip地址和端口。</span><br><span class="line"></span><br><span class="line">### 1）工作流程</span><br><span class="line"></span><br><span class="line">&gt; **一个 Service 由一组 backend Pod 组成。这些 Pod 通过 endpoints 暴露出来**。 Service Selector 将持续评估，结果被 POST 到一个名称为 Service-hello 的 Endpoint 对象上。 当 Pod 终止后，它会自动从 Endpoint 中移除，新的能够匹配上 Service Selector 的 Pod 将自动地被添加到 Endpoint 中。 检查该 Endpoint，注意到 IP 地址与创建的 Pod 是相同的。现在，能够从集群中任意节点上使用 curl 命令请求 hello Service &lt;CLUSTER-IP&gt;:&lt;PORT&gt; 。</span><br><span class="line"></span><br><span class="line">### 2）示例</span><br><span class="line"></span><br><span class="line">1、deployment-hello.yaml</span><br><span class="line"></span><br><span class="line">```auto</span><br><span class="line">$ cat &lt;&lt; EOF &gt; deployment-hello.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">     run: hello</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: hello</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.17.1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>2、service-hello.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ cat &lt;&lt; EOF &gt; service-hello.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: service-hello</span><br><span class="line">  labels:</span><br><span class="line">  name: service-hello</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort      #这里代表是NodePort类型的,另外还有ingress,LoadBalancer</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80          </span><br><span class="line">    targetPort: 8080</span><br><span class="line">    protocol: TCP</span><br><span class="line">    nodePort: 31111   # 所有的节点都会开放此端口30000--32767，此端口供外部调用。</span><br><span class="line">  selector:</span><br><span class="line">    run: hello</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>3、查看验证</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f deployment-hello.yaml</span><br><span class="line">$ kubectl apply -f service-hello.yaml</span><br><span class="line"># 查看pod，如果本地没有镜像，可能等待的时候比较长，一定要等到所有pod都在运行中才行。</span><br><span class="line">$ kubectl get pod -o wide|grep hello-*</span><br><span class="line"># 查看service</span><br><span class="line">$ kubectl get service service-hello -o wide</span><br><span class="line"># 查看service详情</span><br><span class="line">$ kubectl describe service service-hello</span><br><span class="line"># 查看pointer</span><br><span class="line">$ kubectl get endpoints service-hello</span><br></pre></td></tr></table></figure><h2 id="六、Service-Endpoints与Pod的关系-1"><a href="#六、Service-Endpoints与Pod的关系-1" class="headerlink" title="六、Service, Endpoints与Pod的关系"></a>六、Service, Endpoints与Pod的关系</h2><p><img src="https://res.yongwang.lu/img/20230322230158.png" alt="Service, Endpoints与Pod的关系"></p><blockquote><p>Kube-proxy进程获取每个Service的Endpoints,实现Service的负载均衡功能。</p></blockquote><p>Service的负载均衡转发规则<br><img src="https://res.yongwang.lu/img/20230322230057.png" alt="Service的负载均衡转发规则 "></p><blockquote><p>访问Service的请求，不论是Cluster IP+TargetPort的方式；还是用Node节点IP+NodePort的方式，都被Node节点的Iptables规则重定向到Kube-proxy监听Service服务代理端口。kube-proxy接收到Service的访问请求后，根据负载策略，转发到后端的Pod。</p></blockquote><h2 id="七、Service的资源清单文件详解-1"><a href="#七、Service的资源清单文件详解-1" class="headerlink" title="七、Service的资源清单文件详解"></a>七、Service的资源清单文件详解</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">#元数据</span><br><span class="line">  name: string</span><br><span class="line">  #Service名称</span><br><span class="line">  namespace: string</span><br><span class="line">  #命名空间，不指定时默认为default命名空间</span><br><span class="line">  labels:</span><br><span class="line">  #自定义标签属性列表     </span><br><span class="line">    - name: string</span><br><span class="line">  annotations:</span><br><span class="line">  #自定义注解属性列表    </span><br><span class="line">    - name: string</span><br><span class="line">spec:</span><br><span class="line">#详细描述    </span><br><span class="line">  selector: []</span><br><span class="line">  #这里选择器一定要选择容器的标签，也就是pod的标签</span><br><span class="line">  #selector:</span><br><span class="line">  #  app: web</span><br><span class="line">  #Label Selector配置，选择具有指定label标签的pod作为管理范围</span><br><span class="line">  type: string</span><br><span class="line">  #service的类型，指定service的访问方式，默认ClusterIP</span><br><span class="line">  #ClusterIP：虚拟的服务ip地址，用于k8s集群内部的pod访问，在Node上kube-porxy通过设置的iptables规则进行转发</span><br><span class="line">  #NodePort：使用宿主机端口，能够访问各Node的外部客户端通过Node的IP和端口就能访问服务器</span><br><span class="line">  #LoadBalancer：使用外部负载均衡器完成到服务器的负载分发，</span><br><span class="line">  #需要在spec.status.loadBalancer字段指定外部负载均衡服务器的IP，并同时定义nodePort和clusterIP用于公有云环境。</span><br><span class="line">  clusterIP: string</span><br><span class="line">  #虚拟服务IP地址，当type=ClusterIP时，如不指定，则系统会自动进行分配，也可以手动指定。当type=loadBalancer，需要指定</span><br><span class="line">  sessionAffinity: string</span><br><span class="line">  #是否支持session，可选值为ClietIP，默认值为空</span><br><span class="line">  #ClientIP表示将同一个客户端(根据客户端IP地址决定)的访问请求都转发到同一个后端Pod</span><br><span class="line">  ports:</span><br><span class="line">  #service需要暴露的端口列表    </span><br><span class="line">  - name: string</span><br><span class="line">    #端口名称</span><br><span class="line">    protocol: string</span><br><span class="line">    #端口协议，支持TCP或UDP，默认TCP</span><br><span class="line">     port: int</span><br><span class="line">    #服务监听的端口号</span><br><span class="line">     targetPort: int</span><br><span class="line">    #需要转发到后端的端口号</span><br><span class="line">     nodePort: int</span><br><span class="line">    #当type=NodePort时，指定映射到物理机的端口号</span><br><span class="line">  status:</span><br><span class="line">  #当type=LoadBalancer时，设置外部负载均衡的地址，用于公有云环境    </span><br><span class="line">    loadBalancer:</span><br><span class="line">    #外部负载均衡器    </span><br><span class="line">      ingress:</span><br><span class="line">      #外部负载均衡器 </span><br><span class="line">      ip: string</span><br><span class="line">      #外部负载均衡器的IP地址</span><br><span class="line">      hostname: string</span><br><span class="line">     #外部负载均衡器的机主机</span><br></pre></td></tr></table></figure><h2 id="八、kubernetes中的四种port-1"><a href="#八、kubernetes中的四种port-1" class="headerlink" title="八、kubernetes中的四种port"></a>八、kubernetes中的四种port</h2><h3 id="1）nodePort-1"><a href="#1）nodePort-1" class="headerlink" title="1）nodePort"></a>1）nodePort</h3><blockquote><p><strong>nodePort是外部访问k8s集群中service的端口</strong>，通过nodeIP: nodePort可以从外部访问到某个service。</p></blockquote><h3 id="2）port-1"><a href="#2）port-1" class="headerlink" title="2）port"></a>2）port</h3><blockquote><p><strong>port是k8s集群内部访问service的端口</strong>，即通过clusterIP: port可以访问到某个service。</p></blockquote><h3 id="3）targetPort-1"><a href="#3）targetPort-1" class="headerlink" title="3）targetPort"></a>3）targetPort</h3><blockquote><p><strong>targetPort是pod的端口</strong>，从port和nodePort来的流量经过kube-proxy流入到后端pod的targetPort上，最后进入容器。</p></blockquote><h3 id="4）containerPort-1"><a href="#4）containerPort-1" class="headerlink" title="4）containerPort"></a>4）containerPort</h3><blockquote><p><strong>containerPort是pod内部容器的端口</strong>，targetPort映射到containerPort。</p></blockquote><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZyHzM6VdDdXcBuy39GptibDPKJMXiacaicepLhblRia9j7k0YRGicmTZW3RSuF4ofq38gHQY7dYOdu10G45lE1nzE7Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><h2 id="九、kubernetes服务发现-1"><a href="#九、kubernetes服务发现-1" class="headerlink" title="九、kubernetes服务发现"></a>九、kubernetes服务发现</h2><blockquote><p>Kubernetes提供了两种方式进行服务发现, 即<strong>环境变量和DNS</strong>, 简单说明如下:</p></blockquote><h3 id="1）环境变量-1"><a href="#1）环境变量-1" class="headerlink" title="1）环境变量"></a>1）环境变量</h3><blockquote><p>当你创建一个Pod的时候，kubelet会在该Pod中注入集群内所有Service的相关环境变量。</p></blockquote><blockquote><p>【注意】要想一个Pod中注入某个Service的环境变量，则<strong>必须Service要比该Pod先创建</strong>。这一点，几乎使得这种方式进行服务发现不可用。比如，一个ServiceName为redis-master的Service，对应的ClusterIP:Port为172.16.50.11:6379，则其对应的环境变量为:</p></blockquote><blockquote><p>REDIS_MASTER_SERVICE_HOST&#x3D;172.16.50.11<br>REDIS_MASTER_SERVICE_PORT&#x3D;6379<br>REDIS_MASTER_PORT&#x3D;tcp:&#x2F;&#x2F;172.16.50.11:6379<br>REDIS_MASTER_PORT_6379_TCP&#x3D;tcp:&#x2F;&#x2F;172.16.50.11:6379<br>REDIS_MASTER_PORT_6379_TCP_PROTO&#x3D;tcp<br>REDIS_MASTER_PORT_6379_TCP_PORT&#x3D;6379<br>REDIS_MASTER_PORT_6379_TCP_ADDR&#x3D;172.16.50.11</p></blockquote><h3 id="2-DNS-1"><a href="#2-DNS-1" class="headerlink" title="2) DNS"></a>2) DNS</h3><blockquote><p>这是k8s官方强烈推荐的方式!!! 可以通过cluster add-on方式轻松的创建KubeDNS来对集群内的Service进行服务发现。</p></blockquote><h2 id="十、Service代理模式-1"><a href="#十、Service代理模式-1" class="headerlink" title="十、Service代理模式"></a>十、Service代理模式</h2><blockquote><p>k8s群集中的每个节点都运行一个kube-proxy的组件，kube-proxy其实是一个代理层负责实现service。</p></blockquote><blockquote><p><strong>Kubernetes v1.2之前默认是userspace，v1.2之后默认是iptables模式</strong>，iptables模式性能和可靠性更好，但是iptables模式依赖健康检查，在没有健康检查的情况下如果一个pod不响应，iptables模式不会切换另一个pod上。</p></blockquote><h3 id="1）userspace模式-1"><a href="#1）userspace模式-1" class="headerlink" title="1）userspace模式"></a>1）userspace模式</h3><blockquote><p>客户端访问ServiceIP(clusterIP)请求会先从用户空间到内核中的iptables，然后回到用户空间kube-proxy，kube-proxy负责代理工作。</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322231436.png" alt="图片"></p><p>缺点：</p><blockquote><p>可见，userspace这种mode最大的问题是，service的请求会先从用户空间进入内核iptables，然后再回到用户空间，由kube-proxy完成后端Endpoints的选择和代理工作，这样流量从用户空间进出内核带来的性能损耗是不可接受的。这也是k8s v1.0及之前版本中对kube-proxy质疑最大的一点，因此社区就开始研究iptables mode。</p></blockquote><p>详细工作流程：</p><blockquote><p>userspace这种模式下，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；对每个 Service，它都为其在本地节点开放一个端口，作为其服务代理端口；发往该端口的请求会采用一定的策略转发给与该服务对应的后端 Pod 实体。kube-proxy 同时会在本地节点设置 iptables 规则，配置一个 Virtual IP，把发往 Virtual IP 的请求重定向到与该 Virtual IP 对应的服务代理端口上。其工作流程大体如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322231458.png" alt="图片"></p><blockquote><p>【分析】该模式请求在到达 iptables 进行处理时就会进入内核，而 kube-proxy 监听则是在用户态, 请求就形成了从用户态到内核态再返回到用户态的传递过程, 一定程度降低了服务性能。</p></blockquote><h3 id="2）iptables模式（默认模式）-1"><a href="#2）iptables模式（默认模式）-1" class="headerlink" title="2）iptables模式（默认模式）"></a>2）iptables模式（默认模式）</h3><blockquote><p>该模式完全利用内核iptables来实现service的代理和LB, 这是K8s在<strong>v1.2及之后版本默认模式</strong>. 工作原理如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322231525.png" alt="图片"></p><blockquote><p><strong>iptables mode因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗</strong>。另外，如果集群中存在上万的Service&#x2F;Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。这也导致目前大部分企业用k8s上生产时，都不会直接用kube-proxy作为服务代理，而是通过自己开发或者通过Ingress Controller来集成HAProxy, Nginx来代替kube-proxy。</p></blockquote><p>详细工作流程：</p><blockquote><p>iptables 模式与 userspace 相同，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；但它并不在本地节点开启反向代理服务，而是把反向代理全部交给 iptables 来实现；即 iptables 直接将对 VIP 的请求转发给后端 Pod，通过 iptables 设置转发策略。其工作流程大体如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322231458.png" alt="图片"></p><blockquote><p>【分析】 该模式相比 userspace 模式，克服了请求在用户态-内核态反复传递的问题，性能上有所提升，但使用 iptables NAT 来完成转发，存在不可忽视的性能损耗，而且在大规模场景下，iptables 规则的条目会十分巨大，性能上还要再打折扣。</p></blockquote><p>示例:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF &gt; mysql-service.yaml</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Service</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    name: mysql</span></span><br><span class="line"><span class="string">    role: service</span></span><br><span class="line"><span class="string">  name: mysql-service</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  ports:</span></span><br><span class="line"><span class="string">    - port: 3306</span></span><br><span class="line"><span class="string">      targetPort: 3306</span></span><br><span class="line"><span class="string">      nodePort: 30964</span></span><br><span class="line"><span class="string">  type: NodePort</span></span><br><span class="line"><span class="string">  selector:</span></span><br><span class="line"><span class="string">    mysql-service: &quot;true&quot;</span></span><br><span class="line"><span class="string">    name: mysql</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">$ kubectl apply -f mysql-service.yaml</span><br><span class="line">$ kubectl get svc</span><br></pre></td></tr></table></figure><h3 id="3）ipvs模型-1"><a href="#3）ipvs模型-1" class="headerlink" title="3）ipvs模型"></a>3）ipvs模型</h3><blockquote><p>在kubernetes 1.8以上的版本中，对于kube-proxy组件增加了除iptables模式和用户模式之外还支持ipvs模式。<strong>kube-proxy ipvs 是基于 NAT 实现的</strong>，通过ipvs的NAT模式，对访问k8s service的请求进行虚IP到POD IP的转发。当创建一个 service 后，kubernetes 会在每个节点上创建一个网卡，同时帮你将 Service IP(VIP) 绑定上，此时相当于每个 Node 都是一个 ds，而其他任何 Node 上的 Pod，甚至是宿主机服务(比如 kube-apiserver 的 6443)都可能成为 rs；</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322230705.png" alt="ipvs-service模型"></p><p>详细工作流程：</p><blockquote><p>与iptables、userspace 模式一样，kube-proxy 依然监听Service以及Endpoints对象的变化, 不过它并不创建反向代理, 也不创建大量的 iptables 规则, 而是通过netlink 创建ipvs规则，并使用k8s Service与Endpoints信息，对所在节点的ipvs规则进行定期同步; netlink 与 iptables 底层都是基于 netfilter 钩子，但是 netlink 由于采用了 hash table 而且直接工作在内核态，在性能上比 iptables 更优。其工作流程大体如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322230748.png" alt="ipvs工作流程"></p><blockquote><p>【分析】ipvs 是目前 kube-proxy 所支持的最新代理模式，相比使用 iptables，使用 ipvs 具有更高的性能。</p></blockquote><hr><h3 id="4）kube-proxy配置-ipvs模式（所有节点）-1"><a href="#4）kube-proxy配置-ipvs模式（所有节点）-1" class="headerlink" title="4）kube-proxy配置 ipvs模式（所有节点）"></a>4）kube-proxy配置 ipvs模式（所有节点）</h3><p>1、加载ip_vs相关内核模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ modprobe -- ip_vs</span><br><span class="line">$ modprobe -- ip_vs_sh</span><br><span class="line">$ modprobe -- ip_vs_rr</span><br><span class="line">$ modprobe -- ip_vs_wrr</span><br><span class="line">$ modprobe -- nf_conntrack_ipv4</span><br></pre></td></tr></table></figure><p>所有节点验证开启了ipvs：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lsmod |grep ip_vs</span><br></pre></td></tr></table></figure><p>2、安装ipvsadm工具</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install ipset ipvsadm -y</span><br></pre></td></tr></table></figure><p>3、编辑kube-proxy配置文件，mode修改成ipvs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl edit  configmap -n kube-system  kube-proxy</span><br></pre></td></tr></table></figure><p>4、重启kube-proxy<br>先查看之前的kube-proxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy</span><br></pre></td></tr></table></figure><p>删掉上面三个kube-proxy，重新拉起新的服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy |awk <span class="string">&#x27;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&#x27;</span></span><br></pre></td></tr></table></figure><p>再查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy</span><br></pre></td></tr></table></figure><p>5、查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ipvsadm -Ln</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）5、工作负载-Deployment</title>
    <link href="https://blog.yongwang.lu/post/c2d6c004.html"/>
    <id>https://blog.yongwang.lu/post/c2d6c004.html</id>
    <published>2023-01-05T06:40:00.000Z</published>
    <updated>2023-01-05T06:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在 kubernetes 的世界里，Pod 是运行应用的载体。 Pod 是由多个容器组成、是 kubernetes 的最小调度单元、Pod 共享底层资源、由 kubernetes 来管理生命周期。</p></blockquote><blockquote><p>一般情况下，我们并不直接创建 Pod，而是通过 Deployment 来创建 Pod，由 Deployment 来负责创建、更新、维护其所管理的所有 Pods。</p></blockquote><blockquote><p>这里注意并不直接管理pod, 而是通过管理replicaset来间接管理pod，即：deployment管理replicaset，replicaset管理pod。所以deployment比replicaset的功能更强大。</p></blockquote><h2 id="一、Deployment介绍与工作原理"><a href="#一、Deployment介绍与工作原理" class="headerlink" title="一、Deployment介绍与工作原理"></a>一、Deployment介绍与工作原理</h2><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h3><p>Deployment是一种更高阶的工作负载资源。其可以视作为对RS的再一次升级。其不仅可以部署应用还可以通过声明的方式升级应用。具体地，Deployment被创建后，其内部先创建相应的RS资源。再利用该RS资源创建相应的Pod</p><p><img src="https://res.yongwang.lu/img/202303222110977.png" alt="deployment介绍"></p><h3 id="2-工作流程"><a href="#2-工作流程" class="headerlink" title="2.工作流程"></a>2.工作流程</h3><p><img src="https://res.yongwang.lu/img/202303222114591.png" alt="工作流程"><br>图片来源书籍: kubernetes in action</p><p>客户端将创建 Deployment 的请求发送给 Apiserver<br>Apiserver 将 Deployment 信息写入 etcd,etcd 将写入结果响应给 Apiserver,Apiserver 将创建结果响应给客户端 (此时未经过 ControllerManager,deployment 的 READY 状态为 0)<br>ControllerManager 通过 Apiserver 的 watch 接口，获取到新增的 Deployment 资源，Deployment controller 向 Apiserver 发送创建 RS 的请求，Apiserver 将 RS 信息写入 etcd。。。<br>ControllerManager 通过 Apiserver 的 watch 接口，获取到新增的 ReplicaSet 资源，ReplicaSet controller 向 Apiserver 发送创建 Pod 的请求，Apiserver 将 Pod 信息写入 etcd。。。<br>Scheduler 通过 Apiserver 的 watch 接口，获取到未调度的 Pod 的通知，根据调度算法选择一个 node 节点，告诉 Apiserver 这个 Pod 应该运行在哪个节点<br>Apiserver 将这个 Pod 和 node 的绑定信息更新到 etcd,etcd 将写入结果响应给 Apiserver<br>Kubelet 通过 Apiserver 的 watch 接口，获取到当前节点有创建 Pod 的通知，Kubelet 调用 docker 创建容器，Kubelet 将 Pod 运行状态发送给 Apiserver<br>Apiserver 将 Pod 状态信息更新到 etcd</p><h2 id="二、Deployment的使用"><a href="#二、Deployment的使用" class="headerlink" title="二、Deployment的使用"></a>二、Deployment的使用</h2><h3 id="1-创建Deployment"><a href="#1-创建Deployment" class="headerlink" title="1.创建Deployment"></a>1.创建Deployment</h3><p>利用 Deployment 也可以创建 Pod，下面来比较一下两种方式创建 Pod 的区别。</p><blockquote><p>方式一：使用kubectl run 创建 Pod</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run my-nginx --image=nginx:1.20 -n hello-world</span><br></pre></td></tr></table></figure><blockquote><p>方式二：使用 kubectl create deployment 创建 Pod</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment my-nginx-dep --image=nginx:1.20 -n hello-world</span><br></pre></td></tr></table></figure><p>如果此时要删除上面创建的 Pod ，对于方式一创建的，很简单，直接执行下面的这行语句即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod my-nginx -n hello-world</span><br></pre></td></tr></table></figure><p>而要删除方式二创建的 Pod ，我们还能用下面的这行语句吗？</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod my-nginx-dep -n hello-world</span><br></pre></td></tr></table></figure><p>这么做，是删不掉的。因为每删除一次，k8s又会自动创建一个新的 my-nginx-dep 的 Pod（Deployment自愈能力的体现）。</p><p>原因在于方式二是通过 Deployment 的方式创建的 Pod（以部署一个应用的方式）。</p><p>如果要删除方式二创建的 Pod，可以通过下面的步骤进行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一步，获取deploy（部署）列表信息</span></span><br><span class="line">kubectl get deploy -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步，通过删除deploy（部署）的方式删除Pod</span></span><br><span class="line">kubectl delete deploy my-tomcat -n hello-world</span><br></pre></td></tr></table></figure><h3 id="2-Deployment的各种能力"><a href="#2-Deployment的各种能力" class="headerlink" title="2.Deployment的各种能力"></a>2.Deployment的各种能力</h3><h4 id="2-1-多副本能力"><a href="#2-1-多副本能力" class="headerlink" title="2.1 多副本能力"></a>2.1 多副本能力</h4><p>副本：可以理解为对于一个应用而言，将它运行在一个 Pod 上就是起一个副本，将它运行在多个 Pod 上，就是起了多个副本。</p><p>下面通过两种方式，来实现多副本的能力。</p><blockquote><p>方式一：命令行方式</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过部署的方式，创建nginx的3个副本</span></span><br><span class="line">kubectl create deployment nginx-base --image=nginx --replicas=3 -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看部署列表信息</span></span><br><span class="line">kubectl get deploy -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除本次部署（即同时删除3个Pod）</span></span><br><span class="line">kubectl delete deploy nginx-base -n hello-world</span><br></pre></td></tr></table></figure><blockquote><p>方式二：yaml文件方式</p></blockquote><p>（1）写一个 yaml 文件，例如 nginx-base.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span> <span class="comment"># 版本号</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span>    <span class="comment"># 资源类型</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx-base</span> <span class="comment"># 标签名称</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-base</span>  <span class="comment"># 本次部署的名字</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">hello-world</span> <span class="comment"># 命名空间</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span> <span class="comment"># 副本数量</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx-base</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx-base</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx:1.20</span> <span class="comment"># 容器的镜像名称</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">nginx</span>  <span class="comment"># 容器名</span></span><br></pre></td></tr></table></figure><p>（2）执行 nginx-base.yaml 文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx-base.yaml</span><br></pre></td></tr></table></figure><h4 id="2-2-扩缩容能力"><a href="#2-2-扩缩容能力" class="headerlink" title="2.2 扩缩容能力"></a>2.2 扩缩容能力</h4><p>扩容：遇到流量高峰，需要增加副本数量，降低当前负载</p><p>缩容：流量高峰过去，不需要这么多副本，减少数量</p><blockquote><p>方式一：命令行方式</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将nginx-base的副本数量扩容到5个</span></span><br><span class="line">kubectl scale --replicas=5 deployment/nginx-base -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将nginx-base的副本数量缩减到2个</span></span><br><span class="line">kubectl scale --replicas=2 deployment/nginx-base -n hello-world</span><br></pre></td></tr></table></figure><blockquote><p>方式二：修改yaml方式</p></blockquote><p>打开对应的 yaml 文件，修改副本发数量，然后保存退出即可。</p><p>（1）打开文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deploy</span><br><span class="line"></span><br><span class="line">kubectl edit deployment nginx-base</span><br></pre></td></tr></table></figure><p>（2）编辑文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1 <span class="comment"># 版本号</span></span><br><span class="line">kind: Deployment    <span class="comment"># 资源类型</span></span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx-base <span class="comment"># 标签名称</span></span><br><span class="line">  name: nginx-base  <span class="comment"># 本次部署的名字</span></span><br><span class="line">  namespace: hello-world <span class="comment"># 命名空间</span></span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 <span class="comment"># 副本数量</span></span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx-base</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-base</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:1.20 <span class="comment"># 容器的镜像名称</span></span><br><span class="line">        name: nginx  <span class="comment"># 容器名</span></span><br></pre></td></tr></table></figure><p>（3）保存退出</p><h4 id="2-3-自愈及故障转移能力"><a href="#2-3-自愈及故障转移能力" class="headerlink" title="2.3 自愈及故障转移能力"></a>2.3 自愈及故障转移能力</h4><p>自愈能力：假设某个 Pod 发生故障，不能正常对外提供服务，此时 k8s 会自动杀死该 Pod ，然后再将该 Pod 重新启动。</p><p>转移能力：假设某个 Pod 所在的结点 node-01 发生了宕机，整个服务器坏掉，不能正常使用，当 k8s 经过一小段时间（可能是5分钟）检测后，发现该服务器确实不能提供服务了，那么 k8s 会自动在其它结点上重新创建 node-01 结点里面的所有 Pod ，以达到正常对外提供服务的目的。</p><h4 id="2-4-滚动更新能力"><a href="#2-4-滚动更新能力" class="headerlink" title="2.4 滚动更新能力"></a>2.4 滚动更新能力</h4><p>k8s 在更新集群应用时，对于每个结点上的 Pod ，会先启动一个新的 Pod ，然后再停掉对应的老的 Pod，如此循环，直至实现每个结点上的 Pod 的更新，才算完成本次集群应用的更新。</p><blockquote><p>方式一：命令行方式</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">set</span> image deployment/nginx-base nginx=nginx:1.16.1 --record -n hello-world</span><br><span class="line"></span><br><span class="line">kubectl rollout status deployment/nginx-base -n hello-world</span><br></pre></td></tr></table></figure><blockquote><p>方式二：修改yaml方式</p></blockquote><p>打开对应的 yaml 文件，修改副本发数量，然后保存退出即可。</p><p>（1）打开文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deploy</span><br><span class="line"></span><br><span class="line">kubectl edit deployment nginx-base</span><br></pre></td></tr></table></figure><p>（2）编辑文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1 <span class="comment"># 版本号</span></span><br><span class="line">kind: Deployment    <span class="comment"># 资源类型</span></span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx-base <span class="comment"># 标签名称</span></span><br><span class="line">  name: nginx-base  <span class="comment"># 本次部署的名字</span></span><br><span class="line">  namespace: hello-world <span class="comment"># 命名空间</span></span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 <span class="comment"># 副本数量</span></span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx-base</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-base</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:1.16.1 <span class="comment"># 容器的镜像名称</span></span><br><span class="line">        name: nginx  <span class="comment"># 容器名</span></span><br></pre></td></tr></table></figure><p>（3）保存退出</p><h4 id="2-5-版本回退能力"><a href="#2-5-版本回退能力" class="headerlink" title="2.5 版本回退能力"></a>2.5 版本回退能力</h4><p>如果对当前这次部署不满意，无法正常提供服务，k8s可以回滚到之前的历史版本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#历史记录</span></span><br><span class="line">kubectl rollout <span class="built_in">history</span> deployment/nginx-base -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看某个历史详情</span></span><br><span class="line">kubectl rollout <span class="built_in">history</span> deployment/nginx-base --revision=2</span><br><span class="line"></span><br><span class="line"><span class="comment">#回滚到上次版本</span></span><br><span class="line">kubectl rollout undo deployment/nginx-base -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment">#回滚到指定版本</span></span><br><span class="line">kubectl rollout undo deployment/nginx-base --to-revision=2 -n hello-world</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）4、工作负载</title>
    <link href="https://blog.yongwang.lu/post/c56c5900.html"/>
    <id>https://blog.yongwang.lu/post/c56c5900.html</id>
    <published>2023-01-05T05:40:00.000Z</published>
    <updated>2023-01-05T05:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>参考： <a href="https://kubernetes.io/zh/docs/" title="Kubernetes 文档">Kubernetes 文档</a> &#x2F; <a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/" title="工作负载">工作负载</a></p><h2 id="一、kubernetes中的工作负载"><a href="#一、kubernetes中的工作负载" class="headerlink" title="一、kubernetes中的工作负载"></a>一、kubernetes中的工作负载</h2><h3 id="1、k8s中的工作负载"><a href="#1、k8s中的工作负载" class="headerlink" title="1、k8s中的工作负载"></a>1、k8s中的工作负载</h3><p>Kubernetes中内建了很多controller（控制器），这些相当于一个状态机，用来控制Pod的具体状态和行为</p><ul><li><p><code>Deployment</code>：适合无状态的服务部署</p></li><li><p><code>ReplicaSet</code> : 通俗讲副本的管理, 实际应用中建议使用 Deployment 而不是直接使用 ReplicaSet</p></li><li><p><code>StatefullSet</code>：用来管理有状态应用的工作负载 API 对象。</p></li><li><p><code>DaemonSet</code>：一次部署，所有的node节点都会部署，例如一些典型的应用场景：<br>运行集群存储 daemon，例如在每个Node上运行 glusterd、ceph 在每个Node上运行日志收集 daemon，例如 fluentd、 logstash 在每个Node上运行监控 daemon，例如 Prometheus Node Exporter</p></li><li><p><code>Job</code>：一次性的执行任务</p></li><li><p><code>Cronjob</code>：周期性的执行任务</p></li><li><p><code>ReplicationController</code> 确保在任何时候都有特定数量的 Pod 副本处于运行状态。  现在推荐使用配置 ReplicaSet 的 Deployment 来建立副本管理机制。</p></li></ul><h3 id="2、k8s的控制器图示"><a href="#2、k8s的控制器图示" class="headerlink" title="2、k8s的控制器图示"></a>2、k8s的控制器图示</h3><p><img src="https://res.yongwang.lu/img/202303221916960.png" alt="K8s控制器"></p>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）3、Pod详解</title>
    <link href="https://blog.yongwang.lu/post/3664a83f.html"/>
    <id>https://blog.yongwang.lu/post/3664a83f.html</id>
    <published>2023-01-05T04:40:00.000Z</published>
    <updated>2023-01-05T04:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/">https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/</a></p><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><blockquote><p>在Kubernetes集群中，Pod是所有业务类型的基础，也是K8S管理的<strong>最小单位级</strong>，它是<strong>一个或多个容器的组合</strong>。这些容器<strong>共享存储、网络和命名空间</strong>，以及如何运行的规范。在Pod中，所有容器都被统一安排和调度，并运行在共享的上下文中。对于具体应用而言，Pod是它们的逻辑主机，Pod包含业务相关的多个应用容器。</p></blockquote><h2 id="二、Pod实现机制与设计模式"><a href="#二、Pod实现机制与设计模式" class="headerlink" title="二、Pod实现机制与设计模式"></a>二、Pod实现机制与设计模式</h2><blockquote><p>每个Pod都有一个特殊的被称为”根容器”的Pause 容器（Pause容器，又叫Infrastructure容器）。 Pause容器对应的镜像属于Kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或者多个紧密相关的用户业务容器。</p></blockquote><p><img src="https://res.yongwang.lu/img/202303221607453.png" alt="pod组成"></p><blockquote><p>众所周知，容器之间是通过Namespace隔离的，Pod要想解决上述应用场景，那么就要让Pod里的容器之间高效共享。<br>具体分为两个部分：<strong>网络和存储</strong></p></blockquote><ul><li>共享网络</li></ul><blockquote><p>kubernetes的解法是这样的：会在每个Pod里先启动一个infra container小容器，然后让其他的容器连接进来这个网络命名空间，然后其他容器看到的网络试图就完全一样了，即网络设备、IP地址、Mac地址等，这就是解决网络共享问题。在Pod的IP地址就是infra container的IP地址。</p></blockquote><ul><li>共享存储</li></ul><blockquote><p>比如有两个容器，一个是nginx，另一个是普通的容器，普通容器要想访问nginx里的文件，就需要nginx容器将共享目录通过volume挂载出来，然后让普通容器挂载的这个volume，最后大家看到这个共享目录的内容一样。</p></blockquote><p>例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pod-write-read.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span>  </span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-podspec</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">write</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">centos</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;bash&quot;</span>,<span class="string">&quot;-c&quot;</span>,<span class="string">&quot;for i in &#123;1..100&#125;;do echo $i &gt;&gt; /data/hello;sleep 1;done&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">        <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line"> </span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">read</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">centos</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;bash&quot;</span>,<span class="string">&quot;-c&quot;</span>,<span class="string">&quot;tail -f /data/hello&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">        <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">   </span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">    <span class="attr">emptyDir:</span> &#123;&#125;</span><br></pre></td></tr></table></figure><p>上述示例中有两个容器，write容器负责提供数据，read消费数据，通过数据卷将写入数据的目录和读取数据的目录都放到了该卷中，这样每个容器都能看到该目录。<br>验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f pod-write-read.yaml</span><br><span class="line">$ kubectl logs my-pod -c <span class="built_in">read</span> -f</span><br></pre></td></tr></table></figure><p>在Pod中容器分为以下几个类型：</p><ul><li><p>Infrastructure Container：基础容器，维护整个Pod网络空间，对用户不可见</p></li><li><p>InitContainers：初始化容器，先于业务容器开始执行，一般用于业务容器的初始化工作</p></li><li><p>Containers：业务容器，具体跑应用程序的镜像</p></li></ul><h2 id="三、镜像拉取策略"><a href="#三、镜像拉取策略" class="headerlink" title="三、镜像拉取策略"></a>三、镜像拉取策略</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod001</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">busybox001</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">      <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br></pre></td></tr></table></figure><p>imagePullPolicy 字段有三个可选值：</p><ul><li><p>IfNotPresent：镜像在宿主机上不存在时才拉取</p></li><li><p>Always：<strong>默认值</strong>,每次创建 Pod 都会重新拉取一次镜像</p></li><li><p>Never： Pod 永远不会主动拉取这个镜像</p></li></ul><blockquote><p>注意，这里的重启是指在Pod所在Node上面本地重启，并不会调度到其他Node上去。</p></blockquote><h2 id="四、资源限制"><a href="#四、资源限制" class="headerlink" title="四、资源限制"></a>四、资源限制</h2><p>Pod资源配额有两种：</p><p>申请配额：调度时使用，参考是否有节点满足该配置</p><ul><li><p>spec.containers[].resources.limits.cpu</p></li><li><p>spec.containers[].resources.limits.memory</p></li></ul><p>限制配额：容器能使用的最大配置</p><ul><li><p>spec.containers[].resources.requests.cpu</p></li><li><p>spec.containers[].resources.requests.memory</p></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod002</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">busybox002</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;64Mi&quot;</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;250m&quot;</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;128Mi&quot;</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;500m&quot;</span></span><br></pre></td></tr></table></figure><p>其中cpu值比较抽象，可以这么理解：</p><blockquote><p>1核&#x3D;1000m</p><p>1.5核&#x3D;1500m</p></blockquote><p>那上面限制配置就是1核的二分之一（500m），即该容器最大使用半核CPU。</p><p>该值也可以写成浮点数，更容易理解：</p><blockquote><p>半核&#x3D;0.5</p><p>1核&#x3D;1</p><p>1.5核&#x3D;1.5</p></blockquote><h2 id="五、重启策略"><a href="#五、重启策略" class="headerlink" title="五、重启策略"></a>五、重启策略</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod003</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">busybox003</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br></pre></td></tr></table></figure><p>restartPolicy字段有三个可选值：</p><ul><li><p>Always：当容器终止退出后，总是重启容器，<strong>默认策略</strong>。</p></li><li><p>OnFailure：当容器异常退出（退出状态码非0）时，才重启容器。适于job</p></li><li><p>Never：当容器终止退出，从不重启容器。适于job</p></li></ul><h2 id="六、-健康检查"><a href="#六、-健康检查" class="headerlink" title="六、 健康检查"></a>六、 健康检查</h2><blockquote><p>默认情况下，kubelet 根据容器状态作为健康依据，但不能容器中应用程序状态，例如程序假死。这就会导致无法提供服务，丢失流量。因此引入健康检查机制确保容器健康存活。</p></blockquote><p>健康检查有两种类型：</p><ul><li>livenessProbe</li></ul><blockquote><p>如果检查失败，将杀死容器，根据Pod的restartPolicy来操作。</p></blockquote><ul><li>readinessProbe</li></ul><blockquote><p>如果检查失败，Kubernetes会把Pod从service endpoints中剔除。</p></blockquote><p>这两种类型支持三种检查方法：</p><ul><li>httpGet</li></ul><blockquote><p>发送HTTP请求，返回200-400范围状态码为成功。</p></blockquote><ul><li>exec</li></ul><blockquote><p>执行Shell命令返回状态码是0为成功。</p></blockquote><ul><li>tcpSocket</li></ul><blockquote><p>发起TCP Socket建立成功。</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># health-check.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">test:</span> <span class="string">liveness</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-exec</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">liveness</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">touch</span> <span class="string">/tmp/healthy;</span> <span class="string">sleep</span> <span class="number">30</span><span class="string">;</span> <span class="string">rm</span> <span class="string">-rf</span> <span class="string">/tmp/healthy;</span> <span class="string">sleep</span> <span class="number">60</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">exec:</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">cat</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/tmp/healthy</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">5</span></span><br></pre></td></tr></table></figure><p>上述示例：启动容器第一件事在容器内创建文件，停止30s，删除该文件，再停止60s，确保容器还在运行中。</p><p>验证现象：容器启动正常，30s后异常，会restartPolicy策略自动重建，容器继续正常，反复现象。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f health-check.yaml</span><br><span class="line">$ kubectl describe pod liveness-exec</span><br></pre></td></tr></table></figure><h2 id="七、调度策略"><a href="#七、调度策略" class="headerlink" title="七、调度策略"></a>七、调度策略</h2><blockquote><p>先看下创建一个Pod的工作流程： create pod -&gt; apiserver -&gt; write etcd -&gt; scheduler -&gt; bind pod to node -&gt; write etcd -&gt; kubelet( apiserver get pod) -&gt; dcoekr api,create container -&gt; apiserver -&gt; update pod status to etcd -&gt; kubectl get pods</p></blockquote><p><img src="https://res.yongwang.lu/img/202303221620812.png" alt="图片"></p><p>Pod根据调度器默认算法将Pod分配到合适的节点上，一般是比较空闲的节点。但有些情况我们希望将Pod分配到指定节点，该怎么做呢？</p><blockquote><p>这里给你介绍调度策略：<strong>nodeName、nodeSelector和污点</strong></p></blockquote><h3 id="1）nodeName"><a href="#1）nodeName" class="headerlink" title="1）nodeName"></a>1）nodeName</h3><p>nodeName用于将Pod调度到指定的Node名称上。<br>例如：下面示例会绕过调度系统，直接分配到k8s-node1节点。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SchedulePolicy-nodeName.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">busyboxnn</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">k8s-node1</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">bs</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;ping&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;baidu.com&quot;</span></span><br></pre></td></tr></table></figure><p>执行&amp;查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f SchedulePolicy-nodeName.yaml</span><br><span class="line">$ kubectl get pod busyboxnn -o wide</span><br></pre></td></tr></table></figure><h3 id="2）nodeSelector"><a href="#2）nodeSelector" class="headerlink" title="2）nodeSelector"></a>2）nodeSelector</h3><blockquote><p>nodeSelector用于将Pod调度到匹配Label的Node上。先给规划node用途，然后打标签，例如将两台node划分给不同团队使用：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl label nodes k8s-node1 team=a</span><br><span class="line">$ kubectl label nodes k8s-node2 team=b</span><br></pre></td></tr></table></figure><p>后在创建Pod只会被调度到含有team&#x3D;a标签的节点上。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SchedulePolicy-nodeSelector.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">busyboxsn</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">team:</span> <span class="string">b</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">bs</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;ping&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;baidu.com&quot;</span></span><br></pre></td></tr></table></figure><p>执行&amp;查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f SchedulePolicy-nodeSelector.yaml</span><br><span class="line">$ kubectl get pod busyboxsn -o wide</span><br></pre></td></tr></table></figure><h3 id="3）taint（污点）与tolerations（容忍）"><a href="#3）taint（污点）与tolerations（容忍）" class="headerlink" title="3）taint（污点）与tolerations（容忍）"></a>3）taint（污点）与tolerations（容忍）</h3><ul><li>污点应用场景：节点独占，例如具有特殊硬件设备的节点，如GPU<br>设置污点命令：<br>kubectl taint node [node] key&#x3D;value[effect]</li></ul><p>其中[effect] 可取值：</p><ul><li><p>NoSchedule ：一定不能被调度。</p></li><li><p>PreferNoSchedule：尽量不要调度。</p></li><li><p>NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。</p></li></ul><p>示例：</p><p>先给节点设置污点，说明这个节点不是谁都可以调度过来的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint node k8s-node1  abc=123:NoSchedule</span><br></pre></td></tr></table></figure><p>查看污点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe node k8s-node1 |grep Taints</span><br></pre></td></tr></table></figure><blockquote><p>然后在创建Pod<strong>只有声明了容忍污点（tolerations），才允许被调度到abc&#x3D;123污点节点上</strong>。</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SchedulePolicy-tolerations.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">busybox3</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;abc&quot;</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">&quot;Equal&quot;</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;123&quot;</span></span><br><span class="line">    <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">bs</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;ping&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;baidu.com&quot;</span></span><br></pre></td></tr></table></figure><p>如果不配置容忍污点，则永远不会调度到k8s-node1。（也可以叫做<strong>反亲和性</strong>）<br>去掉污点：</p><blockquote><p>kubectl taint node k8s-node1 abc&#x3D;123:NoSchedule</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint node k8s-node1 abc:NoSchedule-</span><br></pre></td></tr></table></figure><p>master节点默认是打了污点标记，不调度的，去掉污点标记</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#添加 尽量不调度 PreferNoSchedule </span></span><br><span class="line">kubectl taint nodes k8s-master node-role.kubernetes.io/master:PreferNoSchedule</span><br><span class="line"><span class="comment">#去除污点NoSchedule，最后一个&quot;-&quot;代表删除</span></span><br><span class="line">kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule-</span><br></pre></td></tr></table></figure><h2 id="八、Pod状态"><a href="#八、Pod状态" class="headerlink" title="八、Pod状态"></a>八、Pod状态</h2><h3 id="1）Pod常见状态"><a href="#1）Pod常见状态" class="headerlink" title="1）Pod常见状态"></a>1）Pod常见状态</h3><p>1、Pending：等待中</p><blockquote><p>Pod已经被创建，但还没有完成调度，或者说有一个或多个镜像正处于从远程仓库下载的过程。处在这个阶段的Pod可能正在写数据到etcd中、调度、pull镜像或启动容器。</p></blockquote><p>2、Running：运行中</p><blockquote><p>该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。</p></blockquote><p>3、Succeeded：正常终止</p><blockquote><p>Pod中的所有的容器已经正常的执行后退出，并且不会自动重启，一般会是在部署job的时候会出现。</p></blockquote><p>4、Failed：异常停止</p><blockquote><p>Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。</p></blockquote><p>5、Terminating 或 Unknown 状态</p><blockquote><p>从 v1.5 开始，Kubernetes 不会因为 Node 失联而删除其上正在运行的 Pod，而是将其标记为 Terminating 或 Unknown 状态。</p></blockquote><p>6、Error 状态</p><blockquote><p>通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括：</p></blockquote><ul><li><p>依赖的 ConfigMap、Secret 或者 PV 等不存在</p></li><li><p>请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等</p></li><li><p>违反集群的安全策略，比如违反了 PodSecurityPolicy 等</p></li><li><p>容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定。</p></li></ul><p>7、Completed状态</p><blockquote><p>状态由ContainerCreating变为Completed再变为CrashLoopBackOff，原因是command: 或args 参数错误无法正常执行。</p></blockquote><p>用一张图来表示Pod的各个状态</p><p><img src="https://res.yongwang.lu/img/202303221614525.png" alt="图片"></p><h3 id="2）Pod-其它状态详细说明"><a href="#2）Pod-其它状态详细说明" class="headerlink" title="2）Pod 其它状态详细说明"></a>2）Pod 其它状态详细说明</h3><table><thead><tr><th>状态</th><th>描述</th></tr></thead><tbody><tr><td>ContainerCreating</td><td>容器创建中</td></tr><tr><td>PodInitializing pod</td><td>初始化中</td></tr><tr><td>CrashLoopBackOff</td><td>容器曾经启动了，但可能又异常退出了，kubelet正在将它重启</td></tr><tr><td>InvalidImageName</td><td>无法解析镜像名称</td></tr><tr><td>ImageInspectError</td><td>无法校验镜像</td></tr><tr><td>ErrImageNeverPull</td><td>策略禁止拉取镜像</td></tr><tr><td>ImagePullBackOff</td><td>正在重试拉取</td></tr><tr><td>RegistryUnavailable</td><td>连接不到镜像中心</td></tr><tr><td>ErrImagePull</td><td>通用的拉取镜像出错</td></tr><tr><td>CreateContainerConfigError</td><td>不能创建kubelet使用的容器配置</td></tr><tr><td>CreateContainerError</td><td>创建容器失败</td></tr><tr><td>m.internalLifecycle.PreStartContainer</td><td>执行hook报错</td></tr><tr><td>RunContainerError</td><td>启动容器失败</td></tr><tr><td>PostStartHookError</td><td>执行hook报错</td></tr><tr><td>ContainersNotInitialized</td><td>容器没有初始化完毕</td></tr><tr><td>ContainersNotRead</td><td>容器没有准备完毕</td></tr><tr><td>DockerDaemonNotReady</td><td>docker还没有完全启动</td></tr><tr><td>NetworkPluginNotReady</td><td>网络插件还没有完全启动</td></tr></tbody></table><p>pod启动后停止问题总结（ContainerCreating-》Completed-》CrashLoopBackOff）：</p><blockquote><p>pod 是否能持续运行，是由执行命令决定的，执行命令如果一执行就停止，控制台也停止持续输出，pod生命周期就结束，pod状态就会变成CrashLoopBackOff。</p></blockquote>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）2、集群搭建(containerd)</title>
    <link href="https://blog.yongwang.lu/post/9866a7eb.html"/>
    <id>https://blog.yongwang.lu/post/9866a7eb.html</id>
    <published>2023-01-05T03:40:00.000Z</published>
    <updated>2023-01-05T03:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h1><p>参考： <a href="https://kubernetes.io/zh/docs/" title="Kubernetes 文档">Kubernetes 文档</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/" title="入门">入门</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/production-environment/" title="生产环境">生产环境</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/" title="使用部署工具安装 Kubernetes">使用部署工具安装 Kubernetes</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/" title="使用 kubeadm 引导集群">使用 kubeadm 引导集群</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" title="安装 kubeadm">安装 kubeadm</a></p><p>流程图</p><p><img src="https://res.yongwang.lu/img/image_rFtNPTs_40.png"></p><h2 id="准备开始"><a href="#准备开始" class="headerlink" title="准备开始"></a>准备开始</h2><ul><li>一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令</li><li>每台机器 <code>2 GB</code> 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)</li><li><code>2 CPU 核</code>或更多</li><li>集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)</li><li>节点之中<code>不可以有重复的</code>主机名、MAC 地址或 product_uuid。请参见<a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#verify-mac-address" title="这里">这里</a>了解更多详细信息。</li><li>开启机器上的某些端口。请参见<a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports" title="这里">这里</a> 了解更多详细信息。</li><li>禁用交换分区。为了保证 kubelet 正常工作，你 <strong>必须</strong> <code>禁用交换分区</code>。</li></ul><h2 id="U-确保每个节点上-MAC-地址和-product-uuid-的唯一性-amp-x20"><a href="#U-确保每个节点上-MAC-地址和-product-uuid-的唯一性-amp-x20" class="headerlink" title="U. 确保每个节点上 MAC 地址和 product_uuid 的唯一性&amp;#x20;"></a>U. 确保每个节点上 MAC 地址和 product_uuid 的唯一性&amp;#x20;</h2><ul><li>你可以使用命令 <code>ip link</code> 或 <code>ifconfig -a</code> 来获取网络接口的 MAC 地址</li><li>可以使用 <code>sudo cat /sys/class/dmi/id/product_uuid</code> 命令对 product_uuid 校验</li></ul><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p><strong>[k8s-master|k8s-worker1|k8s-worker2]$</strong></p><ol><li>设置当前用户sudo免密[选做]<blockquote><p>不想每次都输入密码 - 加速</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 缓存 sudo 密码</span></span><br><span class="line"><span class="built_in">echo</span> ubuntu | sudo -v -S </span><br><span class="line"></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">sudo <span class="built_in">tee</span> /etc/sudoers.d/<span class="variable">$USER</span> &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">$USER ALL=(ALL) NOPASSWD: ALL</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li>使用国内镜像仓库[选做]<blockquote><p>软件安装 - 加速</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更换阿里云加速</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cp</span> /etc/apt/sources.list /etc/apt/sources.list_bak</span><br><span class="line">vim /etc/apt/sources.list</span><br><span class="line"><span class="comment"># 查看/etc/apt/sources.list中的URL是archive.ubuntu还是cn.archive.ubuntu </span></span><br><span class="line"><span class="comment"># 然后再执行：</span></span><br><span class="line">sudo sed -i <span class="string">&#x27;s/cn.archive.ubuntu.com/mirrors.aliyun.com/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line"><span class="comment"># 或 </span></span><br><span class="line">sudo sed -i <span class="string">&#x27;s/archive.ubuntu.com/mirrors.aliyun.com/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>编辑 hosts&lt;必做&gt; <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">tee</span> -a /etc/hosts &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">192.168.147.131 k8s-master</span></span><br><span class="line"><span class="string">192.168.147.132 k8s-worker1</span></span><br><span class="line"><span class="string">192.168.147.133 k8s-worker2</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li></ol><p><strong>[k8s-master|k8s-worker1|k8s-worker2]$</strong></p><ol><li><p>禁用 swap&lt;必做&gt;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交换文件</span></span><br><span class="line">SWAPF=$(awk <span class="string">&#x27;/swap/ &#123;print $1&#125;&#x27;</span> /etc/fstab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即禁用</span></span><br><span class="line">sudo swapoff <span class="variable">$SWAPF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 永久禁用</span></span><br><span class="line">sudo sed -i <span class="string">&#x27;/swap/d&#x27;</span> /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除交换文件</span></span><br><span class="line">sudo <span class="built_in">rm</span> <span class="variable">$SWAPF</span></span><br></pre></td></tr></table></figure></li><li><p>模块支持&lt;必做&gt;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">sudo apt -y install bridge-utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即生效</span></span><br><span class="line">sudo modprobe br_netfilter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 内核支持</span></span><br><span class="line">sudo <span class="built_in">tee</span> /etc/sysctl.d/k8s.conf &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">net.ipv4.ip_forward=1</span></span><br><span class="line"><span class="string">vm.swappiness=0</span></span><br><span class="line"><span class="string">vm.overcommit_memory=1</span></span><br><span class="line"><span class="string">vm.panic_on_oom=0</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即生效</span></span><br><span class="line">sudo sysctl -p /etc/sysctl.d/k8s.conf</span><br></pre></td></tr></table></figure></li><li><p>安装运行时&lt;必做&gt;&amp;#x20;</p><p>这里不再采用docker作为k8s的运行时. 因为K8s自1.24 对docker支持改为 安装指定CRI才能访问.&amp;#x20;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 containerd https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/</span></span><br><span class="line"><span class="comment"># 下载最新版本 containerd 因为在k8s后续版本废弃1.5.x 安装对应的deb版本</span></span><br><span class="line">sudo apt install -y containerd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锁定版本</span></span><br><span class="line">sudo apt-mark hold containerd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line">sudo <span class="built_in">mkdir</span> /etc/containerd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成默认配置文件</span></span><br><span class="line">containerd config default | \</span><br><span class="line">sudo <span class="built_in">tee</span> /etc/containerd/config.toml &gt;/dev/null</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改配置文件 加速</span></span><br><span class="line">sudo sed -i \</span><br><span class="line">-e <span class="string">&#x27;/sandbox_image/s?k8s.gcr.io?registry.aliyuncs.com/google_containers?&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/SystemdCgroup/s?false?true?&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/registry.mirrors/a\        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/registry.mirrors/a\          endpoint = [&quot;https://docker.nju.edu.cn/&quot;]&#x27;</span> /etc/containerd/config.toml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 服务重启</span></span><br><span class="line">sudo systemctl restart containerd</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行时配置私服</span></span><br><span class="line">[plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry]</span><br><span class="line">    [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors]</span><br><span class="line">        [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors.<span class="string">&quot;docker.io&quot;</span>]</span><br><span class="line">          endpoint = [<span class="string">&quot;https://------.mirror.aliyuncs.com&quot;</span>, <span class="string">&quot;https://registry-1.docker.io&quot;</span>]</span><br><span class="line"><span class="comment"># 运行时配置登录</span></span><br><span class="line">[plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry]</span><br><span class="line">   [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors]</span><br><span class="line">       [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors.<span class="string">&quot;docker.io&quot;</span>]</span><br><span class="line">          endpoint = [<span class="string">&quot;https://registry-1.docker.io&quot;</span>]</span><br><span class="line">       [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors.<span class="string">&quot;registry.cn-hangzhou.aliyuncs.com&quot;</span>]</span><br><span class="line">          endpoint = [<span class="string">&quot;https://registry.cn-hangzhou.aliyuncs.com&quot;</span>]</span><br><span class="line">      [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.configs]</span><br><span class="line">        [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.configs.<span class="string">&quot;registry.cn-hangzhou.aliyuncs.com&quot;</span>.tls]</span><br><span class="line">          insecure_skip_verify = <span class="literal">true</span></span><br><span class="line">        [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.configs.<span class="string">&quot;registry.cn-hangzhou.aliyuncs.com&quot;</span>.auth]</span><br><span class="line">          username = <span class="string">&quot;阿里云账户，类似xxx@aliyun.com&quot;</span></span><br><span class="line">          password = <span class="string">&quot;上一步设置的固定密码&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h2 id="安装-K8s"><a href="#安装-K8s" class="headerlink" title="安装 K8s"></a>安装 K8s</h2><p><strong>[kiosk@k8s-master|k8s-worker1|k8s-worker2]$</strong></p><ol><li>安装 kubeadm、kubelet 和 kubectl<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新 apt 包索引并安装使用 Kubernetes apt 仓库所需要的包</span></span><br><span class="line">sudo apt -y install apt-transport-https ca-certificates curl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 Google Cloud 公开签名秘钥</span></span><br><span class="line">curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 Kubernetes apt 仓库</span></span><br><span class="line">MIRROR_URL=https://mirrors.aliyun.com/kubernetes/apt/</span><br><span class="line">sudo <span class="built_in">tee</span> /etc/apt/sources.list.d/kubernetes.list &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">deb $MIRROR_URL kubernetes-xenial main</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新 apt 包索引</span></span><br><span class="line">sudo <span class="built_in">cp</span> /etc/apt/trusted.gpg /etc/apt/trusted.gpg.d</span><br><span class="line">sudo apt update -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询指定版本</span></span><br><span class="line">sudo apt-cache madison kubelet | grep 1.25</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 kubelet、kubeadm 和 kubectl 指定版本</span></span><br><span class="line">sudo apt install -y kubelet=1.25.1-00 kubeadm=1.25.1-00 kubectl=1.25.1-00</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锁定版本</span></span><br><span class="line">sudo apt-mark hold kubelet kubeadm kubectl</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装完K8s以后会自带crictl</span></span><br><span class="line"><span class="comment"># crictl 配置文件</span></span><br><span class="line">sudo <span class="built_in">tee</span> /etc/crictl.yaml &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">runtime-endpoint: unix:///run/containerd/containerd.sock</span></span><br><span class="line"><span class="string">image-endpoint: unix:///run/containerd/containerd.sock</span></span><br><span class="line"><span class="string">timeout: 10</span></span><br><span class="line"><span class="string">debug: false</span></span><br><span class="line"><span class="string">pull-image-on-create: true</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li>k8s 支持<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加 k8s 支持</span></span><br><span class="line">sudo sed -i <span class="string">&#x27;/ExecStart=\//s|$| --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock --cgroup-driver=systemd|&#x27;</span> \</span><br><span class="line">  /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启 kubelet 服务</span></span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart kubelet</span><br></pre></td></tr></table></figure></li></ol><p><strong>[k8s-master]$</strong></p><ol><li>初始化<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成初始文件</span></span><br><span class="line">sudo kubeadm config <span class="built_in">print</span> init-defaults &gt; kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改文件</span></span><br><span class="line">NICP=$(ip a | awk <span class="string">&#x27;/inet / &#123;print $2&#125;&#x27;</span> | grep -v ^127 | sed <span class="string">&#x27;s+/24++&#x27;</span>)</span><br><span class="line">sudo sed -i \</span><br><span class="line">  -e <span class="string">&quot;/advertiseAddress/s?:.*?: <span class="variable">$NICP</span>?&quot;</span> \</span><br><span class="line">  -e <span class="string">&quot;/name/s?:.*?: <span class="subst">$(hostname -s)</span>?&quot;</span> \</span><br><span class="line">  -e <span class="string">&quot;/clusterName/s?:.*?: k8s?&quot;</span> \</span><br><span class="line">  -e <span class="string">&quot;/imageRepository/s?:.*?: registry.aliyuncs.com/google_containers?&quot;</span> kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用初始文件，初始化集群</span></span><br><span class="line">sudo kubeadm init --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><blockquote><p>Your Kubernetes control-plane has initialized <code>successfully</code>!<br>PS: 普通用户管理集群</p><p>To start using your cluster, you need to run the following as a regular user:<br>bash<br>mkdir -p $HOME&#x2F;.kube<br>sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config<br>sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</p><p>PS：root 用户管理集群</p><p>Alternatively, if you are the root user, you can run:</p><p>bash<br>export KUBECONFIG&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;admin.conf</p><p>You should now deploy a pod network to the cluster.Run “<code>kubectl apply -f [podnetwork].yaml</code>“ with one of the options listed at:<a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/" title="https://kubernetes.io/docs/concepts/cluster-administration/addons/">https://kubernetes.io/docs/concepts/cluster-administration/addons/</a><br>Then you can join any number of worker nodes by running the following on each as root:<br>bash<br>kubeadm join 192.168.147.128:6443 –token abcdef.0123456789abcdef –discovery-token-ca-cert-hash sha256:c4781194de65ebb47984fc5e7e64d4897875410825ce4d18df81da1a298afa1f</p></blockquote></li><li>配置文件 - Client<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line"></span><br><span class="line"><span class="comment"># user 复制配置文件</span></span><br><span class="line">sudo \<span class="built_in">cp</span> /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># root 变量</span></span><br><span class="line">sudo <span class="built_in">tee</span> -a ~root/.bashrc &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">export KUBECONFIG=/etc/kubernetes/admin.conf</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li>创建网络<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</span><br></pre></td></tr></table></figure></li><li>命令补全 - Client[建议]<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl completion --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即生效</span></span><br><span class="line"><span class="built_in">source</span> &lt;(kubectl completion bash)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 永久生效</span></span><br><span class="line"><span class="built_in">mkdir</span> ~/.kube 2&gt;/dev/null</span><br><span class="line">kubectl completion bash &gt; ~/.kube/completion.bash.inc</span><br><span class="line"><span class="built_in">printf</span> <span class="string">&quot;</span></span><br><span class="line"><span class="string"># Kubectl shell completion</span></span><br><span class="line"><span class="string">source &#x27;<span class="variable">$HOME</span>/.kube/completion.bash.inc&#x27;</span></span><br><span class="line"><span class="string">&quot;</span> &gt;&gt; <span class="variable">$HOME</span>/.bashrc</span><br><span class="line"><span class="built_in">source</span> <span class="variable">$HOME</span>/.bashrc</span><br></pre></td></tr></table></figure></li><li>命令别名 - Client[建议]<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网址 https://kubernetes.io/zh-cn/docs/reference/kubectl/cheatsheet/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 永久生效</span></span><br><span class="line"><span class="built_in">tee</span> -a <span class="variable">$HOME</span>/.bashrc &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">alias k=&#x27;kubectl&#x27;</span></span><br><span class="line"><span class="string">complete -F __start_kubectl k</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即生效</span></span><br><span class="line"><span class="built_in">source</span> <span class="variable">$HOME</span>/.bashrc</span><br></pre></td></tr></table></figure></li></ol><p><strong>[k8s-worker1|k8s-worker2]$</strong></p><ol><li>加入集群<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo \</span><br><span class="line">  kubeadm <span class="built_in">join</span> 192.168.147.128:6443 \</span><br><span class="line">  --token abcdef.0123456789abcdef \</span><br><span class="line">  --discovery-token-ca-cert-hash sha256:c4781194de65ebb47984fc5e7e64d4897875410825ce4d18df81da1a298afa1f</span><br></pre></td></tr></table></figure></li></ol><h2 id="C-确认环境正常"><a href="#C-确认环境正常" class="headerlink" title="C. 确认环境正常"></a>C. 确认环境正常</h2><p><strong>[k8s-master]</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME          STATUS     ROLES           AGE     VERSION</span><br><span class="line">k8s-master   `Ready`     control-plane   9m17s   `v1.25.1`</span><br><span class="line">k8s-worker1  `Ready`     &lt;none&gt;          90s     `v1.25.1`</span><br><span class="line">k8s-worker2  `Ready`     &lt;none&gt;          51s     `v1.25.1`</span><br><span class="line"></span><br><span class="line">$ kubectl get componentstatuses</span><br><span class="line">Warning: v1 ComponentStatus is deprecated <span class="keyword">in</span> v1.19+</span><br><span class="line">NAME                 STATUS    MESSAGE                         ERROR</span><br><span class="line">scheduler           `Healthy`  ok</span><br><span class="line">controller-manager  `Healthy`  ok</span><br><span class="line">etcd-0              `Healthy`  &#123;<span class="string">&quot;health&quot;</span>:<span class="string">&quot;true&quot;</span>,<span class="string">&quot;reason&quot;</span>:<span class="string">&quot;&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">$ kubectl -n kube-system get pod -w</span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">calico-kube-controllers-798cc86c47-dkdjl   1/1     Running   0          4m5s</span><br><span class="line">calico-node-ftwk8                          1/1     Running   0          4m5s</span><br><span class="line">calico-node-hstcg                          1/1     Running   0          109s</span><br><span class="line">calico-node-lcnw6                          1/1     Running   0          2m28s</span><br><span class="line">coredns-c676cc86f-mxpb8                    1/1     Running   0          10m</span><br><span class="line">coredns-c676cc86f-vhzzh                    1/1     Running   0          10m</span><br><span class="line">etcd-k8s-master                            1/1     Running   0          10m</span><br><span class="line">kube-apiserver-k8s-master                  1/1     Running   0          10m</span><br><span class="line">kube-controller-manager-k8s-master         1/1     Running   0          10m</span><br><span class="line">kube-proxy-g2tz9                           1/1     Running   0          109s</span><br><span class="line">kube-proxy-j4fgc                           1/1     Running   0          10m</span><br><span class="line">kube-proxy-nz8vj                           1/1     Running   0          2m28s</span><br><span class="line">kube-scheduler-k8s-master                  1/1     Running   0          10m</span><br><span class="line">&lt;Ctrl-C&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）1、基本介绍和功能架构</title>
    <link href="https://blog.yongwang.lu/post/700b7316.html"/>
    <id>https://blog.yongwang.lu/post/700b7316.html</id>
    <published>2023-01-05T02:40:00.000Z</published>
    <updated>2023-01-05T02:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>参考： <a href="https://kubernetes.io/zh/docs/" title="Kubernetes 文档">Kubernetes 文档</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/" title="入门">入门</a> &#x2F; <a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/" title="K8s架构">K8s架构</a></p><h2 id="1、kubernetes-概述"><a href="#1、kubernetes-概述" class="headerlink" title="1、kubernetes 概述"></a>1、kubernetes 概述</h2><p>kubernetes，简称 <a href="https://so.csdn.net/so/search?q=K8s&spm=1001.2101.3001.7020">K8s</a>，是用 8 代替 8 个字符“ubernete”而成的缩写。是一个开源 的，用于管理云平台中多个主机上的容器化的应用，Kubernetes 的目标是让部署容器化的 应用简单并且高效(powerful),Kubernetes 提供了应用部署，规划，更新，维护的一种 机制。</p><p>传统的应用部署方式是通过插件或脚本来安装应用。这样做的缺点是应用的运行、配 置、管理、所有生存周期将与当前操作系统绑定，这样做并不利于应用的升级更新&#x2F;回滚等 操作，当然也可以通过<a href="https://so.csdn.net/so/search?q=%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA&spm=1001.2101.3001.7020">创建虚拟机</a>的方式来实现某些功能，但是虚拟机非常重，并不利于 可移植性。</p><p>新的方式是通过部署容器方式实现，每个容器之间互相隔离，每个容器有自己的文件 系统 ，容器之间进程不会相互影响，能区分计算资源。相对于虚拟机，容器能快速部署， 由于容器与底层设施、机器文件系统解耦的，所以它能在不同云、不同版本操作系统间进 行迁移。</p><p>容器占用资源少、部署快，每个应用可以被打包成一个容器镜像，每个应用与容器间 成一对一关系也使容器有更大优势，使用容器可以在 build 或 release 的阶段，为应用创 建容器镜像，因为每个应用不需要与其余的应用堆栈组合，也不依赖于生产环境基础结构， 这使得从研发到测试、生产能提供一致环境。类似地，容器比虚拟机轻量、更“透明”， 这更便于监控和管理。</p><p>Kubernetes 是 Google 开源的一个容器编排引擎，它支持自动化部署、大规模可伸缩、 应用容器化管理。在生产环境中部署一个应用程序时，通常要部署该应用的多个实例以便 对应用请求进行负载均衡。</p><p>在 Kubernetes 中，我们可以创建多个容器，每个容器里面运行一个应用实例，然后通 过内置的负载均衡策略，实现对这一组应用实例的管理、发现、访问，而这些细节都不需 要运维人员去进行复杂的手工配置和处理。</p><h2 id="2、kubernetes-功能和架构"><a href="#2、kubernetes-功能和架构" class="headerlink" title="2、kubernetes 功能和架构"></a>2、kubernetes 功能和架构</h2><h3 id="2-1、概述"><a href="#2-1、概述" class="headerlink" title="2.1、概述"></a>2.1、概述</h3><p>Kubernetes 是一个轻便的和可扩展的开源平台，用于管理容器化应用和服务。通过 Kubernetes 能够进行应用的自动化部署和扩缩容。在 Kubernetes 中，会将组成应用的容 器组合成一个逻辑单元以更易管理和发现。Kubernetes 积累了作为 Google 生产环境运行 工作负载 15 年的经验，并吸收了来自于社区的最佳想法和实践。</p><h3 id="2-2、K8s-特性"><a href="#2-2、K8s-特性" class="headerlink" title="2.2、K8s 特性"></a>2.2、K8s 特性</h3><ul><li>自动化装箱：在不牺牲可用性的条件下，基于容器对资源的要求和约束自动部署容器。同时，为了提高利用率和节省更多资源，将关键和最佳工作量结合在一起。</li><li>自愈能力：当容器失败时，会对容器进行重启；当所部署的Node节点有问题时，会对容器进行重新部署和重新调度；当容器未通过监控检查时，会关闭此容器；直到容器正常运行时，才会对外提供服务。</li><li>水平扩容：通过简单的命令、用户界面或基于CPU的使用情况，能够对应用进行扩容和缩容。</li><li>服务发现和负载均衡：开发者不需要使用额外的服务发现机制，就能够基于Kubernetes进行服务发现和负载均衡。</li><li>自动发布和回滚：Kubernetes能够程序化的发布应用和相关的配置。如果发布有问题，Kubernetes将能够回归发生的变更。</li><li>保密和配置管理：在不需要重新构建镜像的情况下，可以部署和更新保密和应用配置。</li><li>存储编排：自动挂接存储系统，这些存储系统可以来自于本地、公共云提供商（例如：GCP和AWS）、网络存储(例如：NFS、iSCSI、Gluster、Ceph、Cinder和Floker等)。</li></ul><h3 id="2-3、k8s组件"><a href="#2-3、k8s组件" class="headerlink" title="2.3、k8s组件"></a>2.3、k8s组件</h3><p><img src="https://res.yongwang.lu/img/202303221859050.png" alt="k8s组件架构"></p><p>集群中总体分Master节点和Worker节点</p><p>Master节点对应上图控制平面组件（Control Plane Components）相当于组织部长，负责调度分配，不干具体的事。</p><p>Worker节点对应Node，就是实际干活的人</p><p>2.4、master节点下组件</p><ul><li>kube-apiserver</li></ul><p>kube-apiserver是整个集群的入口，他就像个网关，要访问集群中的组件，必须通过他。</p><ul><li>etcd</li></ul><p>一致且高度可用的键值存储，用作 Kubernetes 的所有集群数据的后台数据库。</p><ul><li>kube-scheduler</li></ul><p>负责监视新创建的、未指定运行<a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/" title="节点（node）">节点（node）</a>的 <a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/" title="Pods">Pods</a>， 并选择节点来让 Pod 在上面运行。</p><ul><li>kube-controller-manager</li></ul><p>处理集群中常规后台任务，一个资源对应一个控制器</p><p>2.5、worker节点下组件</p><ul><li>kubelet</li></ul><p><code>kubelet</code>会在集群中每个<a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/" title="节点（node）">节点（node）</a>上运行。 它保证<a href="https://kubernetes.io/zh-cn/docs/concepts/overview/what-is-kubernetes/#why-containers" title="容器（containers）">容器（containers）</a>都运行在 <a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/" title="Pod">Pod</a> 中。kubelet 接收一组通过各类机制提供给它的 PodSpecs， 确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。</p><p>说白了就是节点的控制器，负责管理本节点容器。</p><ul><li>kube-proxy</li></ul><p><a href="https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-proxy/" title="kube-proxy">kube-proxy</a> 是集群中每个<a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/" title="节点（node）">节点（node）</a>上所运行的网络代理， 实现 Kubernetes <a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/" title="服务（Service）">服务（Service）</a> 概念的一部分。kube-proxy 维护节点上的一些网络规则， 这些网络规则会允许从集群内部或外部的网络会话与 Pod 进行网络通信。</p><p>至此，我相信朋友们对kubernetes有了一个大致的了解，下篇我们就开始搭建kubernetes集群</p>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Containerd ctr 和 crictl 客户端命令介绍与实战操作（nerdctl ）</title>
    <link href="https://blog.yongwang.lu/post/a657fa03.html"/>
    <id>https://blog.yongwang.lu/post/a657fa03.html</id>
    <published>2023-01-02T05:05:21.000Z</published>
    <updated>2023-01-02T05:05:21.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><blockquote><p>作为接替Docker运行时的<a href="https://containerd.io/">Containerd</a>在早在Kubernetes1.7时就能直接与Kubelet集成使用，只是大部分时候我们因熟悉Docker，在部署集群时采用了默认的dockershim。在<code>V1.24</code>起的版本的kubelet就彻底移除了<code>dockershim</code>，改为默认使用<code>Containerd</code>了，当然也可以使用 <code>cri-dockerd</code> 适配器来将 <code>Docker Engine</code> 与 Kubernetes 集成。可以参考官方文档。</p></blockquote><p><img src="https://res.yongwang.lu/img/5da250511cf72797f2e2e992bd47240f-2.png" alt="图片"></p><h2 id="二、Containerd-常见命令操作"><a href="#二、Containerd-常见命令操作" class="headerlink" title="二、Containerd 常见命令操作"></a>二、Containerd 常见命令操作</h2><blockquote><p>更换Containerd后，以往我们常用的docker命令也不再使用，取而代之的分别是 <code>crictl</code> 和 <code>ctr</code> 两个命令客户端。</p></blockquote><ul><li><p><code>crictl</code> 是遵循CRI接口规范的一个命令行工具，通常用它来检查和管理<code>kubelet</code>节点上的容器运行时和镜像。</p></li><li><p><code>ctr</code> 是 <code>containerd</code> 的一个客户端工具。</p></li><li><p><code>ctr -v</code> 输出的是 <code>containerd</code> 的版本，<code>crictl -v</code> 输出的是当前 k8s 的版本，从结果显而易见你可以认为 <code>crictl</code> 是用于 <code>k8s</code> 的。</p></li><li><p>一般来说你某个主机安装了 k8s 后，命令行才会有 crictl 命令。而 ctr 是跟 k8s 无关的，你主机安装了 containerd 服务后就可以操作 ctr 命令。</p></li></ul><p>使用<code>crictl</code>命令之前，需要先配置<code>/etc/crictl.yaml</code>如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runtime-endpoint: unix:///run/containerd/containerd.sockimage-endpoint: unix:///run/containerd/containerd.socktimeout: 10debug: false</span><br></pre></td></tr></table></figure><p>也可以通过命令进行设置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl config runtime-endpoint unix:///run/containerd/containerd.sockcrictl config image-endpoint unix:///run/containerd/containerd.sock</span><br></pre></td></tr></table></figure><table><thead><tr><th>命令</th><th>docker</th><th>ctr（containerd）</th><th>crictl（kubernetes）</th></tr></thead><tbody><tr><td>查看运行的容器</td><td>docker ps</td><td>ctr task ls&#x2F;ctr container ls</td><td>crictl ps</td></tr><tr><td>查看镜像</td><td>docker images</td><td>ctr image ls</td><td>crictl images</td></tr><tr><td>查看容器日志</td><td>docker logs</td><td>无</td><td>crictl logs</td></tr><tr><td>查看容器数据信息</td><td>docker inspect</td><td>ctr container info</td><td>crictl inspect</td></tr><tr><td>查看容器资源</td><td>docker stats</td><td>无</td><td>crictl stats</td></tr><tr><td>启动&#x2F;关闭已有的容器</td><td>docker start&#x2F;stop</td><td>ctr task start&#x2F;kill</td><td>crictl start&#x2F;stop</td></tr><tr><td>运行一个新的容器</td><td>docker run</td><td>ctr run</td><td>无（最小单元为pod）</td></tr><tr><td>打标签</td><td>docker tag</td><td>ctr image tag</td><td>无</td></tr><tr><td>创建一个新的容器</td><td>docker create</td><td>ctr container create</td><td>crictl create</td></tr><tr><td>导入镜像</td><td>docker load</td><td>ctr image import</td><td>无</td></tr><tr><td>导出镜像</td><td>docker save</td><td>ctr image export</td><td>无</td></tr><tr><td>删除容器</td><td>docker rm</td><td>ctr container rm</td><td>crictl rm</td></tr><tr><td>删除镜像</td><td>docker rmi</td><td>ctr image rm</td><td>crictl rmi</td></tr><tr><td>拉取镜像</td><td>docker pull</td><td>ctr image pull</td><td>ctictl pull</td></tr><tr><td>推送镜像</td><td>docker push</td><td>ctr image push</td><td>无</td></tr><tr><td>登录或在容器内部执行命令</td><td>docker exec</td><td>无</td><td>crictl exec</td></tr><tr><td>清空不用的容器</td><td>docker image prune</td><td>无</td><td>crictl rmi –prune</td></tr></tbody></table><p>更多命令操作，可以直接在命令行输入命令查看帮助。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker --help</span><br><span class="line">ctr --help</span><br><span class="line">crictl --help</span><br></pre></td></tr></table></figure><p>由于Containerd也有namespaces的概念，对于上层编排系统的支持，<code>ctr</code> 客户端 主要区分了3个命名空间分别是<code>k8s.io</code>、<code>moby</code>和<code>default</code>，以上我们用<code>crictl</code>操作的均在<code>k8s.io</code>命名空间，使用<code>ctr</code> 看镜像列表就需要加上-n参数。crictl是只有一个<code>k8s.io</code>命名空间，但是没有-n参数。</p><blockquote><p>【温馨提示】ctr images pull 拉取的镜像默认放在<code>default</code>，而crictl pull 和 kubelet 默认拉取的镜像都在k8s.io命名空间下。所以通过<code>ctr</code>导入镜像的时候特别注意一点，最好指定命名空间。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 注意-n不能放在命令最后面，下面几行查看的镜像是一样的</span><br><span class="line">ctr -n=k8s.io image ls</span><br><span class="line">ctr -n k8s.io image ls</span><br><span class="line"></span><br><span class="line"># crictl 没有-n参数，操作都在`k8s.io`命名空间下。</span><br><span class="line">crictl image ls</span><br><span class="line">crictl images</span><br><span class="line"># crictl image list = ctr -n=k8s.io image list</span><br><span class="line"># crictl image ls = ctr -n=k8s.io image ls</span><br><span class="line"># crictl images = ctr -n=k8s.io image list</span><br><span class="line"># crictl images = ctr -n=k8s.io image ls</span><br><span class="line"></span><br><span class="line"># 使用ctr命令指定命名空间导入镜像</span><br><span class="line">ctr -n=k8s.io image import dashboard.tar</span><br><span class="line"></span><br><span class="line">#查看镜像，可以看到可以查询到了</span><br><span class="line">crictl images</span><br></pre></td></tr></table></figure><h2 id="三、container-客户端工具-nerdctl"><a href="#三、container-客户端工具-nerdctl" class="headerlink" title="三、container 客户端工具 nerdctl"></a>三、container 客户端工具 nerdctl</h2><p>推荐使用nerdctl，使用效果与docker命令的语法一致<br>github下载链接：<a href="https://github.com/containerd/nerdctl/releases">https://github.com/containerd/nerdctl/releases</a></p><ul><li><p>精简 (nerdctl-</p><p>-linux-amd64.tar.gz): 只包含nerdctl</p></li><li><p>完整 (nerdctl-full-</p><p>-linux-amd64.tar.gz): 包含 containerd, runc, and CNI等依赖</p></li></ul><blockquote><p><code>nerdctl</code> 的目标并不是单纯地复制 docker 的功能，它还实现了很多 docker 不具备的功能，例如延迟拉取镜像（lazy-pulling）、镜像加密（imgcrypt）等。具体看nerdctl。</p></blockquote><h3 id="1）安装-nerdctl（精简版）"><a href="#1）安装-nerdctl（精简版）" class="headerlink" title="1）安装 nerdctl（精简版）"></a>1）安装 nerdctl（精简版）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/containerd/nerdctl/releases/download/v0.22.2/nerdctl-0.22.2-linux-amd64.tar.gz</span><br><span class="line"><span class="comment"># 解压</span></span><br><span class="line">tar -xf nerdctl-0.22.2-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="built_in">ln</span> -s /opt/k8s/nerdctl/nerdctl /usr/local/bin/nerdctl</span><br></pre></td></tr></table></figure><h3 id="2）安装-nerdctl（完整版，这里不装）"><a href="#2）安装-nerdctl（完整版，这里不装）" class="headerlink" title="2）安装 nerdctl（完整版，这里不装）"></a>2）安装 nerdctl（完整版，这里不装）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/containerd/nerdctl/releases/download/v0.22.2/nerdctl-full-0.22.2-linux-amd64.tar.gz</span><br><span class="line">tar -xf nerdctl-full-0.16.0-linux-amd64.tar.gz -C /usr/local/</span><br><span class="line"></span><br><span class="line"><span class="built_in">cp</span> /usr/local/lib/systemd/system/*.service /etc/systemd/system/</span><br></pre></td></tr></table></figure><p>启动服务buildkit</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span>  buildkit containerd --now</span><br><span class="line">systemctl status buildkit containerd</span><br></pre></td></tr></table></figure><h3 id="3）安装-buildkit-支持构建镜像"><a href="#3）安装-buildkit-支持构建镜像" class="headerlink" title="3）安装 buildkit 支持构建镜像"></a>3）安装 buildkit 支持构建镜像</h3><p>buildkit GitHub地址：<a href="https://github.com/moby/buildkit">https://github.com/moby/buildkit</a></p><blockquote><p>使用<strong>精简版 nerdctl无法直接通过containerd构建镜像</strong>，需要与buildkit组全使用以实现镜像构建。当然你也可以安装上面的完整nerdctl；buildkit项目是Docker公司开源出来的一个构建工具包，支持OCI标准的镜像构建。它主要包含以下部分:</p></blockquote><ul><li><p>服务端buildkitd，当前支持runc和containerd作为worker，默认是runc；</p></li><li><p>客户端buildctl，负责解析Dockerfile，并向服务端buildkitd发出构建请求。</p></li></ul><p>buildkit是典型的<strong>C&#x2F;S架构</strong>，client和server可以不在一台服务器上。而nerdctl在构建镜像方面也可以作为buildkitd的客户端。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/moby/buildkit/releases</span></span><br><span class="line">wget https://github.com/moby/buildkit/releases/download/v0.10.4/buildkit-v0.10.4.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">tar -xf buildkit-v0.10.4.linux-amd64.tar.gz  -C /usr/local/</span><br></pre></td></tr></table></figure><p>配置buildkit的启动文件，可以从这里下载：<a href="https://github.com/moby/buildkit/tree/master/examples/systemd">https://github.com/moby/buildkit/tree/master/examples/systemd</a><br>buildkit需要配置两个文件</p><ul><li><code>/usr/lib/systemd/system/buildkit.socket</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /usr/lib/systemd/system/buildkit.socket &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">[Unit]</span></span><br><span class="line"><span class="string">Description=BuildKit</span></span><br><span class="line"><span class="string">Documentation=https://github.com/moby/buildkit</span></span><br><span class="line"><span class="string">[Socket]</span></span><br><span class="line"><span class="string">ListenStream=%t/buildkit/buildkitd.sock</span></span><br><span class="line"><span class="string">SocketMode=0660</span></span><br><span class="line"><span class="string">[Install]</span></span><br><span class="line"><span class="string">WantedBy=sockets.target</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><ul><li><code>/usr/lib/systemd/system/buildkit.service</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /usr/lib/systemd/system/buildkit.service &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">[Unit]</span></span><br><span class="line"><span class="string">Description=BuildKit</span></span><br><span class="line"><span class="string">Requires=buildkit.socket</span></span><br><span class="line"><span class="string">After=buildkit.socket</span></span><br><span class="line"><span class="string">Documentation=https://github.com/moby/buildkit</span></span><br><span class="line"><span class="string">[Service]</span></span><br><span class="line"><span class="string"># Replace runc builds with containerd builds  </span></span><br><span class="line"><span class="string">ExecStart=/usr/local/bin/buildkitd --addr fd://</span></span><br><span class="line"><span class="string">[Install]</span></span><br><span class="line"><span class="string">WantedBy=multi-user.target</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>启动buildkit</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl <span class="built_in">enable</span> buildkit --now</span><br></pre></td></tr></table></figure><h2 id="四、实战操作"><a href="#四、实战操作" class="headerlink" title="四、实战操作"></a>四、实战操作</h2><h3 id="1）修改containerd配置文件"><a href="#1）修改containerd配置文件" class="headerlink" title="1）修改containerd配置文件"></a>1）修改containerd配置文件</h3><p>可以参考我之前的文章：【云原生.大数据】镜像仓库Harbor对接MinIO对象存储</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">containerd config default &gt; /etc/containerd/config.toml</span><br></pre></td></tr></table></figure><p>配置如下：</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]</span></span><br><span class="line">      <span class="attr">config_path</span> = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">      <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.auths]</span></span><br><span class="line"></span><br><span class="line">      <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs]</span></span><br><span class="line">        <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;myharbor-minio.com&quot;.tls]</span></span><br><span class="line">          <span class="attr">insecure_skip_verify</span> = <span class="literal">true</span>  <span class="comment">#跳过认证</span></span><br><span class="line">          <span class="attr">ca_file</span> = <span class="string">&quot;/etc/containerd/myharbor-minio.com/ca.crt&quot;</span></span><br><span class="line">        <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;myharbor-minio.com&quot;.auth]</span></span><br><span class="line">          <span class="attr">username</span> = <span class="string">&quot;admin&quot;</span></span><br><span class="line">          <span class="attr">password</span> = <span class="string">&quot;Harbor12345&quot;</span></span><br><span class="line"></span><br><span class="line">      <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.headers]</span></span><br><span class="line"></span><br><span class="line">      <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]</span></span><br><span class="line">        <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;myharbor-minio.com&quot;]</span></span><br><span class="line">          <span class="attr">endpoint</span> = [<span class="string">&quot;https://myharbor-minio.com&quot;</span>]</span><br></pre></td></tr></table></figure><p>重启containerd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重新加载配置</span></span><br><span class="line">systemctl daemon-reload</span><br><span class="line"><span class="comment">#重启containerd</span></span><br><span class="line">systemctl restart containerd</span><br></pre></td></tr></table></figure><blockquote><p>注意：这个配置文件是给<code>crictl</code>和<code>kubelet</code>使用，<code>ctr</code>是不可以用这个配置文件的，ctr 不使用 CRI，因此它不读取plugins.”io.containerd.grpc.v1.cri”配置。</p></blockquote><h3 id="2）ctr-拉取推送镜像"><a href="#2）ctr-拉取推送镜像" class="headerlink" title="2）ctr 拉取推送镜像"></a>2）ctr 拉取推送镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 推送镜像到harbor</span></span><br><span class="line">ctr --namespace=k8s.io images push myharbor-minio.com/bigdata/minio:2022.8.22-debian-11-r0 --skip-verify --user admin:Harbor12345</span><br><span class="line"></span><br><span class="line"><span class="comment"># --namespace=k8s.io 指定命名空间，不是必须，根据环境而定</span></span><br><span class="line"><span class="comment"># --skip-verify 跳过认证</span></span><br><span class="line"><span class="comment"># --user 指定harbor用户名及密码</span></span><br><span class="line"></span><br><span class="line">ctr  images pull --user admin:Harbor12345  --tlscacert=/etc/containerd/myharbor-minio.com/ca.crt myharbor-minio.com/bigdata/minio:2022.8.22-debian-11-r0</span><br></pre></td></tr></table></figure><p>不想-u user:password每次必须使用 ctr pull&#x2F;ctr push， 可以使用<code>nerdctl</code> 。</p><h3 id="3）镜像构建"><a href="#3）镜像构建" class="headerlink" title="3）镜像构建"></a>3）镜像构建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; Dockerfile &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">FROM nginx:alpine</span></span><br><span class="line"><span class="string">RUN echo &#x27;Hello Nerdctl From Containerd&#x27; &gt; /usr/share/nginx/html/index.html</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>然后在文件所在目录执行镜像构建命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不加-n指定命名空间，crictl看不到，kubelet也不能使用它，默认在default命名空间下</span></span><br><span class="line">nerdctl -n k8s.io build -t nginx:nerctl -f ./Dockerfile . </span><br><span class="line"><span class="comment">### 参数解释</span></span><br><span class="line"><span class="comment"># -t：指定镜像名称</span></span><br><span class="line"><span class="comment"># . ：当前目录Dockerfile</span></span><br><span class="line"><span class="comment"># -f：指定Dockerfile路径</span></span><br><span class="line"><span class="comment">#  --no-cache：不缓存</span></span><br></pre></td></tr></table></figure><h3 id="4）打标签-tag"><a href="#4）打标签-tag" class="headerlink" title="4）打标签 tag"></a>4）打标签 tag</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># crictl没有tag命令，只能使用nerdctl和ctr，必须指定命名空间，要不然kubelet无法使用。</span></span><br><span class="line">ctr -n k8s.io i tag </span><br><span class="line">nerdctl -n k8s.io tag nginx:nerctl myharbor-minio.com/bigdata/nginx:nerctl</span><br><span class="line"><span class="comment"># ctr -n k8s.io tag nginx:nerctl myharbor-minio.com/bigdata/nginx:nerctl</span></span><br><span class="line"><span class="comment"># 查看镜像</span></span><br><span class="line">nerdctl  -n k8s.io  images myharbor-minio.com/bigdata/nginx:nerctl</span><br></pre></td></tr></table></figure><h3 id="5）将镜像推送到-Harbor"><a href="#5）将镜像推送到-Harbor" class="headerlink" title="5）将镜像推送到 Harbor"></a>5）将镜像推送到 Harbor</h3><p>第一种情况：<code>http</code>方式，配置如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下两个哪个都可以</span></span><br><span class="line"><span class="comment"># mkdir -p /etc/docker/certs.d/myharbor-minio.com:443</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /etc/containerd/certs.d/myharbor-minio.com:443</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> &gt; /etc/containerd/certs.d/myharbor-minio.com\:443/hosts.toml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">server = &quot;https://docker.io&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[host.&quot;http://myharbor-minio.com:80&quot;]</span></span><br><span class="line"><span class="string">  capabilities = [&quot;pull&quot;, &quot;resolve&quot;,&quot;push&quot;]</span></span><br><span class="line"><span class="string">  #skip_verify = true</span></span><br><span class="line"><span class="string">  #ca = &quot;ca.crt&quot;   #相对路径</span></span><br><span class="line"><span class="string">  #ca = &quot;/opt/auth/ca.crt&quot;  #绝对路径</span></span><br><span class="line"><span class="string">  #ca = [&quot;/opt/auth/ca.crt&quot;]</span></span><br><span class="line"><span class="string">  #ca = [&quot;ca.crt&quot;]</span></span><br><span class="line"><span class="string">  #client = [[&quot;/opt/auth/nginx.cclinux.cn.crt&quot;, &quot;/opt/auth/nginx.cclinux.cn.key&quot;]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>第一种情况：<code>https</code>方式，配置如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下两个哪个都可以</span></span><br><span class="line"><span class="comment"># mkdir -p /etc/docker/certs.d/myharbor-minio.com:443</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /etc/containerd/certs.d/myharbor-minio.com:443</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> &gt; /etc/containerd/certs.d/myharbor-minio.com\:443/hosts.toml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">server = &quot;https://docker.io&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[host.&quot;https://myharbor-minio.com:443&quot;]</span></span><br><span class="line"><span class="string">  capabilities = [&quot;pull&quot;, &quot;resolve&quot;,&quot;push&quot;]</span></span><br><span class="line"><span class="string">  skip_verify = true</span></span><br><span class="line"><span class="string">  #ca = &quot;ca.crt&quot;   #相对路径</span></span><br><span class="line"><span class="string">  #ca = &quot;/opt/auth/ca.crt&quot;  #绝对路径</span></span><br><span class="line"><span class="string">  #ca = [&quot;/opt/auth/ca.crt&quot;]</span></span><br><span class="line"><span class="string">  ca = [&quot;/etc/containerd/myharbor-minio.com/ca.crt&quot;]</span></span><br><span class="line"><span class="string">  #client = [[&quot;/opt/auth/nginx.cclinux.cn.crt&quot;, &quot;/opt/auth/nginx.cclinux.cn.key&quot;]]</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>通过 nerdctl 登录 harbor</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Harbor12345 | nerdctl login --username <span class="string">&quot;admin&quot;</span> --password-stdin  myharbor-minio.com:443</span><br><span class="line"></span><br><span class="line">$ nerdctl login --username <span class="string">&quot;admin&quot;</span> --password Harbor12345 myharbor-minio.com:443</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登出</span></span><br><span class="line">$ nerdctl <span class="built_in">logout</span></span><br></pre></td></tr></table></figure><p>开始将镜像推送到harbor</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 推送到Harbor</span></span><br><span class="line"><span class="comment"># --insecure-registry        skips verifying HTTPS certs, and allows falling back to plain HTTP</span></span><br><span class="line">nerdctl --insecure-registry --namespace=k8s.io push myharbor-minio.com/bigdata/nginx:nerctl</span><br><span class="line"><span class="comment"># ctr --namespace=k8s.io images push myharbor-minio.com/bigdata/nginx:nerctl --skip-verify --user admin:Harbor12345</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --namespace=k8s.io 指定命名空间，跟-n一样，不是必须，根据环境而定</span></span><br><span class="line"><span class="comment"># --skip-verify 跳过认证</span></span><br><span class="line"><span class="comment"># --user 指定harbor用户名及密码</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">containerd</summary>
    
    
    
    <category term="containerd" scheme="https://blog.yongwang.lu/categories/containerd/"/>
    
    
    <category term="containerd" scheme="https://blog.yongwang.lu/tags/containerd/"/>
    
  </entry>
  
  <entry>
    <title>Containerd 入门到尝试</title>
    <link href="https://blog.yongwang.lu/post/964191ef.html"/>
    <id>https://blog.yongwang.lu/post/964191ef.html</id>
    <published>2023-01-02T03:45:41.000Z</published>
    <updated>2023-01-02T03:45:41.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Kubernetes v1.24 之前的版本直接集成了 Docker Engine 的一个组件，名为 <strong>dockershim</strong> [用于调用Docker]。 这种特殊的直接整合不再是 Kubernetes 的一部分 （这次删除被作为 v1.20 发行版本的一部分宣布）。 这意味Kubernetes从版本1.24开始就弃用Docker作为容器运行时，取而代之的是更加轻量级的Containerd。</p></blockquote><p><img src="https://res.yongwang.lu/img/5da250511cf72797f2e2e992bd47240f-2.png" alt="K8s-CRI流程更改"></p><blockquote><p>containerd可用作 Linux 和 Windows 的守护进程。它管理其主机系统的完整容器生命周期，从图像传输和存储到容器执行和监督，再到低级存储到网络附件等等。</p></blockquote><h2 id="一、Containerd介绍"><a href="#一、Containerd介绍" class="headerlink" title="一、Containerd介绍"></a>一、Containerd介绍</h2><h3 id="1、Containerd的由来"><a href="#1、Containerd的由来" class="headerlink" title="1、Containerd的由来"></a>1、Containerd的由来</h3><p>【Docker名噪一时，捐出runC】2013年docker公司在推出docker产品后，由于其对全球技术产生了一定的影响力，Google公司明显感觉到自己公司内部所使用的Brog系统江湖地位受到的威胁，希望Docker公司能够与自己联合打造一款开源的容器运行时作为Docker核心依赖，但Docker公司拒绝了；接着Google公司联合RedHat、IBM等公司说服Docker公司把其容器核心技术<code>libcontainer</code>捐给OCI（Open Container Intiative），并更名为<code>runC</code>。</p><p>【CNCF成立，kubernetes被迫开源】为了进一步遏制Docker在未来技术市场影响力，避免在容器市场上Docker一家独大，Google公司带领RedHat、IBM等成立了CNCF（Cloud Native Computing Fundation）基金会，即<strong>云原生计算基金会</strong>。CNCF的目标很明确，既然在容器应用领域无法与Docker相抗衡，那就做Google更有经验的技术市场——大规模容器编排应用场景。Google公司把自己内部使用的Brog系统开源——Kubernetes，也就是我们今天所说的云原生技术生态。</p><p>【Docker妥协，贡献出Containerd】2016年Docker公司推出了Docker Swarm，意在一统Docker生态，让Docker既可以实现容器应用管理，也可以实现大规模容器编排，经过近1年左右时间的市场验证后，发现在容器编排方面无法独立抗衡kubernetes，所以Docker公司于2017年正式宣布原生支持Kubernetes。至此，Docker在大规模容器编排应用市场败下阵来，但是Docker依然不甘心失败，把Docker核心依赖Containerd捐给了CNCF，依此说明Docker依旧是一个PaaS平台。</p><p>【k8s宣布不支持Docker，Containerd成为CRI主角】2020年CNCF基金会宣布Kubernetes 1.20版本将不再仅支持Docker容器管理工具，此事的起因主要也与Docker捐给CNCF基金会的Containerd有关，早期为了实现Kubernetes能够使用Docker实现容器管理，专门在Kubernetes组件中集成一个<code>shim</code>技术，用来将Kubernetes <strong>容器运行时接口</strong>（CRI，Container Runntime Interface）调用翻译成Docker的API，这样就可以很好地使用Docker了。但是随着Kubernetes在全球技术市场的广泛应用，有更多的容器管理工具的出现，它们都想能够借助于Kubernetes被用户所使用，所以就提出标准化容器运行时接口，只要适配了这个接口就可以集成到Kubernetes生态当中，所以Kubernetes取消了对shim的维护，并且由于Containerd技术的成功，可以实现无缝对接Kubernetes，所以接下来Kubernetes容器运行时的主角是Containerd。</p><h3 id="2、Containerd概念"><a href="#2、Containerd概念" class="headerlink" title="2、Containerd概念"></a>2、Containerd概念</h3><p>早在2016年3月，Docker 1.11的Docker Engine里就包含了containerd，而现在则是把containerd从Docker Engine里彻底剥离出来，作为一个独立的开源项目独立发展，目标是提供一个更加开放、稳定的容器运行基础设施。</p><p>和原先包含在Docker Engine里containerd相比，独立的containerd将具有更多的功能，可以涵盖整个容器运行时管理的所有需求。另外独立之后containerd的特性演进可以和Docker Engine分开，专注容器运行时管理，可以更稳定。</p><p>Containerd是一个工业标准的容器运行时，重点是它简洁，健壮，便携，在Linux和window上可以作为一个守护进程运行，它可以管理主机系统上容器的完整的生命周期：镜像传输和存储，容器的执行和监控，低级别的存储和网络。</p><p><strong>每个containerd只负责一台机器</strong>，Pull镜像，对容器的操作（启动、停止等），网络，存储都是由containerd完成。<strong>具体运行容器由runC负责</strong>，实际上只要是符合OCI规范的容器都可以支持。</p><p>Containerd和docker不同，containerd重点是集成在大规模的系统中，例如kubernetes、Swarm、Mesos等【对于容器编排服务来说，运行时只需要使用containerd+runC，更加轻量，容易管理。】。<strong>Containerd 被设计成嵌入到一个更大的系统中，而不是直接由开发人员或终端用户使用</strong>。</p><p>Containerd的特点：</p><ul><li>简洁的基于 gRPC 的 API 和 client library。</li><li>完整的 OCI 支持(runtime 和 image spec)。</li><li>同时具备稳定性和高性能的定义良好的容器核心功能。</li><li>一个解耦的系统(让 image、filesystem、runtime 解耦合)，实现插件式的扩展和重用。</li></ul><p>Containerd的作用：</p><ul><li>管理容器的生命周期(从创建容器到销毁容器)。</li><li>拉取&#x2F;推送容器镜像。</li><li>存储管理(管理镜像及容器数据的存储)。</li><li>调用 runC 运行容器(与 runC 等容器运行时交互)。</li><li>管理容器网络接口及网络。</li></ul><p>使用 bucketbench 对 Docker、crio 和 Containerd 的性能测试结果，包括启动、停止和删除容器，以比较它们所耗的时间，可以发现Containerd 在各个方面都表现良好，总体性能优于 Docker 和 crio。</p><h3 id="3、Containerd架构"><a href="#3、Containerd架构" class="headerlink" title="3、Containerd架构"></a>3、Containerd架构</h3><p>Containerd 采用标准的 C&#x2F;S 架构：服务端通过 GRPC 协议提供稳定的 API；客户端通过调用服务端的 API 进行高级的操作。</p><p>为了实现解耦，Containerd 将不同的职责划分给不同的组件，每个组件就相当于一个子系统（subsystem）。连接不同子系统的组件被称为模块。</p><p>Containerd 被分为三个大块： Storage 、 Metadata 和 Runtime。</p><p>Containerd 两大子系统为：<br>Bundle : 在 Containerd 中，Bundle 包含了配置、元数据和根文件系统数据，你可以理解为 容器的文件系统。而 Bundle 子系统允许用户从镜像中提取和打包 Bundles。<br>Runtime : Runtime 子系统用来执行 Bundles，比如创建容器。其中，每一个子系统的行为都由一个或多个模块协作完成（架构图中的 Core 部分）。</p><p><img src="https://containerd.io/img/architecture.png"></p><h3 id="4、几个概念区分"><a href="#4、几个概念区分" class="headerlink" title="4、几个概念区分"></a>4、几个概念区分</h3><p><code>containerd</code> 是一个高级容器运行时，又名容器管理器。简单来说，它是一个守护进程，在单个主机上管理完整的容器生命周期：创建、启动、停止容器、拉取和存储镜像、配置挂载、网络等。</p><p><code>ctr</code> 是作为 containerd 项目的一部分提供的命令行客户端。该<code>ctr</code>界面 与 Docker CLI不兼容，乍一看，可能看起来不太用户友好。因为它的主要受众是测试守护进程的容器开发人员。<strong>ctr + containerd比docker + dockerd更接近实际的容器。</strong></p><p><code>nerdctl</code> 是一个相对较新的containerd命令行客户端。与ctr不同，nerdctl的目标是用户友好和docker兼容。在某种程度上，<strong>nerdctl + containerd可以无缝地替代docker + dockerd。</strong></p><p><code>crictl</code> 是一个命令行客户端，用于 [kubernetes] CRI兼容的容器运行时。引入 Kubernetes 容器运行时接口 (CRI)以使 Kubernetes 容器运行时不可知。Kubernetes节点代理kubelet实现了 CRI客户端 API，可以使用任何实现 CRI 服务器 API的容器运行时来管理其节点上的容器和 Pod。</p><h2 id="二、Containerd安装"><a href="#二、Containerd安装" class="headerlink" title="二、Containerd安装"></a>二、Containerd安装</h2><p>containerd官网：<a href="https://containerd.io/" title="https://containerd.io/">https://containerd.io/</a></p><p>containerd官方安装步骤：<a href="https://github.com/containerd/containerd/blob/main/docs/getting-started.md" title="https://github.com/containerd/containerd/blob/main/docs/getting-started.md">https://github.com/containerd/containerd/blob/main/docs/getting-started.md</a></p><h3 id="step1：安装Containerd"><a href="#step1：安装Containerd" class="headerlink" title="step1：安装Containerd"></a>step1：安装Containerd</h3><p>因后续版本Kubernetes 对containerd运行时有版本要求这里安装1.6.x版本的. Ubuntu源是1.5.9版本. 这里 直接下载deb安装.&amp;#x20;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载Deb</span></span><br><span class="line"><span class="comment"># Docker官方源 https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/</span></span><br><span class="line"><span class="comment"># Tencent源 https://mirrors.cloud.tencent.com/docker-ce/linux/ubuntu/dists/jammy/pool/stable/amd64/</span></span><br><span class="line">$ wget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/containerd.io_1.6.14-1_amd64.deb</span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">$ dpkg -i containerd.io_1.6.14-1_amd64.deb</span><br></pre></td></tr></table></figure><p>将containerd服务设置卫开机启动。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl enable --now containerd</span><br><span class="line"># 验证</span><br><span class="line">$ ctr version</span><br><span class="line">Client:</span><br><span class="line">  Version:  v1.6.14</span><br><span class="line">  Revision: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6</span><br><span class="line">  Go version: go1.17.13</span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line">  Version:  v1.6.14</span><br><span class="line">  Revision: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6</span><br><span class="line">  UUID: 87222662-9eb1-44cb-998a-689c9efce638</span><br></pre></td></tr></table></figure><p>至此，containerd安装完成。</p><h3 id="step2：修改配置"><a href="#step2：修改配置" class="headerlink" title="step2：修改配置"></a>step2：修改配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line"><span class="built_in">mkdir</span> /etc/containerd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成默认配置文件</span></span><br><span class="line">containerd config default | \</span><br><span class="line"><span class="built_in">tee</span> /etc/containerd/config.toml &gt;/dev/null</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改配置文件 加速</span></span><br><span class="line">sed -i \</span><br><span class="line">-e <span class="string">&#x27;/sandbox_image/s?k8s.gcr.io?registry.aliyuncs.com/google_containers?&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/SystemdCgroup/s?false?true?&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/registry.mirrors/a\        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/registry.mirrors/a\          endpoint = [&quot;https://docker.nju.edu.cn/&quot;]&#x27;</span> /etc/containerd/config.toml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 服务重启</span></span><br><span class="line">systemctl restart containerd</span><br></pre></td></tr></table></figure><h3 id="step3：安装cni插件和cni工具包"><a href="#step3：安装cni插件和cni工具包" class="headerlink" title="step3：安装cni插件和cni工具包"></a>step3：安装cni插件和cni工具包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">mkdir</span> -p /home/cni-plugins</span><br><span class="line">$ tar Cxzvf /home/cni-plugins cni-plugins-linux-amd64-v1.0.0.tgz</span><br><span class="line"><span class="comment">## 该二进制文件是静态构建的，可以在任何 Linux 发行版上运行</span></span><br><span class="line"><span class="comment"># cni工具1.1.2兼容cni插件1.0.0</span></span><br><span class="line">$ <span class="built_in">mkdir</span> -p /home/cni-tools</span><br><span class="line">$ tar Cxzvf /home/cni-tools cni-1.1.2.tar.gz</span><br></pre></td></tr></table></figure><p>完成。</p><h2 id="三、ctr命令行操作"><a href="#三、ctr命令行操作" class="headerlink" title="三、ctr命令行操作"></a>三、ctr命令行操作</h2><p><strong>ctr是作为 containerd 项目的一部分提供的命令行客户端。</strong></p><p>该ctr界面 [显然] 与 Docker CLI不兼容，乍一看，可能看起来不太用户友好。显然，它的主要受众是测试守护进程的容器开发人员。但是，由于它最接近实际的 containerd API，因此它可以作为一种很好的探索手段——通过检查可用命令，可以大致了解 containerd可以做什么和不可以做什么。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">$ ctr --<span class="built_in">help</span></span><br><span class="line">NAME:</span><br><span class="line">   ctr - </span><br><span class="line">        __</span><br><span class="line">  _____/ /______</span><br><span class="line"> / ___/ __/ ___/</span><br><span class="line">/ /__/ /_/ /</span><br><span class="line">\___/\__/_/</span><br><span class="line"></span><br><span class="line">containerd CLI</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">USAGE:</span><br><span class="line">   ctr [global options] <span class="built_in">command</span> [<span class="built_in">command</span> options] [arguments...]</span><br><span class="line"></span><br><span class="line">VERSION:</span><br><span class="line">   v1.6.8</span><br><span class="line"></span><br><span class="line">DESCRIPTION:</span><br><span class="line">   </span><br><span class="line">ctr is an unsupported debug and administrative client <span class="keyword">for</span> interacting</span><br><span class="line">with the containerd daemon. Because it is unsupported, the commands,</span><br><span class="line">options, and operations are not guaranteed to be backward compatible or</span><br><span class="line">stable from release to release of the containerd project.</span><br><span class="line"></span><br><span class="line">COMMANDS:</span><br><span class="line">   plugins, plugin            provides information about containerd plugins</span><br><span class="line">   version                    <span class="built_in">print</span> the client and server versions</span><br><span class="line">   containers, c, container   manage containers</span><br><span class="line">   content                    manage content</span><br><span class="line">   events, event              display containerd events</span><br><span class="line">   images, image, i           manage images</span><br><span class="line">   leases                     manage leases</span><br><span class="line">   namespaces, namespace, ns  manage namespaces</span><br><span class="line">   pprof                      provide golang pprof outputs <span class="keyword">for</span> containerd</span><br><span class="line">   run                        run a container</span><br><span class="line">   snapshots, snapshot        manage snapshots</span><br><span class="line">   tasks, t, task             manage tasks</span><br><span class="line">   install                    install a new package</span><br><span class="line">   oci                        OCI tools</span><br><span class="line">   shim                       interact with a shim directly</span><br><span class="line">   <span class="built_in">help</span>, h                    Shows a list of commands or <span class="built_in">help</span> <span class="keyword">for</span> one <span class="built_in">command</span></span><br><span class="line"></span><br><span class="line">GLOBAL OPTIONS:</span><br><span class="line">   --debug                      <span class="built_in">enable</span> debug output <span class="keyword">in</span> logs</span><br><span class="line">   --address value, -a value    address <span class="keyword">for</span> containerd<span class="string">&#x27;s GRPC server (default: &quot;/run/containerd/containerd.sock&quot;) [$CONTAINERD_ADDRESS]</span></span><br><span class="line"><span class="string">   --timeout value              total timeout for ctr commands (default: 0s)</span></span><br><span class="line"><span class="string">   --connect-timeout value      timeout for connecting to containerd (default: 0s)</span></span><br><span class="line"><span class="string">   --namespace value, -n value  namespace to use with commands (default: &quot;default&quot;) [$CONTAINERD_NAMESPACE]</span></span><br><span class="line"><span class="string">   --help, -h                   show help</span></span><br><span class="line"><span class="string">   --version, -v                print the version</span></span><br></pre></td></tr></table></figure><h3 id="1、Containerd镜像管理"><a href="#1、Containerd镜像管理" class="headerlink" title="1、Containerd镜像管理"></a>1、Containerd镜像管理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拉取镜像  拉取镜像，完全合格的参考似乎是必需的，所以不能忽略镜像仓库或标签部分。</span></span><br><span class="line">$ ctr images pull --all-platforms docker.io/library/nginx:alpine</span><br><span class="line"><span class="comment"># --all-platforms  指定所有平台镜像  </span></span><br><span class="line"><span class="comment"># --platform       指定系统平台</span></span><br><span class="line">$ ctr images pull --platform linux/amd64 docker.io/library/nginx:latest</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看镜像</span></span><br><span class="line">$ ctr images <span class="built_in">ls</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂载镜像</span></span><br><span class="line">$ ctr images mount docker.io/library/nginx:alpine /mnt</span><br><span class="line">$ <span class="built_in">ls</span> /mnt</span><br><span class="line">$ umount /mnt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像导出，并保存为nginx.img文件</span></span><br><span class="line">$ ctr i <span class="built_in">export</span> --all-platforms nginx.img docker.io/library/nginx:alpine</span><br><span class="line">$ <span class="built_in">ls</span> -l</span><br><span class="line">-rw-r--r-- 1 root root 70138368 Sep  2 16:33 nginx.img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像导入</span></span><br><span class="line">$ ctr images import nginx.img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像删除</span></span><br><span class="line">$ ctr image <span class="built_in">rm</span> docker.io/library/nginx:alpine</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像打tag</span></span><br><span class="line">$ ctr images tag docker.io/library/nginx:alpine nginx:alpine</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像对比</span></span><br><span class="line">$ ctr images check</span><br></pre></td></tr></table></figure><h3 id="2、Containerd容器管理"><a href="#2、Containerd容器管理" class="headerlink" title="2、Containerd容器管理"></a>2、Containerd容器管理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建【静态】容器</span></span><br><span class="line"><span class="comment"># 该命令创建容器后，容器并没有处于运行状态，其只是一个静态的容器。这个container对象只是包含了运行一个容器所需的资源及配置的数据结构，例如： namespaces、rootfs 和容器的配置都已经初始化成功了，只是用户进程(本案例为nginx)还没有启动。需要使用`ctr tasks`命令才能获取一个动态容器。</span></span><br><span class="line">$ ctr container create docker.io/library/nginx:alpine testnginx1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动【动态】容器  创建容器和任务子命令分离</span></span><br><span class="line">$ ctr task start -d testnginx1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接运行一个动态容器【容器的IP就是宿主机的IP】</span></span><br><span class="line"><span class="comment"># ctr运行命令实际上是ctr容器创建+ ctr任务启动的快捷方式</span></span><br><span class="line">$ ctr run -d --net-host docker.io/library/nginx:alpine testnginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看【静态】容器</span></span><br><span class="line">$ ctr container <span class="built_in">ls</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看【动态】容器</span></span><br><span class="line">$ ctr task ps testnginx1</span><br><span class="line">PID      INFO</span><br><span class="line">64574    -</span><br><span class="line">64613    -</span><br><span class="line">64614    -</span><br><span class="line">64615    -</span><br><span class="line">64616    -</span><br><span class="line"><span class="comment"># 物理机上查看</span></span><br><span class="line">$ ps -ef | grep nginx</span><br><span class="line">root      64574  64553  0 16:41 ?        00:00:00 nginx: master process nginx -g daemon off;</span><br><span class="line">101       64613  64574  0 16:41 ?        00:00:00 nginx: worker process</span><br><span class="line">101       64614  64574  0 16:41 ?        00:00:00 nginx: worker process</span><br><span class="line">101       64615  64574  0 16:41 ?        00:00:00 nginx: worker process</span><br><span class="line">101       64616  64574  0 16:41 ?        00:00:00 nginx: worker process</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">$ ctr task <span class="built_in">exec</span> --exec-id 1 testnginx2 sh</span><br><span class="line"><span class="comment"># 查看网卡</span></span><br><span class="line">ifconfig  </span><br><span class="line">ens33     Link encap:Ethernet  HWaddr 00:0C:29:32:CC:B0  </span><br><span class="line">          inet addr:192.168.168.201  Bcast:192.168.168.255  Mask:255.255.255.0</span><br><span class="line">          inet6 addr: fe80::63d5:da66:d3db:fa1c/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:476940 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:595381 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:175687016 (167.5 MiB)  TX bytes:149770479 (142.8 MiB)</span><br><span class="line">lo        Link encap:Local Loopback  </span><br><span class="line">          inet addr:127.0.0.1  Mask:255.0.0.0</span><br><span class="line">          inet6 addr: ::1/128 Scope:Host</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:65536  Metric:1</span><br><span class="line">          RX packets:0 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)</span><br><span class="line"><span class="comment"># 为容器中运行的网站添加网站文件</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;hello nginx2&quot;</span> &gt; /usr/share/nginx/html/index.html</span><br><span class="line"><span class="comment"># 在容器里访问</span></span><br><span class="line">curl 127.0.0.1</span><br><span class="line"><span class="comment"># 退出容器          </span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在宿主机上访问</span></span><br><span class="line">$ curl 192.168.168.201</span><br><span class="line">hello nginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暂停容器</span></span><br><span class="line">$ ctr task pause testnginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复容器</span></span><br><span class="line">$ ctr task resume testnginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止容器</span></span><br><span class="line">$ ctr task <span class="built_in">kill</span> testnginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除动态容器【必须先停止】</span></span><br><span class="line">$ ctr task delete testnginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除静态容器</span></span><br><span class="line">$ ctr container delete testnginx2</span><br></pre></td></tr></table></figure><h3 id="3、Containerd命名空间管理"><a href="#3、Containerd命名空间管理" class="headerlink" title="3、Containerd命名空间管理"></a>3、Containerd命名空间管理</h3><p><strong>containerd中namespace的作用为：隔离运行的容器，可以实现运行多个容器。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拉取镜像</span></span><br><span class="line">$ ctr namespace --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建命名空间</span></span><br><span class="line">$ ctr ns create testns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看命名空间</span></span><br><span class="line">$ ctr ns <span class="built_in">ls</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在命名空间中进行镜像、容器等相关操作</span></span><br><span class="line">$ ctr -n testns images pull docker.io/library/nginx:latest</span><br><span class="line">$ ctr -n testns images <span class="built_in">ls</span></span><br><span class="line">$ ctr -n testns container create docker.io/library/nginx:latest</span><br><span class="line">$ ctr -n testns container <span class="built_in">ls</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除命名空间</span></span><br><span class="line">$ ctr ns <span class="built_in">rm</span> testns</span><br></pre></td></tr></table></figure><h3 id="4、Containerd网络管理"><a href="#4、Containerd网络管理" class="headerlink" title="4、Containerd网络管理"></a>4、Containerd网络管理</h3><p><strong>默认Containerd管理的容器仅有lo网络，无法访问容器之外的网络，可以为其添加网络插件，使用容器可以连接外网，CNI（Container Network Interface）</strong></p><p>cni插件和cni工具包已经在前面第二章节已经部署完毕。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cni插件目录</span></span><br><span class="line">$ <span class="built_in">cd</span> /home/cni-plugins</span><br><span class="line"><span class="comment"># cni工具包目录</span></span><br><span class="line">$ <span class="built_in">cd</span> /home/cni-tools/cni-1.1.2</span><br><span class="line"><span class="comment"># cni工具1.1.2兼容cni插件1.0.0</span></span><br></pre></td></tr></table></figure><ul><li><p>下面先开始准备容器网络配置文件，用于为容器提供网关、IP地址等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/cni/net.d/10-mynet.conf</span><br><span class="line">$ vim /etc/cni/net.d/99-loopback.conf</span><br><span class="line">$ <span class="built_in">cat</span> /etc/cni/net.d/10-mynet.conf</span><br><span class="line">&#123;</span><br><span class="line">        <span class="string">&quot;cniVersion&quot;</span>: <span class="string">&quot;1.0.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;mynet&quot;</span>,</span><br><span class="line">        <span class="string">&quot;type&quot;</span>: <span class="string">&quot;bridge&quot;</span>,</span><br><span class="line">        <span class="string">&quot;bridge&quot;</span>: <span class="string">&quot;cni0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;isGateway&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">        <span class="string">&quot;ipMasq&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">        <span class="string">&quot;ipam&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;host-local&quot;</span>,</span><br><span class="line">            <span class="string">&quot;subnet&quot;</span>: <span class="string">&quot;10.66.0.0/16&quot;</span>,</span><br><span class="line">            <span class="string">&quot;routes&quot;</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;dst&quot;</span>: <span class="string">&quot;0.0.0.0/0&quot;</span></span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">$ <span class="built_in">cat</span> /etc/cni/net.d/99-loopback.conf</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;cniVerion&quot;</span>: <span class="string">&quot;1.0.0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;lo&quot;</span>,</span><br><span class="line">    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;loopback&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>生成cni网络</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取epel源</span></span><br><span class="line">$ wget -O /etc/yum.repos.d/epel.repo [http://mirrors.aliyun.com/repo/epel-7.repo](http://mirrors.aliyun.com/repo/epel-7.repo <span class="string">&quot;http://mirrors.aliyun.com/repo/epel-7.repo&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装jq【jq是一个&#x27;出色&#x27;的&#x27;针对--&gt;JSON处理器&#x27;的命令行】</span></span><br><span class="line">$ yum -y install jq</span><br><span class="line"><span class="comment"># 进入cni工具目录</span></span><br><span class="line">$ <span class="built_in">cd</span> /home/cni-tools/cni-1.1.2</span><br><span class="line"><span class="comment"># 执行脚本文件，基于/etc/cni/net.d/目录中的 \*.conf配置文件生成容器网络</span></span><br><span class="line"><span class="comment"># CNI\_PATH是cni插件安装的目录</span></span><br><span class="line">$ CNI\_PATH=/home/cni-plugins ./priv-net-run.sh <span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line"><span class="comment"># cni插件和cni工具包版本要兼容，否则可能会报错如下：</span></span><br><span class="line">mynet : error executing ADD: &#123;</span><br><span class="line">    <span class="string">&quot;code&quot;</span>: 1,</span><br><span class="line">    <span class="string">&quot;msg&quot;</span>: <span class="string">&quot;incompatible CNI versions&quot;</span>,</span><br><span class="line">    <span class="string">&quot;details&quot;</span>: <span class="string">&quot;config is &quot;</span>1.0.0<span class="string">&quot;, plugin supports \[&quot;</span>0.1.0<span class="string">&quot; &quot;</span>0.2.0<span class="string">&quot; &quot;</span>0.3.0<span class="string">&quot; &quot;</span>0.3.1<span class="string">&quot; &quot;</span>0.4.0<span class="string">&quot;]&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装成功后，在宿主机上查看是否生成容器网络名为cni0的网桥</span></span><br><span class="line">$ ip a s</span><br><span class="line">    1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">    valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">    valid_lft forever preferred_lft forever</span><br><span class="line">    ...</span><br><span class="line">    7: cni0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/ether 6a:32:bc:eb:0f:23 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.66.0.1/16 brd 10.66.255.255 scope global cni0</span><br><span class="line">    valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::6832:bcff:feeb:f23/64 scope <span class="built_in">link</span></span><br><span class="line">    valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></li><li><p>创建容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ ctr images pull docker.io/library/busybox:latest</span><br><span class="line">$ ctr run -d docker.io/library/busybox:latest busybox</span><br><span class="line">$ ctr tasks <span class="built_in">exec</span> --exec-id<span class="variable">$RANDOM</span> -t busybox sh ip a s</span><br><span class="line">    1: lo: \&lt;LOOPBACK,UP,LOWER\_UP&gt; mtu 65536 qdisc noqueue qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    2: tunl0\@NONE:  mtu 1480 qdisc noop qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line"><span class="comment">#获取容器进程ID及其网络命名空间</span></span><br><span class="line">$ pid= $(ctr tasks <span class="built_in">ls</span> | grep busybox | awk <span class="string">&#x27;&#123;print $ 2&#125;&#x27;</span>) &amp;&amp; <span class="built_in">echo</span> <span class="variable">$pid</span></span><br><span class="line">39287</span><br><span class="line">$ netnspath=/proc/ <span class="variable">$pid</span>/ns/net &amp;&amp; <span class="built_in">echo</span> <span class="variable">$netnspath</span>/proc/39287/ns/net</span><br><span class="line"><span class="comment"># 进入目录为指定容器添加网络配置</span></span><br><span class="line"><span class="variable">$cd</span> /home/cni-tools/cni-1.1.2/scripts/$ CNI\_PATH=/home/cni-plugins ./exec-plugins.sh add $ pid  <span class="variable">$netnspath</span></span><br></pre></td></tr></table></figure></li><li><p>验证容器网络与宿主机网络的互访功能</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证</span></span><br><span class="line">    <span class="comment"># 进入容器确认是否添加网卡信息</span></span><br><span class="line">    <span class="variable">$ctr</span> tasks <span class="built_in">exec</span> --exec-id<span class="variable">$RANDOM</span> -t busybox sh</span><br><span class="line">    $ ip a s</span><br><span class="line">    1: lo: \&lt;LOOPBACK,UP,LOWER\_UP&gt; mtu 65536 qdisc noqueue qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    2: tunl0\@NONE:&amp;<span class="comment">#x20;</span></span><br><span class="line"></span><br><span class="line">    &amp;<span class="comment">#x20;mtu 1480 qdisc noop qlen 1000</span></span><br><span class="line">    <span class="built_in">link</span>/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">    3: eth0\@if9: \&lt;BROADCAST,MULTICAST,UP,LOWER\_UP,M-DOWN&gt; mtu 1500 qdisc noqueue</span><br><span class="line">    <span class="built_in">link</span>/ether 5a:36:90:c1:33:b1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.66.0.4/16 brd 10.66.255.255 scope global eth0</span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    inet6 fe80::5836:90ff:fec1:33b1/64 scope <span class="built_in">link</span></span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    <span class="comment"># 在容器中ping容器宿主机IP地址</span></span><br><span class="line">    $ ping -c 2 192.168.168.201</span><br><span class="line">    PING 192.168.168.201 (192.168.168.201): 56 data bytes</span><br><span class="line">    64 bytes from 192.168.168.201: <span class="built_in">seq</span>=0 ttl=64 time=0.067 ms</span><br><span class="line">    64 bytes from 192.168.168.201: <span class="built_in">seq</span>=1 ttl=64 time=0.070 ms</span><br><span class="line"></span><br><span class="line">    \--- 192.168.168.201 ping statistics ---</span><br><span class="line">    2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">    round-trip min/avg/max = 0.067/0.068/0.070 ms</span><br><span class="line">    <span class="comment"># 在容器中ping容器宿主机IP地址</span></span><br><span class="line">    $ ping -c 2 192.168.168.2</span><br><span class="line">    PING 192.168.168.2 (192.168.168.2): 56 data bytes</span><br><span class="line">    64 bytes from 192.168.168.2: <span class="built_in">seq</span>=0 ttl=127 time=0.141 ms</span><br><span class="line">    64 bytes from 192.168.168.2: <span class="built_in">seq</span>=1 ttl=127 time=0.324 ms</span><br><span class="line"></span><br><span class="line">    \--- 192.168.168.2 ping statistics ---</span><br><span class="line">    2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">    round-trip min/avg/max = 0.141/0.232/0.324 ms</span><br><span class="line">    <span class="comment"># 在容器中ping容器宿主机IP地址</span></span><br><span class="line">    $ ping -c 2 192.168.168.202</span><br><span class="line">    PING 192.168.168.201 (192.168.168.202): 56 data bytes</span><br><span class="line">    64 bytes from 192.168.168.202: <span class="built_in">seq</span>=0 ttl=64 time=0.067 ms</span><br><span class="line">    64 bytes from 192.168.168.202: <span class="built_in">seq</span>=1 ttl=64 time=0.070 ms</span><br><span class="line"></span><br><span class="line">    \--- 192.168.168.202 ping statistics ---</span><br><span class="line">    2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">    round-trip min/avg/max = 0.067/0.068/0.070 ms</span><br><span class="line">    <span class="comment"># 在容器中开启httpd服务</span></span><br><span class="line">    $ <span class="built_in">echo</span> <span class="string">&quot;containerd net web test&quot;</span> &gt; /tmp/index.html</span><br><span class="line">    $ httpd -h /tmp</span><br><span class="line">    $ wget -O - -q 127.0.0.1</span><br><span class="line">    containerd net web <span class="built_in">test</span></span><br><span class="line">    $ <span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">    \<span class="comment">#在宿主机访问容器提供的httpd服务</span></span><br><span class="line">    \$ curl 10.66.0.4</span><br><span class="line">    containerd net web <span class="built_in">test</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="5、Containerd数据持久化"><a href="#5、Containerd数据持久化" class="headerlink" title="5、Containerd数据持久化"></a>5、Containerd数据持久化</h3><p>实现把宿主机目录挂载至Containerd容器中，<strong>实现容器数据持久化存储</strong>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个静态容器，实现宿主机目录与容器挂载，src=/tmp 为宿主机目录 dst=/hostdir 为容器中目录</span></span><br><span class="line">$ ctr container create docker.io/library/busybox:latest busybox3 --mount <span class="built_in">type</span>=<span class="built_in">bind</span>,src=/tmp,dst=/hostdir,options=rbind:rw</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行用户进程</span></span><br><span class="line">$ ctr tasks start -d busybox3 bash</span><br><span class="line">$ ctr t <span class="built_in">ls</span></span><br><span class="line">TASK        PID       STATUS    </span><br><span class="line">busybox     39287     RUNNING</span><br><span class="line">busybox3    105003    RUNNING</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入容器，查看是否挂载成功</span></span><br><span class="line">$ ctr tasks <span class="built_in">exec</span> --exec-id <span class="variable">$RANDOM</span> -t busybox3 sh</span><br><span class="line">$ <span class="built_in">ls</span> /hostdir/</span><br><span class="line">ks-script-GSRMK8                                                         vmware-root_6389-1958486695</span><br><span class="line">systemd-private-cc74dd8802f0428490924757e690c750-chronyd.service-9TDICh  vmware-root_6511-1689719547</span><br><span class="line">vmware-root_6359-1949639453                                              yum.log</span><br><span class="line"><span class="comment"># 向容器中挂载目录中添加文件</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;hello world&quot;</span> &gt; /hostdir/test.txt</span><br><span class="line">$ <span class="built_in">ls</span> /hostdir/</span><br><span class="line">ks-script-GSRMK8                                                         vmware-root_6389-1958486695</span><br><span class="line">systemd-private-cc74dd8802f0428490924757e690c750-chronyd.service-9TDICh  vmware-root_6511-1689719547</span><br><span class="line">test.txt                                                                 yum.log</span><br><span class="line">vmware-root_6359-1949639453</span><br><span class="line"><span class="comment"># 在宿主机上查看被容器挂载的目录中是否添加了新的文件，已添加表明被容器挂载成功，并可以读写此目录中内容。</span></span><br><span class="line">$ <span class="built_in">cat</span> /tmp/test.txt </span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h3 id="6、Docker集成Container容器管理"><a href="#6、Docker集成Container容器管理" class="headerlink" title="6、Docker集成Container容器管理"></a>6、Docker集成Container容器管理</h3><p>目前Containerd主要任务还在于解决容器运行时的问题，对于其周边生态还不完善，所以有时需要借助：Docker结合Containerd来实现Docker完整的功能应用。</p><p>Docker安装与使用教程：<a href="https://blog.csdn.net/qq_41822345/article/details/107123094" title="https://blog.csdn.net/qq_41822345/article/details/107123094">https://blog.csdn.net/qq_41822345&#x2F;article&#x2F;details&#x2F;107123094</a></p><p>docker运行的容器默认在moby命名空间下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装docker</span></span><br><span class="line">$ yum install docker</span><br><span class="line"><span class="comment"># 启动docker</span></span><br><span class="line">$ systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行一个容器</span></span><br><span class="line">$ docker run -d nginx:latest</span><br><span class="line">$ docker ps</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看ctr命令下的namespace，发现多了一个moby命名空间。moby即为docker使用的命名空间。</span></span><br><span class="line">$ ctr namespace <span class="built_in">ls</span></span><br><span class="line">NAME    LABELS </span><br><span class="line">default        </span><br><span class="line">k8s.io         </span><br><span class="line">moby</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看moby命名空间下的容器和任务</span></span><br><span class="line">$ ctr -n moby container <span class="built_in">ls</span></span><br><span class="line">$ ctr -n moby tasks <span class="built_in">ls</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">containerd</summary>
    
    
    
    <category term="containerd" scheme="https://blog.yongwang.lu/categories/containerd/"/>
    
    
    <category term="containerd" scheme="https://blog.yongwang.lu/tags/containerd/"/>
    
  </entry>
  
</feed>
