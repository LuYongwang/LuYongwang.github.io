<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L丶lulu‘s Blog</title>
  
  <subtitle>敬往事一杯酒,再爱我也不回头🌹</subtitle>
  <link href="https://blog.yongwang.lu/atom.xml" rel="self"/>
  
  <link href="https://blog.yongwang.lu/"/>
  <updated>2023-06-30T16:00:00.000Z</updated>
  <id>https://blog.yongwang.lu/</id>
  
  <author>
    <name>L丶lulu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>程序员延寿指南</title>
    <link href="https://blog.yongwang.lu/post/13a7815d.html"/>
    <id>https://blog.yongwang.lu/post/13a7815d.html</id>
    <published>2023-06-30T16:00:00.000Z</published>
    <updated>2023-06-30T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-术语"><a href="#1-术语" class="headerlink" title="1. 术语"></a>1. 术语</h3><ul><li>ACM: All-Cause Mortality &#x2F; 全因死亡率</li></ul><h3 id="2-目标"><a href="#2-目标" class="headerlink" title="2. 目标"></a>2. 目标</h3><ul><li>稳健的活得更久</li><li>花更少时间工作：见<a href="https://github.com/geekan/MetaGPT">MetaGPT</a></li></ul><h3 id="3-关键结果"><a href="#3-关键结果" class="headerlink" title="3. 关键结果"></a>3. 关键结果</h3><ul><li>降低66.67%全因死亡率</li><li>增加~20年预期寿命</li><li><del>维持多巴胺于中轴</del></li></ul><h3 id="4-分析"><a href="#4-分析" class="headerlink" title="4. 分析"></a>4. 分析</h3><ul><li>主要参考：对ACM的学术文献相对较多，可以作为主要参考</li><li>增加寿命与ACM关系非线性：显然增加寿命与ACM关系是非线性函数，这里假设 <code>ΔLifeSpan=(1/(1+ΔACM)-1)*10</code>（ΔACM为ACM变化值；公式欢迎优化）</li><li>变量无法简单叠加：显然各个变量之间并不符合独立同分布假设，变量之间的实际影响也并不明确</li><li>存在矛盾观点：所有的证据都有文献&#x2F;研究对应，但注意到：有些文献之间有显著矛盾的观点（如对于碳水摄入比例的矛盾）；有些文献存在较大争议（如认为22点前睡觉会提升43%全因死亡率）</li><li>研究仅表达相关：所有文献表明的更多是相关而非因果，在阅读时要考虑文献是否充分证明了因果 —— 如某文献表明了日均&gt;&#x3D;7000步的人有显著低的全因死亡率。但步数少的人可能包含更多长期病患，如果没有合理的排除这块数据，那此文献调查失真</li></ul><h3 id="5-行动"><a href="#5-行动" class="headerlink" title="5. 行动"></a>5. 行动</h3><ul><li>输入<ul><li>固体：吃白肉（-11%~-3% ACM）、蔬果为主（-26%~-17% ACM），多吃辣（-23% ACM），多吃坚果（-27%~-4% ACM），中量碳水、多吃植物蛋白（-10% ACM），少吃超加工食物（-62%~-18%）</li><li>液体：喝咖啡（-22%~-12% ACM），喝牛奶（-17%~-10% ACM），喝茶（-15%~-8% ACM），少喝或不喝甜味饮料（否则每天一杯+7% ACM，+多巴胺），戒酒或每周100g（纯酒精量(g)&#x3D;饮酒量(ml)×酒精浓度(%)×酒精密度0.8g&#x2F;ml）内（否则+~50% ACM，无上限）</li><li>气体：不吸烟（否则+<del>50% ACM，-12\</del>-11年寿命）</li><li>光照：晒太阳（-~40% ACM）</li><li>药物：二甲双胍（糖尿病人相比正常人可以+3年）、复合维生素（-8%癌症风险）、亚精胺（-60%~-30% ACM）、葡萄糖胺（-39% ACM）</li></ul></li><li>输出<ul><li>运动：每周3次45分钟挥拍运动（-47% ACM）</li><li>日常：刷牙（-25% ACM）</li><li>睡眠：每天睡7小时全因死亡率最低；且22-24点间最好，<em>早睡+43% ACM，晚睡+15% ACM（存在争议）</em></li></ul></li><li>上下文<ul><li>体重：减肥（-54% ACM）</li></ul></li></ul><h3 id="6-证据"><a href="#6-证据" class="headerlink" title="6. 证据"></a>6. 证据</h3><h4 id="6-1-输入"><a href="#6-1-输入" class="headerlink" title="6.1. 输入"></a>6.1. 输入</h4><h5 id="6-1-1-固体"><a href="#6-1-1-固体" class="headerlink" title="6.1.1. 固体"></a>6.1.1. 固体</h5><ul><li>白肉<ul><li><a href="https://zhuanlan.zhihu.com/p/268401670">JAMA子刊：食用红肉和加工肉类会增加心脏病和死亡风险！鱼肉和家禽肉则不会</a><ul><li>出处：<a href="https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2759737/jamainternal_zhong_2020_oi_190112.pdf">Associations of Processed Meat, Unprocessed Red Meat, Poultry, or Fish Intake With Incident Cardiovascular Disease and All-Cause Mortality</a></li><li>增加红肉摄入与死亡风险相关。八年内平均每天增加至少半份红肉摄入（半份红肉相当于14g加工红肉或40g非加工红肉）的调查对象，在接下来八年内全因死亡风险增加10％（HR, 1.10; 95%CI, 1.04-1.17）；每周吃两份红肉或加工肉类（但不包括家禽或鱼类）会使全因死亡风险增加3%</li><li><img src="https://user-images.githubusercontent.com/2707039/163703960-6f321de5-4daa-4ea5-95b9-af9c96f1c1bc.jpg" alt="红肉"></li></ul></li><li><a href="https://www.zhihu.com/question/67223570/answer/809785380">红肉和白肉最大的区别是什么？为啥要这么分呢？</a></li></ul></li><li>蔬果<ul><li><a href="https://www.sohu.com/a/322360740_164924">每年54万人死亡，竟是因为水果吃得少！？这已成十大死亡因素之一！</a><ul><li>出处：<a href="https://academic.oup.com/cdn/article-abstract/3/Supplement_1/nzz028.FS01-01-19/5516583">Estimated Global, Regional, and National Cardiovascular Disease Burdens Related to Fruit and Vegetable Consumption: An Analysis from the Global Dietary Database (FS01-01-19) </a></li><li>每天摄入200克新鲜水果可使死亡率降低17%，糖尿病大血管并发症（如中风、缺血性心脏病等）风险降低13%，及糖尿病小血管并发症（如糖尿病肾病、糖尿病眼病、糖尿病足病等）风险降低28%</li></ul></li><li><a href="https://mp.weixin.qq.com/s/E6BAi-Vnhr1jXBm0Pys2ZQ">《自然》子刊：每天二两西兰花，健康长寿都有啦！分析近6万人23年的数据发现，吃含黄酮类食物与死亡风险降低20%相关丨临床大发现</a><ul><li>出处：<a href="https://www.nature.com/articles/s41467-019-11622-x">Flavonoid intake is associated with lower mortality in the Danish Diet Cancer and Health Cohort</a></li><li>吃含黄酮类食物与死亡风险降低20%相关</li><li><img src="https://user-images.githubusercontent.com/2707039/163703969-42e64f88-e727-4e7d-85f2-07a92e29b613.jpg" alt="黄酮"></li><li>Bondonno博士说道“吃不同蔬菜、水果补充，不同种类的黄酮类化合物是很重要的，这很容易通过饮食实现：一杯茶、一个苹果、一个橘子、100克蓝莓，或100克西兰花，就能提供各种黄酮类化合物，并且总含量超过500毫克。</li></ul></li></ul></li><li>辣椒<ul><li><a href="https://3g.163.com/dy/article/F6Q7I1ME053228ZU.html">辣椒成死亡克星？据调研，常吃辣患病死亡风险可降低61%</a><ul><li>出处1：<a href="https://www.sciencedirect.com/science/article/pii/S0735109719382063">Chili pepper consumption and mortality in Italian adults</a></li><li>出处2：<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169876">The Association of Hot Red Chili Pepper Consumption and Mortality: A Large Population-Based Cohort Study</a></li><li>2017年Plos One 的另一项来自美国的研究以16179名，年龄在18岁以上的人群为对象，并对其进行了高达19年的随访，发现在4946例死亡患者中，食用辣椒的参与者的全因死亡率为21.6％，而未食用辣椒的参与者的全因死亡率为33.6％。相较于不吃辣或很少吃（少于每周两次）的人群，每周吃辣＞4次的人群总死亡风险降低23%，心血管死亡风险降低34%。</li></ul></li></ul></li><li>鸡蛋<ul><li><a href="https://m.thepaper.cn/baijiahao_11540780">每天多吃半个蛋，增加7%的全因和心血管死亡风险？</a><ul><li>出处：<a href="https://dietandhealth.cancer.gov/">NIH-AARP工作主页</a>、<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7872242/">Egg and cholesterol consumption and mortality from cardiovascular and different causes in the United States: A population-based cohort study</a></li><li>每天多吃半个蛋，增加7%的全因和心血管死亡风险？在假设性替代分析中，研究者发现，用等量的蛋清&#x2F;鸡蛋替代物、家禽、鱼、乳制品、坚果和豆类分别替代半只全蛋（25克&#x2F;天）可以降低6%、8%、9%、7%、13%和10%的全因死亡率。<br>*<a href="https://raw.githubusercontent.com/qhy040404/Image-Resources-Repo/master/pmed.1003508.g002.jpg">鸡蛋</a></li></ul></li></ul></li><li>坚果<ul><li><a href="https://www.163.com/dy/article/GKVOMMMF05148PF4.html">哈佛20年研究：吃核桃的人更长寿，显著减少全因死亡，延长寿命</a><ul><li>出处：<a href="https://www.mdpi.com/2072-6643/13/8/2699/pdf">Association of Walnut Consumption with Total and Cause-Specific Mortality and Life Expectancy in US Adults</a></li><li>通过分析发现，经常食用核桃可以延长寿命，降低心血管疾病死亡风险。比起不吃核桃，每周食用核桃5份以上（1份28克）的健康预期寿命延长1.3岁，全因死亡风险降低14%，心血管疾病死亡率降低25%。</li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/44454030">研究：每日食生坚果，死亡率降20%</a><ul><li>出处1：<a href="https://www.nejm.org/doi/full/10.1056/NEJMoa1307352">Association of nut consumption with total and cause-specific mortality</a></li><li>出处2：<a href="https://americanpistachios.cn/sites/china/files/inline-files/APG_Health-%26-Nutrition-Research-Brochure_DEC-19-18.pdf">APG_Health-&amp;-Nutrition-Research-Brochure_DEC-19-18</a></li><li>研究人员发现，每周吃树坚果低于1盎司份量的人，死亡率降低7％。而每周吃了1盎司份量的人，减少11％的死亡率；每周吃2份量的人，减低13％；每周5至6份量者，减少了15％；一周7份以上的人，死亡率则减少20％。</li><li>另外两篇发表在《公共科学图书馆在线期刊》(Public Library of Science Online Journal)和《生物医学中心》(BioMed Central)上的医学预科研究论文，展示了试验开始时的横断面数据。这两项研究都评估了7,216名对象，以及他们食用坚果的频率和数量之间的关系。那些每周食用三份以上坚果(包括开心果)的研究对象的死亡率降低39%。</li></ul></li></ul></li><li>钠（存有大量争议）<ul><li><a href="https://nursing.medsci.cn/article/show_article.do;jsessionid=A34E8A33918A152CB55BDD2E5FB1798D?id=afe720486ee7">Eur Heart J：钠摄入量与预期寿命、全因死亡率的关系</a><ul><li>出处：<a href="https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC8169157&blobtype=pdf">Messerli F H, Hofstetter L, Syrogiannouli L, et al. Sodium intake, life expectancy, and all-cause mortality[J]. European heart journal, 2021, 42(21): 2103-2112.</a></li><li><img src="https://user-images.githubusercontent.com/2707039/164894778-9710f18d-e055-4f62-bdcb-618687771d77.jpeg" alt="ehaa947f6"></li><li>在该分析所包含的181个国家中，研究人员发现钠摄入量与出生时的健康预期寿命（β&#x3D;2.6年&#x2F;克每日钠摄入量，R<sup>2</sup>&#x3D;0.66，P&lt;0.001）和60岁时的健康预期寿命（β&#x3D;0.3年&#x2F;克每日钠摄入量，R<sup>2</sup>&#x3D;0.60，P&#x3D;0.048）之间存在正相关关系，但与非传染性疾病死亡（β&#x3D;17次事件&#x2F;克每日钠摄入量，R<sup>2</sup>&#x3D;0.43，P&#x3D;0.100）无关。相反，全因死亡率与钠摄入量成负相关（β&#x3D;−131次事件&#x2F;克每日钠摄入量，R<sup>2</sup>&#x3D;0.60，P&lt;0.001）。在仅限于46个收入最高国家的敏感性分析中，钠摄入量与出生时的健康预期寿命呈正相关（β&#x3D;3.4年&#x2F;克每日钠摄入量，R<sup>2</sup>&#x3D;0.53，P&lt;0.001），而与全因死亡率（β&#x3D;−168次事件&#x2F;克每日钠摄入量，R<sup>2</sup>&#x3D;0.50，P&lt;0.001）呈负相关。</li><li>该（大范围）研究认为更多的钠摄入与显著更低的全因死亡率有关</li><li><a href="https://www.tctmd.com/news/fresh-foray-salt-wars-life-expectancy-higher-greater-sodium-intake">针对该论文的延伸解读和讨论：A Fresh Foray in the Salt Wars: Life Expectancy Higher With Greater Sodium Intake</a></li></ul></li><li><a href="https://ibook.antpedia.com/x/669028.html">NEJM&#x2F;Lancet：不要吃太多盐，中国饮食所致心血管病和癌症死亡全球第一，吃低钠盐可降低全因死亡率</a><ul><li>但也有多项研究认为用低钠盐可以降低一系列疾病的发生概率，对全因死亡率的减少有积极影响</li></ul></li></ul></li><li>碳水（存有大量争议）<ul><li><a href="https://zhuanlan.zhihu.com/p/137815934">低碳生酮饮食（四）碳水化合物与长期死亡率</a><ul><li>出处：The Lancet Public Health - <a href="https://www.sciencedirect.com/science/article/pii/S246826671830135X">Dietary carbohydrate intake and mortality: a prospective cohort study and meta-analysis</a></li><li>碳水越低，寿命越短；碳水越高，寿命也轻微缩短；碳水50%左右（其实按照一般的说法，这也算高碳水）是最长寿命区间 </li><li><img src="https://user-images.githubusercontent.com/2707039/163703985-a2e2f8ac-101a-4f3c-903b-6850507f144b.jpg" alt="碳水"></li></ul></li><li><a href="https://www.chinacdc.cn/gwxx/202003/t20200323_214639.html">最强营养搭配！BMJ：这么吃，心血管疾病和死亡风险更低</a></li></ul></li><li>槟榔<ul><li><a href="https://www.zhihu.com/question/312784161/answer/603370131">如何看待槟榔嚼出来的癌症？槟榔致癌风险究竟有多大？ - 丁香医生的回答 - 知乎</a><ul><li>出处：Chewing Betel Quid and the Risk of Metabolic Disease, Cardiovascular Disease, and All-Cause Mortality: A Meta-Analysis(<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0070679">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0070679</a>)</li><li>嚼槟榔会增加21%的全因死亡率</li></ul></li></ul></li><li>热量限制<ul><li><a href="https://www.zhihu.com/question/31395511">怎么看待BBC《进食、断食与长寿》？</a><ul><li>限制卡路里动物实验：CR（热量限制，即少吃）延迟了恒河猴的多种疾病发病和死亡率，与CR动物相比，正常喂养的猴子的各种疾病患病风险增加2.9倍，死亡风险增加3.0倍。</li><li><img src="https://user-images.githubusercontent.com/2707039/163703988-8767185b-326a-4783-b2e2-f190322bb7d6.jpg" alt="热量限制-恒河猴"></li></ul></li></ul></li><li>综合<ul><li><a href="https://www.chinacdc.cn/gwxx/202003/t20200323_214639.html">最强营养搭配！BMJ：这么吃，心血管疾病和死亡风险更低</a></li><li><a href="https://doi.org/10.1136/bmj.m688">Associations of fat and carbohydrate intake with cardiovascular disease and mortality: prospective cohort study of UK Biobank participants</a><ul><li>通过对这些参与者的数据进行分析，研究人员发现碳水化合物（糖、淀粉和纤维）和蛋白质的摄入与全因死亡率呈非线性关系，而脂肪则与全因死亡率呈线性相关。其中，较高的糖分摄入与全因死亡风险和患心血管疾病的风险较高均有关联，而较高的饱和脂肪酸摄入与全因死亡风险较高有关。</li><li>图1：各种营养元素与全因死亡之间的关系</li><li><img src="https://user-images.githubusercontent.com/2707039/163702022-8c2bfea9-ed5d-4fe0-8ead-e8740014b92b.jpg" alt="各种营养元素与全因死亡之间的关系"></li><li>图2：各种营养元素与心血管疾病之间的关系</li><li><img src="https://user-images.githubusercontent.com/2707039/163702084-97fb4a03-707c-475d-b88e-6fe2f8e87f92.jpg" alt="各种营养元素与心血管疾病之间的关系"></li><li><strong>进一步研究表明，在所有的饮食模式中，全因死亡率风险最低的饮食方式为：10-30g高纤维、14-30%蛋白质、10-25%单不饱和脂肪酸、5%-7%多不饱和脂肪酸以及20%-30%淀粉摄入。</strong></li><li><strong>最优能量来源配比：&lt;24%淀粉，15%-17%蛋白质，&gt;15%单不饱和脂肪酸，&lt;15%糖，6%饱和脂肪酸，6%多不饱和脂肪酸，30g+高纤维</strong></li></ul></li><li><a href="https://med.ckcest.cn/details.html?id=5183272274855936&classesEn=news">BMJ | 常吃薯片汉堡巧克力等食品，平均死亡年龄仅仅为58岁，死亡风险剧增</a><ul><li><a href="https://www.bmj.com/content/365/bmj.l1949.full">Rico-Campà A, Martínez-González M A, Alvarez-Alvarez I, et al. Association between consumption of ultra-processed foods and all cause mortality: SUN prospective cohort study[J]. bmj, 2019, 365.</a></li><li><a href="https://www.bmj.com/content/365/bmj.l1451">Srour B, Fezeu L K, Kesse-Guyot E, et al. Ultra-processed food intake and risk of cardiovascular disease: prospective cohort study (NutriNet-Santé)[J]. bmj, 2019, 365.</a></li><li><a href="https://www.researchgate.net/profile/Phillip-Baker-5/publication/333483796_Ultra-processed_food_and_adverse_health_outcomes/links/5f0c646ca6fdcc2f32336a95/Ultra-processed-food-and-adverse-health-outcomes.pdf">Lawrence M A, Baker P I. Ultra-processed food and adverse health outcomes[J]. bmj, 2019, 365.</a></li></ul></li></ul></li></ul><h5 id="6-1-2-液体"><a href="#6-1-2-液体" class="headerlink" title="6.1.2. 液体"></a>6.1.2. 液体</h5><ul><li>牛奶<ul><li><a href="https://www.sohu.com/a/253940257_419768">《柳叶刀》调研21个国家13万人：每天1斤牛奶或酸奶，心血管死亡风险下降23%</a></li><li>出处：<a href="http://mdrf-eprints.in/1114/1/Association_of_dietary_patterns_and_dietary_diversity_with_cardiometabolic_disease_risk_factors.pdf">Association of dairy intake with cardiovascular disease and mortality in 21 countries from five continents (PURE): a prospective cohort study</a></li><li>与不食用乳制品的人相比，每天摄入两份乳制品（一份指244克牛奶&#x2F;酸奶，15克奶酪或5克黄油）的人，**全因死亡风险下降了17%**，心血管死亡风险下降23%，中风风险下降33%</li></ul></li><li>茶<ul><li><a href="https://www.jianshu.com/p/5461a205cf95?utm_campaign=hugo">10万中国人随访7年发现，每周喝三次茶与全因死亡风险降低15%，预期寿命增加1.26年相关 </a></li><li>出处：<a href="https://www.researchgate.net/profile/Fangchao-Liu-4/publication/338483323_Tea_consumption_and_the_risk_of_atherosclerotic_cardiovascular_disease_and_all-cause_mortality_The_China-PAR_project/links/5e55e5e94585152ce8efe511/Tea-consumption-and-the-risk-of-atherosclerotic-cardiovascular-disease-and-all-cause-mortality-The-China-PAR-project.pdf">Tea consumption and the risk of atherosclerotic cardiovascular disease and all-cause mortality: The China-PAR project</a></li><li><a href="http://rs.yiigle.com/CN112338202202/1351516.htm">中国成年人饮茶与死亡风险的前瞻性关联研究</a></li><li>纳入分析的438 443例研究对象随访11.1年共发生死亡34 661例。与从不饮茶者相比，当前非每日饮茶者和每日饮茶者全因死亡HR值（95%CI）依次为0.89（0.86-0.91）和0.92（0.88-0.95）。分性别分析显示，饮茶对全因死亡风险的保护作用主要见于男性（交互P&lt;0.05）</li></ul></li><li>无糖（甜味）饮料<ul><li><a href="https://www.zhihu.com/question/418598272/answer/1450648364">「无糖饮料使死亡风险增加 26 %」，是真的吗？</a><ul><li>相比于软饮料摄入量＜1杯&#x2F;月的参与者，混合软饮料摄入≥1杯&#x2F;天的参与者死亡风险增加18%，而<strong>摄入含糖软饮料或无糖软饮料会令死亡风险分别增加11%和27%。</strong></li><li><img src="https://user-images.githubusercontent.com/2707039/163704346-e7d92e7f-eba5-4673-8f15-3a96782c2e32.png" alt="饮料"></li></ul></li><li><a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2749350">Association Between Soft Drink Consumption and Mortality in 10 European Countries</a></li></ul></li><li>有糖饮料<ul><li><a href="https://zhuanlan.zhihu.com/p/400746073">可乐和奶茶，增加全因死亡率高达62%！果汁降低免疫力，影响肝代谢！含糖饮料那些事</a><ul><li>每天1杯含糖饮料增加7%全因死亡率，2杯21%</li><li>在34年的随访中，研究人员发现，相比那些一个月喝1杯或者更少含糖饮料的人，每天喝2杯的人总体死亡风险升高了21%，心血管疾病死亡风险升高了31%，癌症死亡风险上升了16%。</li><li>只要每天多喝一杯含糖饮料，总体死亡风险将增加7%，心血管疾病的风险将增加10%，癌症相关的死亡风险将16%。</li><li>发表在国际顶级期刊《BMJ》上的一篇论文就证明了含糖饮料会在增加患癌风险，当然这篇文章验证的不仅仅是果汁，奶茶也有份——和含糖饮料相关的总体患癌风险要高出通常值18%，100%的鲜榨果汁也会使得整体的患癌风险上升12%。</li></ul></li></ul></li><li>果汁<ul><li><a href="https://zhuanlan.zhihu.com/p/66513350">JAMA子刊：100%纯果汁可能比含糖饮料更危险</a><ul><li>每天多摄入一份12盎司的含糖饮料，全因死亡率风险增加11%；</li><li>每天多摄入一份12盎司的果汁，全因死亡率风险增加24%。</li></ul></li></ul></li><li>咖啡<ul><li><a href="https://news.bioon.com/article/6725420.html">重磅！多篇研究证实喝咖啡与人群全因死亡率降低直接相关</a></li><li><a href="https://www.sohu.com/a/439412995_100003595">科普 | 喝咖啡又多了一个新理由：降低死亡率！ </a></li><li><a href="https://fanyi.pdf365.cn/help/249">地中海成年人咖啡消耗量及全因，心血管疾病和癌症的死亡率</a><ul><li>在最近的荟萃分析中，该研究包括来自不同国家的40项研究和3,852,651名受试者。在这项荟萃分析显示，咖啡摄入量与各种原因的死亡率，CVD和癌症死亡率之间存在非线性关系，每天摄入两杯咖啡的癌症死亡率最低(RR &#x3D; 0.96)，CVD最低的死亡率，每天2.5杯(RR&#x3D; 0.83)，全天最低死亡率为每天3.5杯(RR&#x3D; 0.85)，并且随着咖啡消费量的增加，死亡率没有进一步降低或增加</li></ul></li></ul></li><li>亚精胺<ul><li><a href="https://www.medsci.cn/article/show_article.do?id=420d12904103">Science：科学背书！从精液中发现的亚精胺，竟然有着抗衰老、抗癌、保护心血管和神经、改善肥胖和2型糖尿病等逆天神效</a></li><li><a href="https://zhuanlan.zhihu.com/p/388942219">饮食中亚精胺摄入量高会降低死亡率</a></li></ul></li></ul><h5 id="6-1-3-气体"><a href="#6-1-3-气体" class="headerlink" title="6.1.3. 气体"></a>6.1.3. 气体</h5><ul><li>吸烟<ul><li><a href="https://www.medsci.cn/article/show_article.do?id=02ca2083319b">即使是低强度吸烟，也增加死亡风险！</a><ul><li>研究发现：在42 416名男性和86 735名女性（年龄在35-89岁之间，以前没有患病）中，18 985名男性（45%）和18 072名女性（21%）目前吸烟，其中33%的男性吸烟者和39%的女性吸烟者并不每天吸烟。8866名男性（21%）和53 912名女性（62%）从不吸烟。在随访期间，与从不吸烟相比，每天&lt;10支烟或每天≥10支烟的全因死亡率危险比分别为1.17（95%置信区间1.10-1.25）和1.54（1.42-1.67）。无论年龄或性别，危险比相似。与每日吸烟关系最密切的疾病是呼吸道癌症、慢性阻塞性肺病和胃肠道及血管疾病。在招募时已经戒烟的人的死亡率低于现在每天吸烟者。</li><li>吸烟者平均减少寿命11-12年</li></ul></li><li><a href="https://www.zhihu.com/question/24846224/answer/1719798177">吸烟让人过瘾是什么原理？有节制的吸烟依旧有害吗？</a></li></ul></li></ul><h5 id="6-1-4-光照"><a href="#6-1-4-光照" class="headerlink" title="6.1.4. 光照"></a>6.1.4. 光照</h5><ul><li>晒太阳<ul><li><a href="https://zhuanlan.zhihu.com/p/76301306">晒太阳和死亡率的关系，如何科学，安全的晒太阳？</a><ul><li>丹麦一项长达26年的研究发现，多晒太阳能显著延长寿命，即使是由于过度暴晒诱发皮肤癌的患者，平均寿命也比普通人长了6岁。</li></ul></li></ul></li></ul><h5 id="6-1-5-药物"><a href="#6-1-5-药物" class="headerlink" title="6.1.5. 药物"></a>6.1.5. 药物</h5><ul><li>NMN</li><li>二甲双胍<ul><li><a href="https://zhuanlan.zhihu.com/p/419202902">“胍”吹必看 丨我就是神药——二甲双胍</a><ul><li>二甲双胍不仅在多种肿瘤、心血管疾病及糖尿病中发挥保护作用，而且在肥胖、肝病、肾病及衰老方面也大放异彩。</li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/357807109">二甲双胍2020最值得了解的“吃瓜”大新闻——护胃、健脑、抗衰、防癌还是致癌？</a></li><li><a href="https://baijiahao.baidu.com/s?id=1729999374705305768">二甲双胍真的那么神吗？美研究：父亲服用二甲双胍或致子女有缺陷</a></li><li><img src="https://user-images.githubusercontent.com/2707039/163702325-5d427542-9ae5-4311-8979-d0d326a9832f.jpg" alt="二甲双胍"></li><li>不良反应<ul><li>作为一种使用近百年的药物，二甲双胍的不良反应已经非常明确，常见的有：维生素B12缺乏（7%-17.4%），胃肠道不良反应（最高53%），疲倦（9%），头痛（6%）；严重但不常见的不良反应包括乳酸酸中毒、肝损伤；也有研究表明可能对胎儿致畸</li></ul></li></ul></li><li>复合维生素<ul><li><a href="https://health.qq.com/a/20121023/000026.htm">服用复合维生素可降低癌症危险8%，其他效果并不显著</a></li></ul></li><li>葡萄糖胺<ul><li><a href="https://www.sohu.com/a/436372221_120873241">神奇！氨糖降低心血管死亡率65%，与定期运动效果相当</a></li><li>美国西弗吉尼亚大学最新研究发现 氨糖（软骨素） 可以降低心血管死亡率65%，降低总体死亡率39%，效果与坚持定期运动相对</li><li>该研究使用1999年至2010年，16,686名成年人的国家健康和营养检查(NHANES)数据，参与者的中位追踪时间为107个月，而其中有648位参与者定期且每服用日500-1000毫克的葡萄糖胺&#x2F;软骨素一年以上。</li></ul></li><li>亚精胺<ul><li><a href="https://www.medsci.cn/article/show_article.do?id=420d12904103">Science：科学背书！从精液中发现的亚精胺，竟然有着抗衰老、抗癌、保护心血管和神经、改善肥胖和2型糖尿病等逆天神效</a></li><li>亚精胺是最容易从人体肠道吸收的多胺。许多的食物中都含有大量的亚精胺，例如新鲜的青椒、小麦胚芽、花椰菜、西兰花、蘑菇和各种奶酪，尤其在纳豆等大豆制品、香菇和榴莲中含量更高。在本实验中，研究人员选择了829位年龄在45-84岁之间的参与者进行了为期20年的随访，分析了饮食中亚精胺摄入量与人类死亡率之间的潜在关联。</li><li>研究发现，女性的亚精胺摄入量高于男性，并且摄入量都会随着年龄的增长而下降。亚精胺的主要来源是全谷物（占13.4%）、苹果和梨（占13.3%）、沙拉（占9.8%）、芽菜（占7.3%）和马铃薯（占6.4%）。研究根据亚精胺摄入量将人群分为三组，低摄入量组（&lt;62.2 µmol &#x2F; d）、中摄入量组（62.2–79.8 µmol &#x2F; d）和高摄入量组（&gt; 79.8 µmol &#x2F; d）。随访期间共记录了341例死亡，其中血管疾病137例，癌症94例，其他原因110例。经计算低中高三组的粗略死亡率分别为40.5%、23.7%和15.1%，这些数据表明亚精胺摄入量与全因死亡率之间的负相关关系显著。随着逐步对年龄、性别和热量的比例进行调整，这种相关关系依然显著。</li></ul></li><li>综合<ul><li><a href="https://zhuanlan.zhihu.com/p/145495570">《自然》子刊深度综述：如何开发抗衰老药</a></li><li><img src="https://user-images.githubusercontent.com/2707039/163702474-205baeec-f0ce-4e8d-96a4-36efe47534de.jpg" alt="如何开发抗衰老药"></li></ul></li></ul><h4 id="6-2-输出"><a href="#6-2-输出" class="headerlink" title="6.2. 输出"></a>6.2. 输出</h4><h5 id="6-2-1-挥拍运动"><a href="#6-2-1-挥拍运动" class="headerlink" title="6.2.1. 挥拍运动"></a>6.2.1. 挥拍运动</h5><ul><li><a href="https://www.sohu.com/a/535581770_121124216">哪种运动性价比最高？权威医学杂志“柳叶刀”给出答案了 </a><ul><li>一周三次，每次45-60分钟，挥拍运动，降低~47%全因死亡率</li><li>羽毛球、乒乓球、网球等都算挥拍运动，但由于西化研究背景，可能指网球更多。这隐式的表达了全身锻炼更为重要</li></ul></li></ul><h5 id="6-2-2-剧烈运动"><a href="#6-2-2-剧烈运动" class="headerlink" title="6.2.2. 剧烈运动"></a>6.2.2. 剧烈运动</h5><ul><li><a href="https://academic.oup.com/eurheartj/advance-article/doi/10.1093/eurheartj/ehac572/6771381">新研究：每天剧烈运动8分钟，可降低全因死亡和心脏病风险</a><ul><li>每周15-20分钟的剧烈运动，降低16-40%的全因死亡率，剧烈运动时间达到50-57分钟&#x2F;周，可以进一步降低全因死亡率。这些发现表明，通过在一周的短时间内累积相对少量的剧烈运动可以降低健康风险。</li></ul></li></ul><h5 id="6-2-3-走路"><a href="#6-2-3-走路" class="headerlink" title="6.2.3. 走路"></a>6.2.3. 走路</h5><ul><li><a href="http://www.shcell.org/219/3571.html">走路降低全因死亡率超过50%！每天走多少步最合适？《JAMA》子刊超10年研究告诉你答案</a><ul><li><img src="https://user-images.githubusercontent.com/2707039/163704147-afec1c79-799b-4db8-b547-1a2431d504c9.jpg" alt="走路降低全因死亡率"></li><li>注1：这项研究参与者的平均年龄为45.2岁</li><li>注2：平均步数的多少与职业有关，此项研究仅表明相关性，还没有更深度的因果分析</li></ul></li></ul><h5 id="6-2-4-刷牙"><a href="#6-2-4-刷牙" class="headerlink" title="6.2.4. 刷牙"></a>6.2.4. 刷牙</h5><ul><li><a href="https://www.cn-healthcare.com/articlewm/20211209/content-1293760.html">50万国人研究证实：不好好刷牙，致癌！血管疾病也会增多！</a><ul><li>经常不刷牙的人：癌症、慢性阻塞性肺病及肝硬化风险分别增加了9%、12%和25%，过早死亡风险增加25%。</li></ul></li></ul><h5 id="6-2-5-泡澡"><a href="#6-2-5-泡澡" class="headerlink" title="6.2.5. 泡澡"></a>6.2.5. 泡澡</h5><ul><li><a href="https://www.cn-healthcare.com/article/20200326/content-533379.html">定期洗澡降低心血管疾病发作风险</a><ul><li>与每周一至两次泡澡或根本不泡澡相比，每天洗热水澡可以降低28%的心血管疾病总风险，降低26%的中风总风险，脑出血风险下降46%。而浴缸浴的频率与心源性猝死的风险增加无关。</li></ul></li></ul><h5 id="6-2-6-做家务（老年男性）"><a href="#6-2-6-做家务（老年男性）" class="headerlink" title="6.2.6. 做家务（老年男性）"></a>6.2.6. 做家务（老年男性）</h5><ul><li><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0061529">Housework Reduces All-Cause and Cancer Mortality in Chinese Men</a><ul><li>72岁之后男性每周做重型家务可以减少29%平均死亡率</li><li>重型家务：吸尘、擦地板、拖地、擦洗窗户、洗车、搬动家具、搬煤气罐等等。</li><li>轻型家务：掸灰尘、洗碗、手洗衣服、熨烫、晾衣服、做饭、买日用品等等。</li></ul></li></ul><h5 id="6-2-7-睡眠"><a href="#6-2-7-睡眠" class="headerlink" title="6.2.7. 睡眠"></a>6.2.7. 睡眠</h5><ul><li><a href="https://med.sina.com/article_detail_103_1_105491.html">超30万亚洲人数据：每天睡几个小时最有益长寿？</a><ul><li>在男性中，与睡眠时长为7小时相比：睡眠持续时间≥10小时与全因死亡风险增加34%相关；</li><li><img src="https://user-images.githubusercontent.com/2707039/163704166-226b7ebb-92ce-4753-a3e7-77a87652a104.jpg" alt="睡眠-男"></li><li>在女性中，与睡眠持续时间7小时相比：睡眠持续时间≥10小时与全因死亡风险增加48%相关；</li><li><img src="https://user-images.githubusercontent.com/2707039/163704169-c5c715aa-7130-403b-b0d1-ec34fab094d8.png" alt="睡眠-女"></li></ul></li><li><a href="https://www.thepaper.cn/newsDetail_forward_14461799">颠覆认知！加拿大研究发现：早睡比熬夜或许更伤身，几点睡才好？</a><ul><li>其中一个结论为，就寝时间与全因死亡率的关联性强，过早睡觉和过晚睡觉都会影响健康，但是早睡增加的全因死亡率比晚睡增加的死亡率高，早睡增加了43%的死亡风险，而晚睡增加了15%的死亡风险。</li><li>这项调查研究，还存在很多局限性，比如没有直接证明就寝时间与死亡的关系，仅仅说明相关性，通过参与人群自我报告统计睡眠时间，数据不够客观</li></ul></li></ul><h5 id="6-2-8-久坐"><a href="#6-2-8-久坐" class="headerlink" title="6.2.8. 久坐"></a>6.2.8. 久坐</h5><ul><li><a href="https://www.chinanutri.cn/yyjkzxpt/yyjkkpzx/yytsg/zgjm/202103/P020210311486742870527.pdf">中国居民膳食指南科学研究报告（2021年）</a><ul><li>久坐和看电视时间与全因死亡、心血管疾病、癌症和2型糖尿病发病高风险相关，是独立风险因素。久坐时间每天每增加1小时，心血管疾病发生风险增加4%，癌症增加1%，全因死亡风险增加3%。全因死亡和CVD死亡风险增加的久坐时间阈值是6~8h&#x2F;d，看电视时间阈值是3~4h&#x2F;d。</li></ul></li><li><a href="https://apps.who.int/iris/bitstream/handle/10665/337001/9789240014947-chi.pdf">世卫组织关于身体活动和久坐行为的指南</a></li></ul><h4 id="6-3-上下文"><a href="#6-3-上下文" class="headerlink" title="6.3. 上下文"></a>6.3. 上下文</h4><h5 id="6-3-1-情绪"><a href="#6-3-1-情绪" class="headerlink" title="6.3.1. 情绪"></a>6.3.1. 情绪</h5><ul><li><a href="https://www.x-mol.com/paper/1288184397379059712/t?recommendPaper=1263704526086578176">悲观情绪与更高的全因死亡率和心血管疾病死亡率有关，但乐观情绪并不能起到保护作用</a></li><li><a href="https://www.nature.com/articles/s41598-020-69388-y?utm_source=xmol&utm_medium=affiliate&utm_content=meta&utm_campaign=DDCN_1_GL01_metadata_scirep">Pessimism is associated with greater all-cause and cardiovascular mortality, but optimism is not protective</a><ul><li>在1993-1995年间，一项针对50岁以上澳大利亚人健康的双胞胎研究中包括了生活取向测试（LOT），其中包含乐观和悲观的项目。平均20年后，参与者与来自澳大利亚国家死亡指数的死亡信息相匹配。在2,978名具有很多可用分数的参与者中，有1,068人死亡。生存分析测试了各种乐观因素和悲观情绪分数与任何原因，癌症，心血管疾病或其他已知原因的死亡率之间的关联。年龄调整后的悲观量表上的核心与全因和心血管疾病死亡率相关（每1个标准差单位的危险比，95％置信区间和p值1.134、1.065–1.207、8.85×10 –5和1.196、1.045–1.368、0.0093 ），但不会因癌症死亡。乐观得分与悲观得分之间的相关性很弱（年龄调整后的等级相关系数&#x3D; − 0.176），但与总死亡率或特定原因死亡率没有显着相关性。反向因果关系（引起悲观情绪的疾病）是不可能的，因为在那种情况下，心血管疾病和癌症都会导致悲观情绪。</li></ul></li></ul><h5 id="6-3-2-贫富"><a href="#6-3-2-贫富" class="headerlink" title="6.3.2. 贫富"></a>6.3.2. 贫富</h5><ul><li><a href="https://www.cn-healthcare.com/articlewm/20210727/content-1246348.html">JAMA子刊：贫富差距真能影响寿命？这可能是真的！</a><ul><li>该研究使用1994-1996年第一次收集的数据，并通过生存模型来分析净资产和长寿之间的关联。结果显示，共收纳5414 名参与者，平均年龄为 46.7岁，包括 2766 名女性。较高的净资产与较低的死亡风险相关。特别是在兄弟姐妹和双胞胎中（n &#x3D; 2490），在较高的净资产和较低的死亡率之间观察到类似的关联，表明拥有更多财富的兄弟姐妹或双胞胎比拥有更少财富的兄弟姐妹&#x2F;双胞胎活得更久。</li></ul></li></ul><h5 id="6-3-3-体重"><a href="#6-3-3-体重" class="headerlink" title="6.3.3. 体重"></a>6.3.3. 体重</h5><ul><li><a href="https://www.chinacdc.cn/gwxx/202009/t20200904_218959.html">JAMA子刊：减肥要趁早，才能有效降低死亡率风险</a><ul><li>对体重减轻的死亡率风险评估发现，体重从肥胖减轻到超重的成年人与稳定肥胖人群相比，全因死亡率降低了54％（危险比为0.46），然而从成年初期的超重减轻到中年以前的正常体重的人群的死亡率风险并未降低（风险比为1.12）。</li><li><img src="https://raw.githubusercontent.com/qhy040404/Image-Resources-Repo/master/zoi200509t3_1596761185.02415.png" alt="Table3"></li></ul></li></ul><h5 id="6-3-4-新冠"><a href="#6-3-4-新冠" class="headerlink" title="6.3.4. 新冠"></a>6.3.4. 新冠</h5><ul><li><a href="https://www.nature.com/articles/s41591-020-1112-0.pdf">Magnitude, demographics and dynamics of the effect of the first wave of the COVID-19 pandemic on all-cause mortality in 21 industrialized countries</a><ul><li>目前来看，新冠死亡率（美国）在1.5%左右，人均预期寿命减少了2年</li></ul></li><li><a href="https://www.zhihu.com/question/510943670/answer/2308499719">如何看待美国CDC宣称新冠死亡人数被高估？</a></li><li><a href="https://www.cdc.gov/nchs/nvss/deaths.htm">NVSS deaths</a></li></ul>]]></content>
    
    
    <summary type="html">alive</summary>
    
    
    
    <category term="alive" scheme="https://blog.yongwang.lu/categories/alive/"/>
    
    
    <category term="alive" scheme="https://blog.yongwang.lu/tags/alive/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）7、健康检查详解</title>
    <link href="https://blog.yongwang.lu/post/bb9b1e8e.html"/>
    <id>https://blog.yongwang.lu/post/bb9b1e8e.html</id>
    <published>2023-01-05T08:40:00.000Z</published>
    <updated>2023-01-05T08:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><blockquote><p>Kubernetes中的健康检查主要使用 就绪性探针（<code>readinessProbes</code>）和 存活性探针（<code>livenessProbes</code>） 来实现，service即为负载均衡，k8s保证 service 后面的 pod 都可用，是k8s中自愈能力的主要手段，主要基于这两种探测机制，可以实现如下需求：</p></blockquote><ul><li><p>异常实例自动剔除，并重启新实例</p></li><li><p>多种类型探针检测，保证异常pod不接入流量</p></li><li><p>不停机部署，更安全的滚动升级</p></li></ul><p><img src="https://res.yongwang.lu/img/20230322232410.png" alt="图片"><br>官方文档：<a href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</a>  </p><h3 id="1）k8s中的探针种类"><a href="#1）k8s中的探针种类" class="headerlink" title="1）k8s中的探针种类"></a>1）k8s中的探针种类</h3><h4 id="1、就绪检查（readinessProbe，就绪探针）"><a href="#1、就绪检查（readinessProbe，就绪探针）" class="headerlink" title="1、就绪检查（readinessProbe，就绪探针）"></a>1、就绪检查（readinessProbe，就绪探针）</h4><blockquote><p><code>readiness probes</code>准备就绪检查，通过readiness是否准备接受流量，准备完毕加入到<code>Endpoint</code>，否则剔除。如果容器不提供就绪探针，则<strong>默认状态为 Success</strong>。</p></blockquote><h4 id="2、存活检查（livenessProbe，存活探针）"><a href="#2、存活检查（livenessProbe，存活探针）" class="headerlink" title="2、存活检查（livenessProbe，存活探针）"></a>2、存活检查（livenessProbe，存活探针）</h4><blockquote><p><code>liveness probes</code>在线检查机制，检查应用是否可用，如死锁，无法响应，异常时将根据<code>restartPolicy</code>来设置 Pod 状态会自动重启容器，如果容器不提供存活探针，则<strong>默认状态为 Success</strong>。</p></blockquote><p><code>restartPolicy</code>有三个可选值：</p><ul><li><p><code>Always</code>：当容器终止退出后，总是重启容器，<strong>默认策略</strong>。</p></li><li><p><code>OnFailure</code>：当容器异常退出（退出状态码非0）时，才重启容器。</p></li><li><p><code>Never</code>：当容器终止退出，从不重启容器。</p></li></ul><h4 id="3、启动检查（startupProbe，启动探针，1-17-版本新增）"><a href="#3、启动检查（startupProbe，启动探针，1-17-版本新增）" class="headerlink" title="3、启动检查（startupProbe，启动探针，1.17 版本新增）"></a>3、启动检查（startupProbe，启动探针，1.17 版本新增）</h4><ul><li><p><code>startupProbes</code> 启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被前面的探针kill掉。</p></li><li><p>判断容器内的应用程序是否已启动，主要针对于不能确定具体启动时间的应用。如果匹配了 <code>startupProbes</code> 探测，则在 <code>startupProbes</code> 状态为 Success 之前，其他所有探针都处于无效状态，直<strong>到它成功后其他探针才起作用</strong>。</p></li><li><p>如果 <code>startupProbe</code> 失败，kubelet 将杀死容器，容器将根据 <code>restartPolicy</code> 来重启。如果容器没有配置 <code>startupProbe</code>，则<strong>默认状态为 Success</strong>。其实一般主要是设置上面两种即可。</p></li></ul><p><strong>就绪、存活两种探针的区别：</strong></p><blockquote><p><strong>readinessProbe</strong> 和 <strong>livenessProbe</strong> 可以使用相同探测方式，只是对 Pod 的处置方式不同。</p></blockquote><ul><li><p><strong>livenessProbe</strong> 当检测失败后，将杀死容器并根据 Pod 的<strong>重启策略来决定作出对应的措施</strong>。</p></li><li><p><strong>readinessProbe</strong> 当检测失败后，将 Pod 的 IP:Port 从对应的 <code>EndPoint</code> 列表中删除。</p></li></ul><h3 id="2）k8s中的三种探测方式"><a href="#2）k8s中的三种探测方式" class="headerlink" title="2）k8s中的三种探测方式"></a>2）k8s中的三种探测方式</h3><blockquote><p>每种探测机制支持三种健康检查方法，分别是命令行exec，httpGet和tcpSocket，其中exec通用性最强，适用与大部分场景，tcpSocket适用于TCP业务，httpGet适用于web业务。</p></blockquote><ul><li><p><code>exec</code>（自定义健康检查）：在容器中执行指定的命令，如果执行成功，退出码为 0 则探测成功。</p></li><li><p><code>httpGet</code>：通过容器的IP地址、端口号及路径调用 HTTP Get方法，如果响应的状态码大于等于200且小于400，则认为容器 健康。</p></li><li><p><code>tcpSocket</code>：通过容器的 IP 地址和端口号执行 TCP 检 查，如果能够建立 TCP 连接，则表明容器健康。</p></li></ul><p>探针探测结果有以下值：</p><ul><li><p><code>Success</code>：表示通过检测。</p></li><li><p><code>Failure</code>：表示未通过检测。</p></li><li><p><code>Unknown</code>：表示检测没有正常进行。</p></li></ul><h2 id="二、readinessProbe（就绪性探针）"><a href="#二、readinessProbe（就绪性探针）" class="headerlink" title="二、readinessProbe（就绪性探针）"></a>二、readinessProbe（就绪性探针）</h2><ul><li><p><strong>readiness probe</strong> 就绪性探针，用于判断容器内的程序是否存活（或者说是否健康），只有程序(服务)正常， 容器开始对外提供网络访问（启动完成并就绪）；</p></li><li><p>容器启动后按照<code>readiness probe</code>配置进行探测，无问题后结果为成功即状态为 <code>Success</code>；</p></li><li><p>pod的<code>READY</code>状态为 true，从0&#x2F;1变为1&#x2F;1。如果失败继续为0&#x2F;1，状态为 false；</p></li><li><p>若<strong>未配置就绪探针</strong>，则**默认状态容器启动后为<code>Success</code>**。对于此pod、此pod关联的<code>Service</code>资源、<code>EndPoint</code> 的关系也将基于 Pod 的 <code>Ready</code> 状态进行设置；</p></li><li><p>如果 Pod 运行过程中 <strong>Ready 状态变为 false</strong>，则系统自动<strong>从 <code>Service</code>资源 关联的 <code>EndPoint</code>列表中去除此pod</strong>，届时service资源接收到GET请求后，<code>kube-proxy</code>将一定不会把流量引入此pod中，通过这种机制就能防止将流量转发到不可用的 Pod 上。</p></li><li><p>如果 <strong>Pod 恢复为 Ready 状态</strong>。将再<strong>会被加回 <code>Endpoint</code> 列表</strong>。<code>kube-proxy</code>也将有概率通过负载机制会引入流量到此pod中。</p></li></ul><h2 id="三、livenessProbe（存活性探针）"><a href="#三、livenessProbe（存活性探针）" class="headerlink" title="三、livenessProbe（存活性探针）"></a>三、livenessProbe（存活性探针）</h2><ul><li><p><strong>liveness probe</strong>存活性探针，用于判断容器是不是健康，<strong>如果不满足健康条件</strong>，<strong>那么 Kubelet 将根据 Pod 中设置的 <code>restartPolicy</code> （重启策略）来判断，Pod 是否要进行重启操作</strong>；</p></li><li><p>LivenessProbe按照配置去探测 ( <strong>进程、或者端口、或者命令执行后是否成功等等</strong>)，来判断容器是不是正常；</p></li><li><p>如果探测不到，代表容器不健康（可以配置连续多少次失败才记为不健康），则 kubelet 会杀掉该容器，并根据容器的重启策略做相应的处理；</p></li><li><p>如果<strong>未配置存活探针</strong>，则<strong>默认容器启动为通过（Success）状态</strong>。即探针返回的值永远是 <code>Success</code>。即Success后pod状态是<code>RUNING</code>。</p></li></ul><h2 id="四、实战演示"><a href="#四、实战演示" class="headerlink" title="四、实战演示"></a>四、实战演示</h2><p>常用的探针可选参数：</p><table><thead><tr><th>参数名称</th><th>默认值</th><th>最小值</th><th>描述</th></tr></thead><tbody><tr><td>initialDelaySeconds</td><td>0秒</td><td>0秒</td><td>探测延迟时长，容器启动后多久开始进行第一次探测工作。</td></tr><tr><td>periodSeconds</td><td>10秒</td><td>1秒</td><td>探测频度，频率过高会对pod带来较大的额外开销，频率过低则无法及时反映容器产生的错误。</td></tr><tr><td>timeoutSeconds</td><td>1秒</td><td>1秒</td><td>探测的超时时长。</td></tr><tr><td>failureThreshold</td><td>3</td><td>1</td><td>处于成功状态时，探测连续失败几次可被认为失败。</td></tr><tr><td>successThreshold</td><td>1</td><td>1</td><td>处于失败状态时，探测连续成功几次，被认为成功。</td></tr></tbody></table><h3 id="1）exec方式"><a href="#1）exec方式" class="headerlink" title="1）exec方式"></a>1）exec方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;exec-liveness.yaml&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Pod</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    test: liveness</span></span><br><span class="line"><span class="string">  name: liveness-exec</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  # 为了测试方便，指定调度机器</span></span><br><span class="line"><span class="string">  nodeName: local-168-182-110</span></span><br><span class="line"><span class="string">  containers:</span></span><br><span class="line"><span class="string">  - name: liveness</span></span><br><span class="line"><span class="string">    image: registry.aliyuncs.com/google_containers/busybox</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">    - /bin/sh</span></span><br><span class="line"><span class="string">    - -c</span></span><br><span class="line"><span class="string">    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600</span></span><br><span class="line"><span class="string">    livenessProbe:</span></span><br><span class="line"><span class="string">      exec:</span></span><br><span class="line"><span class="string">        command:</span></span><br><span class="line"><span class="string">        - cat</span></span><br><span class="line"><span class="string">        - /tmp/healthy</span></span><br><span class="line"><span class="string">      initialDelaySeconds: 5</span></span><br><span class="line"><span class="string">      periodSeconds: 5</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>解释：</p><ul><li><p><code>initialDelaySeconds</code> 字段告诉 kubelet 在执行第一次探测前应该<strong>等待 5 秒</strong>。</p></li><li><p><code>periodSeconds</code> 字段指定了 kubelet 应该<strong>每 5 秒执行一次存活探测</strong>。</p></li><li><p>kubelet 在容器内执行命令 <strong>cat &#x2F;tmp&#x2F;healthy 来进行探测</strong>。</p></li><li><p>如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。</p></li><li><p>如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。</p></li><li><p>当容器启动时，执行如下的命令：</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/bin/sh -c <span class="string">&quot;touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600&quot;</span></span><br></pre></td></tr></table></figure><ul><li>这个容器生命的前 30 秒，&#x2F;tmp&#x2F;healthy 文件是存在的。 所以在这最开始的 30 秒内，执行命令 cat &#x2F;tmp&#x2F;healthy 会返回成功代码。 30 秒之后，执行命令 cat &#x2F;tmp&#x2F;healthy 就会返回失败代码。</li></ul><p>创建 Pod：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最好先拉取镜像，如果是使用docker，就换成docker就行</span></span><br><span class="line">crictl pull registry.aliyuncs.com/google_containers/busybox</span><br><span class="line"></span><br><span class="line">kubectl apply -f exec-liveness.yaml</span><br></pre></td></tr></table></figure><p>【问题】<code>ERRO[0000] unable to determine image API version: rpc error: code = Unavailable desc = connection error: desc = “transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory”</code><br>【解决】原因：未配置endpoints</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crictl config runtime-endpoint unix:///run/containerd/containerd.sock</span><br><span class="line">crictl config image-endpoint unix:///run/containerd/containerd.sock</span><br></pre></td></tr></table></figure><p>查看</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod liveness-exec</span><br></pre></td></tr></table></figure><p>【现象】30s之后检查失败后就重启pod了，又正常了。</p><h3 id="2）httpGet-方式"><a href="#2）httpGet-方式" class="headerlink" title="2）httpGet 方式"></a>2）httpGet 方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;http-liveness.yaml&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Pod</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: liveness-httpget</span></span><br><span class="line"><span class="string">  namespace: default</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  # 为了测试方便，指定调度机器</span></span><br><span class="line"><span class="string">  nodeName: local-168-182-110</span></span><br><span class="line"><span class="string">  containers:</span></span><br><span class="line"><span class="string">  - name: liveness-httpget-container</span></span><br><span class="line"><span class="string">    image: nginx</span></span><br><span class="line"><span class="string">    imagePullPolicy: IfNotPresent</span></span><br><span class="line"><span class="string">    ports:</span></span><br><span class="line"><span class="string">    - name: nginx</span></span><br><span class="line"><span class="string">      containerPort: 80</span></span><br><span class="line"><span class="string">    livenessProbe:</span></span><br><span class="line"><span class="string">      httpGet:</span></span><br><span class="line"><span class="string">        port: nginx</span></span><br><span class="line"><span class="string">        path: /index.html</span></span><br><span class="line"><span class="string">      initialDelaySeconds: 1</span></span><br><span class="line"><span class="string">      periodSeconds: 3</span></span><br><span class="line"><span class="string">      timeoutSeconds: 10</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>解释：</p><ul><li><p><code>initialDelaySeconds</code>字段告诉 kubelet 在执行第一次探测前应该<strong>等待 1 秒</strong>。</p></li><li><p><code>periodSeconds</code> 字段指定了 kubelet <strong>每隔 3 秒</strong>执行一次存活探测。</p></li><li><p>kubelet 会向容器内运行的服务（服务在监听 80 端口）发送一个 HTTP GET 请求来执行探测。</p></li><li><p>如果服务器上<code>/index.html</code>路径下的处理程序返回成功代码，则 kubelet 认为容器是健康存活的。</p></li><li><p>如果处理程序返回失败代码，则 kubelet 会杀死这个容器并将其重启。</p></li><li><p>返回<strong>大于或等于 <code>200</code></strong> 并且<strong>小于 <code>400</code></strong> 的任何代码都<strong>标示成功</strong>，其它返回代码都标示失败。</p></li></ul><p>执行并查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crictl pull nginx</span><br><span class="line">kubectl apply -f http-liveness.yaml</span><br><span class="line">kubectl describe pod liveness-httpget</span><br></pre></td></tr></table></figure><p>删除 Pod 的 index.html 文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it liveness-httpget -- <span class="built_in">rm</span> -rf /usr/share/nginx/html/index.html</span><br><span class="line"><span class="comment"># 再查看</span></span><br><span class="line">kubectl describe pod liveness-httpget</span><br><span class="line">kubectl get pod liveness-httpget</span><br></pre></td></tr></table></figure><ul><li><p>重启原因是 HTTP 探测得到的状态返回码是 404，<code>Liveness probe failed: HTTP probe failed with statuscode: 404</code>。</p></li><li><p>重启完成后，不会再次重启，因为重新拉取的镜像中包含了 index.html 文件。</p></li></ul><p>HTTP Probes 允许针对 httpGet 配置额外的字段：</p><ul><li><p><code>host</code>：连接使用的主机名，默认是 Pod 的 IP。也可以在 HTTP 头中设置 “Host” 来代替。</p></li><li><p><code>scheme</code> ：用于设置连接主机的方式（HTTP 还是 HTTPS）。默认是 “HTTP”。</p></li><li><p><code>path</code>：访问 HTTP 服务的路径。默认值为 “&#x2F;“。</p></li><li><p><code>httpHeaders</code>：请求中自定义的 HTTP 头。HTTP 头字段允许重复。</p></li><li><p><code>port</code>：访问容器的端口号或者端口名。如果数字必须在 1～65535 之间。</p></li></ul><p>你可以通过为探测设置 .httpHeaders 来重载默认的头部字段值；例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">httpHeaders:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Accept</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">application/json</span></span><br><span class="line"></span><br><span class="line"><span class="attr">startupProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">httpHeaders:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">User-Agent</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">MyUserAgent</span></span><br></pre></td></tr></table></figure><h3 id="3）tcpSocket-方式"><a href="#3）tcpSocket-方式" class="headerlink" title="3）tcpSocket 方式"></a>3）tcpSocket 方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; tcp-liveness-readiness.yaml &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Pod</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: liveness-readiness-tcpsocket</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    app: liveness-readiness-tcpsocket</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  # 为了测试方便，指定调度机器</span></span><br><span class="line"><span class="string">  nodeName: local-168-182-110</span></span><br><span class="line"><span class="string">  containers:</span></span><br><span class="line"><span class="string">  - name: liveness-readiness-tcpsocket</span></span><br><span class="line"><span class="string">    image: nginx</span></span><br><span class="line"><span class="string">    ports:</span></span><br><span class="line"><span class="string">    - containerPort: 80</span></span><br><span class="line"><span class="string">    readinessProbe:</span></span><br><span class="line"><span class="string">      tcpSocket:</span></span><br><span class="line"><span class="string">        port: 80</span></span><br><span class="line"><span class="string">      initialDelaySeconds: 5</span></span><br><span class="line"><span class="string">      periodSeconds: 10</span></span><br><span class="line"><span class="string">    livenessProbe:</span></span><br><span class="line"><span class="string">      tcpSocket:</span></span><br><span class="line"><span class="string">        port: 80</span></span><br><span class="line"><span class="string">      initialDelaySeconds: 15</span></span><br><span class="line"><span class="string">      periodSeconds: 20</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>解释：</p><ul><li><p>kubelet 会在容器启动 5 秒后发送第一个<strong>就绪探测（livenessProbe）</strong>。</p></li><li><p>探测器会尝试连接 goproxy 容器的 80 端口。 如果探测成功，这个 Pod 会被标记为就绪状态，kubelet 将继续每隔 10 秒运行一次检测。</p></li><li><p>除了就绪探测，这个配置包括了一个<strong>存活探测（livenessProbe）</strong>。</p></li><li><p>kubelet 会在容器启动 <strong>15 秒后进行第一次存活探测（livenessProbe）</strong>。</p></li><li><p>与就绪探测类似，活跃探测器会尝试连接 goproxy 容器的 80 端口。 如果存活探测失败，容器会被重新启动。<br>执行</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f tcp-liveness-readiness.yaml</span><br><span class="line">kubectl get pod liveness-readiness-tcpsocket</span><br><span class="line">kubectl describe pod liveness-readiness-tcpsocket</span><br></pre></td></tr></table></figure><h3 id="4）使用命名端口"><a href="#4）使用命名端口" class="headerlink" title="4）使用命名端口"></a>4）使用命名端口</h3><blockquote><p>对于 HTTP 或者 TCP 存活检测可以使用命名的 port。</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ports:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">hostPort:</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">    <span class="attr">port:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure><p><strong>完整版配置</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ports:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">hostPort:</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># readinessProbe，就绪探针</span></span><br><span class="line"><span class="attr">readinessProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">    <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">  <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">  <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment">#  超时时间</span></span><br><span class="line">  <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">  <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">  <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># livenessProbe，存活探针</span></span><br><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">    <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">  <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">  <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment">#  超时时间</span></span><br><span class="line">  <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">  <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">  <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># startupProbe，启动探针</span></span><br><span class="line"><span class="attr">startupProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">    <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">  <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">  <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment">#  超时时间</span></span><br><span class="line">  <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">  <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">  <span class="attr">successThreshold:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>一般使用控制器去创建管理pod，对k8s 控制器不清晰的小伙伴，可以参考我之前的文章：Kubernetes（k8s）Deployment、StatefulSet、DaemonSet、Job、CronJob五种控制器详解</p><p>下面是一个完整版的示例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deployment-probe</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">     <span class="attr">app:</span> <span class="string">deployment-probe</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">deployment-probe</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.17.1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># readinessProbe，就绪探针</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment">#  超时时间</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">          <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># livenessProbe，存活探针</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment">#  超时时间</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">          <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># startupProbe，启动探针</span></span><br><span class="line">        <span class="attr">startupProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="comment"># 延迟多久后开始探测</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment"># 执行探测频率(秒) 【 每隔秒执行一次 】</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="comment">#  超时时间</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">          <span class="comment"># 处于成功状态时，探测连续失败几次可被认为失败。</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="comment"># 处于失败状态时，探测连续成功几次，被认为成功。</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>执行查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crictl pull nginx:1.17.1</span><br><span class="line">kubectl apply -f deployment-probe.yaml</span><br><span class="line">kubectl get pod,deploy</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）6、kube-proxy、Service详解</title>
    <link href="https://blog.yongwang.lu/post/9f7e260b.html"/>
    <id>https://blog.yongwang.lu/post/9f7e260b.html</id>
    <published>2023-01-05T07:40:00.000Z</published>
    <updated>2023-01-05T07:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、kube-proxy简介"><a href="#一、kube-proxy简介" class="headerlink" title="一、kube-proxy简介"></a>一、kube-proxy简介</h2><blockquote><p><strong>kube-proxy负责为Service提供cluster内部的服务发现和负载均衡</strong>，它运行在每个Node计算节点上，负责Pod网络代理, 它会定时从etcd服务获取到service信息来做相应的策略，维护网络规则和四层负载均衡工作。在K8s集群中微服务的负载均衡是由Kube-proxy实现的，它是K8s集群内部的负载均衡器，也是一个分布式代理服务器，在K8s的每个节点上都有一个，这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。</p></blockquote><blockquote><p><strong>service是一组pod的服务抽象，相当于一组pod的LB</strong>，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP。<strong>kube-proxy的作用主要是负责service的实现</strong>，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。</p></blockquote><p>简单来说:</p><ul><li><p>kube-proxy其实就是管理service的访问入口，包括集群内Pod到Service的访问和集群外访问service。</p></li><li><p>kube-proxy管理sevice的Endpoints，该service对外暴露一个Virtual IP，也成为Cluster IP, 集群内通过访问这个Cluster IP:Port就能访问到集群内对应的serivce下的Pod。</p></li><li><p>service是通过Selector选择的一组Pods的服务抽象，其实就是一个微服务，提供了服务的LB和反向代理的能力，而kube-proxy的主要作用就是负责service的实现。</p></li><li><p>service另外一个重要作用是，一个服务后端的Pods可能会随着生存灭亡而发生IP的改变，service的出现，给服务提供了一个固定的IP，而无视后端Endpoint的变化。</p></li></ul><h2 id="二、Service-简介"><a href="#二、Service-简介" class="headerlink" title="二、Service 简介"></a>二、Service 简介</h2><blockquote><p>Kubernetes Service定义了这样一种抽象： Service是一种可以访问 Pod逻辑分组的策略， Service通常是通过 Label Selector访问 Pod组。</p></blockquote><blockquote><p>Service能够提供负载均衡的能力，但是在使用上有以下限制：<strong>只提供 4 层负载均衡能力</strong>，而没有 7 层功能，但有时我们可能需要更多的匹配规则来转发请求，这点上 4 层负载均衡是不支持的。</p></blockquote><h2 id="三、Service-类型"><a href="#三、Service-类型" class="headerlink" title="三、Service 类型"></a>三、Service 类型</h2><p>Service在 K8s中有以下四种类型：</p><h3 id="1）ClusterIp（集群内部使用）"><a href="#1）ClusterIp（集群内部使用）" class="headerlink" title="1）ClusterIp（集群内部使用）"></a>1）ClusterIp（集群内部使用）</h3><blockquote><p><strong>默认类型</strong>，自动分配一个仅Cluster内部可以访问的虚拟IP（VIP）。</p></blockquote><h3 id="2）NodePort（对外暴露应用）"><a href="#2）NodePort（对外暴露应用）" class="headerlink" title="2）NodePort（对外暴露应用）"></a>2）NodePort（对外暴露应用）</h3><blockquote><p>在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过NodeIP:NodePort访问来访问该服务。<br>端口范围：30000~32767</p></blockquote><h3 id="3）LoadBalancer（对外暴露应用，适用于公有云）"><a href="#3）LoadBalancer（对外暴露应用，适用于公有云）" class="headerlink" title="3）LoadBalancer（对外暴露应用，适用于公有云）"></a>3）LoadBalancer（对外暴露应用，适用于公有云）</h3><blockquote><p>在NodePort的基础上，借助Cloud Provider创建一个外部负载均衡器，并将请求转发到NodePort。</p></blockquote><h3 id="4）ExternalName"><a href="#4）ExternalName" class="headerlink" title="4）ExternalName"></a>4）ExternalName</h3><blockquote><p>创建一个dns别名指到service name上，主要是防止service name发生变化，要配合dns插件使用。通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容。这只有 Kubernetes 1.7或更高版本的kube-dns才支持（<strong>我这里是Kubernetes 1.22.1版本</strong>）。</p></blockquote><h2 id="四、Service-工作流程"><a href="#四、Service-工作流程" class="headerlink" title="四、Service 工作流程"></a>四、Service 工作流程</h2><p><img src="https://res.yongwang.lu/img/20230322225854.png" alt="Service 工作流程"></p><ol><li><p>客户端访问节点时通过 iptables实现的</p></li><li><p>iptables规则是通过 kube-proxy写入的</p></li><li><p>apiserver通过监控 kube-proxy去进行对服务和端点的监控</p></li><li><p>kube-proxy通过 pod的标签（ lables）去判断这个断点信息是否写入到 Endpoints里</p></li></ol><h2 id="五、Endpoints简介"><a href="#五、Endpoints简介" class="headerlink" title="五、Endpoints简介"></a>五、Endpoints简介</h2><blockquote><p>endpoint是k8s集群中的一个资源对象，存储在etcd中，<strong>用来记录一个service对应的所有pod的访问地址</strong>。service配置selector，endpoint controller才会自动创建对应的endpoint对象；否则，不会生成endpoint对象。</p></blockquote><blockquote><p>【例如】k8s集群中创建一个名为hello的service，就会生成一个同名的endpoint对象，ENDPOINTS就是service关联的pod的ip地址和端口。</p></blockquote><h3 id="1）工作流程"><a href="#1）工作流程" class="headerlink" title="1）工作流程"></a>1）工作流程</h3><blockquote><p><strong>一个 Service 由一组 backend Pod 组成。这些 Pod 通过 endpoints 暴露出来</strong>。 Service Selector 将持续评估，结果被 POST 到一个名称为 Service-hello 的 Endpoint 对象上。 当 Pod 终止后，它会自动从 Endpoint 中移除，新的能够匹配上 Service Selector 的 Pod 将自动地被添加到 Endpoint 中。 检查该 Endpoint，注意到 IP 地址与创建的 Pod 是相同的。现在，能够从集群中任意节点上使用 curl 命令请求 hello Service <CLUSTER-IP>:<PORT> 。</p></blockquote><h3 id="2）示例"><a href="#2）示例" class="headerlink" title="2）示例"></a>2）示例</h3><p>1、deployment-hello.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ cat &lt;&lt; EOF &gt; deployment-hello.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">     run: hello</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: hello</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.17.1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>2、service-hello.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ cat &lt;&lt; EOF &gt; service-hello.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: service-hello</span><br><span class="line">  labels:</span><br><span class="line">  name: service-hello</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort      #这里代表是NodePort类型的,另外还有ingress,LoadBalancer</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80          </span><br><span class="line">    targetPort: 8080</span><br><span class="line">    protocol: TCP</span><br><span class="line">    nodePort: 31111   # 所有的节点都会开放此端口30000--32767，此端口供外部调用。</span><br><span class="line">  selector:</span><br><span class="line">    run: hello</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>3、查看验证</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f deployment-hello.yaml</span><br><span class="line">$ kubectl apply -f service-hello.yaml</span><br><span class="line"># 查看pod，如果本地没有镜像，可能等待的时候比较长，一定要等到所有pod都在运行中才行。</span><br><span class="line">$ kubectl get pod -o wide|grep hello-*</span><br><span class="line"># 查看service</span><br><span class="line">$ kubectl get service service-hello -o wide</span><br><span class="line"># 查看service详情</span><br><span class="line">$ kubectl describe service service-hello</span><br><span class="line"># 查看pointer</span><br><span class="line">$ kubectl get endpoints service-hello</span><br></pre></td></tr></table></figure><h2 id="六、Service-Endpoints与Pod的关系"><a href="#六、Service-Endpoints与Pod的关系" class="headerlink" title="六、Service, Endpoints与Pod的关系"></a>六、Service, Endpoints与Pod的关系</h2><p><img src="https://res.yongwang.lu/img/20230322230158.png" alt="Service, Endpoints与Pod的关系"></p><blockquote><p>Kube-proxy进程获取每个Service的Endpoints,实现Service的负载均衡功能。</p></blockquote><p>Service的负载均衡转发规则<br><img src="https://res.yongwang.lu/img/20230322230057.png" alt="Service的负载均衡转发规则 "></p><blockquote><p>访问Service的请求，不论是Cluster IP+TargetPort的方式；还是用Node节点IP+NodePort的方式，都被Node节点的Iptables规则重定向到Kube-proxy监听Service服务代理端口。kube-proxy接收到Service的访问请求后，根据负载策略，转发到后端的Pod。</p></blockquote><h2 id="七、Service的资源清单文件详解"><a href="#七、Service的资源清单文件详解" class="headerlink" title="七、Service的资源清单文件详解"></a>七、Service的资源清单文件详解</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">#元数据</span><br><span class="line">  name: string</span><br><span class="line">  #Service名称</span><br><span class="line">  namespace: string</span><br><span class="line">  #命名空间，不指定时默认为default命名空间</span><br><span class="line">  labels:</span><br><span class="line">  #自定义标签属性列表     </span><br><span class="line">    - name: string</span><br><span class="line">  annotations:</span><br><span class="line">  #自定义注解属性列表    </span><br><span class="line">    - name: string</span><br><span class="line">spec:</span><br><span class="line">#详细描述    </span><br><span class="line">  selector: []</span><br><span class="line">  #这里选择器一定要选择容器的标签，也就是pod的标签</span><br><span class="line">  #selector:</span><br><span class="line">  #  app: web</span><br><span class="line">  #Label Selector配置，选择具有指定label标签的pod作为管理范围</span><br><span class="line">  type: string</span><br><span class="line">  #service的类型，指定service的访问方式，默认ClusterIP</span><br><span class="line">  #ClusterIP：虚拟的服务ip地址，用于k8s集群内部的pod访问，在Node上kube-porxy通过设置的iptables规则进行转发</span><br><span class="line">  #NodePort：使用宿主机端口，能够访问各Node的外部客户端通过Node的IP和端口就能访问服务器</span><br><span class="line">  #LoadBalancer：使用外部负载均衡器完成到服务器的负载分发，</span><br><span class="line">  #需要在spec.status.loadBalancer字段指定外部负载均衡服务器的IP，并同时定义nodePort和clusterIP用于公有云环境。</span><br><span class="line">  clusterIP: string</span><br><span class="line">  #虚拟服务IP地址，当type=ClusterIP时，如不指定，则系统会自动进行分配，也可以手动指定。当type=loadBalancer，需要指定</span><br><span class="line">  sessionAffinity: string</span><br><span class="line">  #是否支持session，可选值为ClietIP，默认值为空</span><br><span class="line">  #ClientIP表示将同一个客户端(根据客户端IP地址决定)的访问请求都转发到同一个后端Pod</span><br><span class="line">  ports:</span><br><span class="line">  #service需要暴露的端口列表    </span><br><span class="line">  - name: string</span><br><span class="line">    #端口名称</span><br><span class="line">    protocol: string</span><br><span class="line">    #端口协议，支持TCP或UDP，默认TCP</span><br><span class="line">     port: int</span><br><span class="line">    #服务监听的端口号</span><br><span class="line">     targetPort: int</span><br><span class="line">    #需要转发到后端的端口号</span><br><span class="line">     nodePort: int</span><br><span class="line">    #当type=NodePort时，指定映射到物理机的端口号</span><br><span class="line">  status:</span><br><span class="line">  #当type=LoadBalancer时，设置外部负载均衡的地址，用于公有云环境    </span><br><span class="line">    loadBalancer:</span><br><span class="line">    #外部负载均衡器    </span><br><span class="line">      ingress:</span><br><span class="line">      #外部负载均衡器 </span><br><span class="line">      ip: string</span><br><span class="line">      #外部负载均衡器的IP地址</span><br><span class="line">      hostname: string</span><br><span class="line">     #外部负载均衡器的机主机</span><br></pre></td></tr></table></figure><h2 id="八、kubernetes中的四种port"><a href="#八、kubernetes中的四种port" class="headerlink" title="八、kubernetes中的四种port"></a>八、kubernetes中的四种port</h2><h3 id="1）nodePort"><a href="#1）nodePort" class="headerlink" title="1）nodePort"></a>1）nodePort</h3><blockquote><p><strong>nodePort是外部访问k8s集群中service的端口</strong>，通过nodeIP: nodePort可以从外部访问到某个service。</p></blockquote><h3 id="2）port"><a href="#2）port" class="headerlink" title="2）port"></a>2）port</h3><blockquote><p><strong>port是k8s集群内部访问service的端口</strong>，即通过clusterIP: port可以访问到某个service。</p></blockquote><h3 id="3）targetPort"><a href="#3）targetPort" class="headerlink" title="3）targetPort"></a>3）targetPort</h3><blockquote><p><strong>targetPort是pod的端口</strong>，从port和nodePort来的流量经过kube-proxy流入到后端pod的targetPort上，最后进入容器。</p></blockquote><h3 id="4）containerPort"><a href="#4）containerPort" class="headerlink" title="4）containerPort"></a>4）containerPort</h3><blockquote><p><strong>containerPort是pod内部容器的端口</strong>，targetPort映射到containerPort。</p></blockquote><p><img src="https://res.yongwang.lu/img/202303221413828.png" alt="图片"></p><h2 id="九、kubernetes服务发现"><a href="#九、kubernetes服务发现" class="headerlink" title="九、kubernetes服务发现"></a>九、kubernetes服务发现</h2><blockquote><p>Kubernetes提供了两种方式进行服务发现, 即<strong>环境变量和DNS</strong>, 简单说明如下:</p></blockquote><h3 id="1）环境变量"><a href="#1）环境变量" class="headerlink" title="1）环境变量"></a>1）环境变量</h3><blockquote><p>当你创建一个Pod的时候，kubelet会在该Pod中注入集群内所有Service的相关环境变量。</p></blockquote><blockquote><p>【注意】要想一个Pod中注入某个Service的环境变量，则<strong>必须Service要比该Pod先创建</strong>。这一点，几乎使得这种方式进行服务发现不可用。比如，一个ServiceName为redis-master的Service，对应的ClusterIP:Port为172.16.50.11:6379，则其对应的环境变量为:</p></blockquote><blockquote><p>REDIS_MASTER_SERVICE_HOST&#x3D;172.16.50.11<br>REDIS_MASTER_SERVICE_PORT&#x3D;6379<br>REDIS_MASTER_PORT&#x3D;tcp:&#x2F;&#x2F;172.16.50.11:6379<br>REDIS_MASTER_PORT_6379_TCP&#x3D;tcp:&#x2F;&#x2F;172.16.50.11:6379<br>REDIS_MASTER_PORT_6379_TCP_PROTO&#x3D;tcp<br>REDIS_MASTER_PORT_6379_TCP_PORT&#x3D;6379<br>REDIS_MASTER_PORT_6379_TCP_ADDR&#x3D;172.16.50.11</p></blockquote><h3 id="2-DNS"><a href="#2-DNS" class="headerlink" title="2) DNS"></a>2) DNS</h3><blockquote><p>这是k8s官方强烈推荐的方式!!! 可以通过cluster add-on方式轻松的创建KubeDNS来对集群内的Service进行服务发现。</p></blockquote><h2 id="十、Service代理模式"><a href="#十、Service代理模式" class="headerlink" title="十、Service代理模式"></a>十、Service代理模式</h2><blockquote><p>k8s群集中的每个节点都运行一个kube-proxy的组件，kube-proxy其实是一个代理层负责实现service。</p></blockquote><blockquote><p><strong>Kubernetes v1.2之前默认是userspace，v1.2之后默认是iptables模式</strong>，iptables模式性能和可靠性更好，但是iptables模式依赖健康检查，在没有健康检查的情况下如果一个pod不响应，iptables模式不会切换另一个pod上。</p></blockquote><h3 id="1）userspace模式"><a href="#1）userspace模式" class="headerlink" title="1）userspace模式"></a>1）userspace模式</h3><blockquote><p>客户端访问ServiceIP(clusterIP)请求会先从用户空间到内核中的iptables，然后回到用户空间kube-proxy，kube-proxy负责代理工作。</p></blockquote><p>缺点：</p><blockquote><p>可见，userspace这种mode最大的问题是，service的请求会先从用户空间进入内核iptables，然后再回到用户空间，由kube-proxy完成后端Endpoints的选择和代理工作，这样流量从用户空间进出内核带来的性能损耗是不可接受的。这也是k8s v1.0及之前版本中对kube-proxy质疑最大的一点，因此社区就开始研究iptables mode。</p></blockquote><p>详细工作流程：</p><blockquote><p>userspace这种模式下，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；对每个 Service，它都为其在本地节点开放一个端口，作为其服务代理端口；发往该端口的请求会采用一定的策略转发给与该服务对应的后端 Pod 实体。kube-proxy 同时会在本地节点设置 iptables 规则，配置一个 Virtual IP，把发往 Virtual IP 的请求重定向到与该 Virtual IP 对应的服务代理端口上。其工作流程大体如下:</p></blockquote><blockquote><p>【分析】该模式请求在到达 iptables 进行处理时就会进入内核，而 kube-proxy 监听则是在用户态, 请求就形成了从用户态到内核态再返回到用户态的传递过程, 一定程度降低了服务性能。</p></blockquote><h3 id="2）iptables模式（默认模式）"><a href="#2）iptables模式（默认模式）" class="headerlink" title="2）iptables模式（默认模式）"></a>2）iptables模式（默认模式）</h3><blockquote><p>该模式完全利用内核iptables来实现service的代理和LB, 这是K8s在<strong>v1.2及之后版本默认模式</strong>. 工作原理如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322225854.png" alt="iptables模式"></p><blockquote><p><strong>iptables mode因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗</strong>。另外，如果集群中存在上万的Service&#x2F;Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。这也导致目前大部分企业用k8s上生产时，都不会直接用kube-proxy作为服务代理，而是通过自己开发或者通过Ingress Controller来集成HAProxy, Nginx来代替kube-proxy。</p></blockquote><p>详细工作流程：</p><blockquote><p>iptables 模式与 userspace 相同，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；但它并不在本地节点开启反向代理服务，而是把反向代理全部交给 iptables 来实现；即 iptables 直接将对 VIP 的请求转发给后端 Pod，通过 iptables 设置转发策略。其工作流程大体如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322230525.png" alt="iptables-详细工作流程"></p><blockquote><p>【分析】 该模式相比 userspace 模式，克服了请求在用户态-内核态反复传递的问题，性能上有所提升，但使用 iptables NAT 来完成转发，存在不可忽视的性能损耗，而且在大规模场景下，iptables 规则的条目会十分巨大，性能上还要再打折扣。</p></blockquote><p>示例:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF &gt; mysql-service.yaml</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Service</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    name: mysql</span></span><br><span class="line"><span class="string">    role: service</span></span><br><span class="line"><span class="string">  name: mysql-service</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  ports:</span></span><br><span class="line"><span class="string">    - port: 3306</span></span><br><span class="line"><span class="string">      targetPort: 3306</span></span><br><span class="line"><span class="string">      nodePort: 30964</span></span><br><span class="line"><span class="string">  type: NodePort</span></span><br><span class="line"><span class="string">  selector:</span></span><br><span class="line"><span class="string">    mysql-service: &quot;true&quot;</span></span><br><span class="line"><span class="string">    name: mysql</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">$ kubectl apply -f mysql-service.yaml</span><br><span class="line">$ kubectl get svc</span><br></pre></td></tr></table></figure><h3 id="3）ipvs模型"><a href="#3）ipvs模型" class="headerlink" title="3）ipvs模型"></a>3）ipvs模型</h3><blockquote><p>在kubernetes 1.8以上的版本中，对于kube-proxy组件增加了除iptables模式和用户模式之外还支持ipvs模式。<strong>kube-proxy ipvs 是基于 NAT 实现的</strong>，通过ipvs的NAT模式，对访问k8s service的请求进行虚IP到POD IP的转发。当创建一个 service 后，kubernetes 会在每个节点上创建一个网卡，同时帮你将 Service IP(VIP) 绑定上，此时相当于每个 Node 都是一个 ds，而其他任何 Node 上的 Pod，甚至是宿主机服务(比如 kube-apiserver 的 6443)都可能成为 rs；</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322230705.png" alt="ipvs-service模型"></p><p>详细工作流程：</p><blockquote><p>与iptables、userspace 模式一样，kube-proxy 依然监听Service以及Endpoints对象的变化, 不过它并不创建反向代理, 也不创建大量的 iptables 规则, 而是通过netlink 创建ipvs规则，并使用k8s Service与Endpoints信息，对所在节点的ipvs规则进行定期同步; netlink 与 iptables 底层都是基于 netfilter 钩子，但是 netlink 由于采用了 hash table 而且直接工作在内核态，在性能上比 iptables 更优。其工作流程大体如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322230748.png" alt="ipvs工作流程"></p><blockquote><p>【分析】ipvs 是目前 kube-proxy 所支持的最新代理模式，相比使用 iptables，使用 ipvs 具有更高的性能。</p></blockquote><hr><h3 id="4）kube-proxy配置-ipvs模式（所有节点）"><a href="#4）kube-proxy配置-ipvs模式（所有节点）" class="headerlink" title="4）kube-proxy配置 ipvs模式（所有节点）"></a>4）kube-proxy配置 ipvs模式（所有节点）</h3><p>1、加载ip_vs相关内核模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ modprobe -- ip_vs</span><br><span class="line">$ modprobe -- ip_vs_sh</span><br><span class="line">$ modprobe -- ip_vs_rr</span><br><span class="line">$ modprobe -- ip_vs_wrr</span><br><span class="line">$ modprobe -- nf_conntrack_ipv4</span><br></pre></td></tr></table></figure><p>所有节点验证开启了ipvs：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lsmod |grep ip_vs</span><br></pre></td></tr></table></figure><p>2、安装ipvsadm工具</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install ipset ipvsadm -y</span><br></pre></td></tr></table></figure><p>3、编辑kube-proxy配置文件，mode修改成ipvs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl edit  configmap -n kube-system  kube-proxy</span><br></pre></td></tr></table></figure><p>4、重启kube-proxy<br>先查看之前的kube-proxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy</span><br></pre></td></tr></table></figure><p>删掉上面三个kube-proxy，重新拉起新的服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy |awk <span class="string">&#x27;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&#x27;</span></span><br></pre></td></tr></table></figure><p>再查看</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">title: Kubernetes（k8s）kube-proxy、Service详解</span><br><span class="line">abbrlink: 5475e8a5</span><br><span class="line">tags: kubernetes</span><br><span class="line">categories: kubernetes</span><br><span class="line">keywords: k8s</span><br><span class="line">description: kubernetes1.26</span><br><span class="line">top_img: &#x27;https://kubernetes.io/images/nav_logo2.svg&#x27;</span><br><span class="line">cover: &#x27;https://res.yongwang.lu/img/k8s_card.png&#x27;</span><br><span class="line">date: 2023-03-21 21:40:00</span><br><span class="line">updated: 2023-03-21 21:40:00</span><br><span class="line">comments:</span><br><span class="line">toc:</span><br><span class="line">toc_number:</span><br><span class="line">toc_style_simple:</span><br><span class="line">copyright:</span><br><span class="line">copyright_author:</span><br><span class="line">copyright_author_href:</span><br><span class="line">copyright_url:</span><br><span class="line">copyright_info:</span><br><span class="line">mathjax:</span><br><span class="line">katex:</span><br><span class="line">aplayer:</span><br><span class="line">highlight_shrink:</span><br><span class="line">aside:</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## 一、kube-proxy简介</span><br><span class="line"></span><br><span class="line">&gt; **kube-proxy负责为Service提供cluster内部的服务发现和负载均衡**，它运行在每个Node计算节点上，负责Pod网络代理, 它会定时从etcd服务获取到service信息来做相应的策略，维护网络规则和四层负载均衡工作。在K8s集群中微服务的负载均衡是由Kube-proxy实现的，它是K8s集群内部的负载均衡器，也是一个分布式代理服务器，在K8s的每个节点上都有一个，这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。</span><br><span class="line"></span><br><span class="line">&gt; **service是一组pod的服务抽象，相当于一组pod的LB**，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP。**kube-proxy的作用主要是负责service的实现**，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。</span><br><span class="line"></span><br><span class="line">简单来说:</span><br><span class="line"></span><br><span class="line">-   kube-proxy其实就是管理service的访问入口，包括集群内Pod到Service的访问和集群外访问service。</span><br><span class="line">    </span><br><span class="line">-   kube-proxy管理sevice的Endpoints，该service对外暴露一个Virtual IP，也成为Cluster IP, 集群内通过访问这个Cluster IP:Port就能访问到集群内对应的serivce下的Pod。</span><br><span class="line">    </span><br><span class="line">-   service是通过Selector选择的一组Pods的服务抽象，其实就是一个微服务，提供了服务的LB和反向代理的能力，而kube-proxy的主要作用就是负责service的实现。</span><br><span class="line">    </span><br><span class="line">-   service另外一个重要作用是，一个服务后端的Pods可能会随着生存灭亡而发生IP的改变，service的出现，给服务提供了一个固定的IP，而无视后端Endpoint的变化。</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">## 二、Service 简介</span><br><span class="line"></span><br><span class="line">&gt; Kubernetes Service定义了这样一种抽象： Service是一种可以访问 Pod逻辑分组的策略， Service通常是通过 Label Selector访问 Pod组。</span><br><span class="line"></span><br><span class="line">&gt; Service能够提供负载均衡的能力，但是在使用上有以下限制：**只提供 4 层负载均衡能力**，而没有 7 层功能，但有时我们可能需要更多的匹配规则来转发请求，这点上 4 层负载均衡是不支持的。</span><br><span class="line"></span><br><span class="line">## 三、Service 类型</span><br><span class="line"></span><br><span class="line">Service在 K8s中有以下四种类型：</span><br><span class="line"></span><br><span class="line">### 1）ClusterIp（集群内部使用）</span><br><span class="line"></span><br><span class="line">&gt; **默认类型**，自动分配一个仅Cluster内部可以访问的虚拟IP（VIP）。</span><br><span class="line"></span><br><span class="line">### 2）NodePort（对外暴露应用）</span><br><span class="line"></span><br><span class="line">&gt; 在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过NodeIP:NodePort访问来访问该服务。  </span><br><span class="line">&gt; 端口范围：30000~32767</span><br><span class="line"></span><br><span class="line">### 3）LoadBalancer（对外暴露应用，适用于公有云）</span><br><span class="line"></span><br><span class="line">&gt; 在NodePort的基础上，借助Cloud Provider创建一个外部负载均衡器，并将请求转发到NodePort。</span><br><span class="line"></span><br><span class="line">### 4）ExternalName</span><br><span class="line"></span><br><span class="line">&gt; 创建一个dns别名指到service name上，主要是防止service name发生变化，要配合dns插件使用。通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容。这只有 Kubernetes 1.7或更高版本的kube-dns才支持（**我这里是Kubernetes 1.22.1版本**）。</span><br><span class="line"></span><br><span class="line">## 四、Service 工作流程</span><br><span class="line"></span><br><span class="line">![Service 工作流程](https://res.yongwang.lu/img/20230322225854.png)</span><br><span class="line"></span><br><span class="line">1.  客户端访问节点时通过 iptables实现的</span><br><span class="line">    </span><br><span class="line">2.  iptables规则是通过 kube-proxy写入的</span><br><span class="line">    </span><br><span class="line">3.  apiserver通过监控 kube-proxy去进行对服务和端点的监控</span><br><span class="line">    </span><br><span class="line">4.  kube-proxy通过 pod的标签（ lables）去判断这个断点信息是否写入到 Endpoints里</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">## 五、Endpoints简介</span><br><span class="line"></span><br><span class="line">&gt; endpoint是k8s集群中的一个资源对象，存储在etcd中，**用来记录一个service对应的所有pod的访问地址**。service配置selector，endpoint controller才会自动创建对应的endpoint对象；否则，不会生成endpoint对象。</span><br><span class="line"></span><br><span class="line">&gt; 【例如】k8s集群中创建一个名为hello的service，就会生成一个同名的endpoint对象，ENDPOINTS就是service关联的pod的ip地址和端口。</span><br><span class="line"></span><br><span class="line">### 1）工作流程</span><br><span class="line"></span><br><span class="line">&gt; **一个 Service 由一组 backend Pod 组成。这些 Pod 通过 endpoints 暴露出来**。 Service Selector 将持续评估，结果被 POST 到一个名称为 Service-hello 的 Endpoint 对象上。 当 Pod 终止后，它会自动从 Endpoint 中移除，新的能够匹配上 Service Selector 的 Pod 将自动地被添加到 Endpoint 中。 检查该 Endpoint，注意到 IP 地址与创建的 Pod 是相同的。现在，能够从集群中任意节点上使用 curl 命令请求 hello Service &lt;CLUSTER-IP&gt;:&lt;PORT&gt; 。</span><br><span class="line"></span><br><span class="line">### 2）示例</span><br><span class="line"></span><br><span class="line">1、deployment-hello.yaml</span><br><span class="line"></span><br><span class="line">```auto</span><br><span class="line">$ cat &lt;&lt; EOF &gt; deployment-hello.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">     run: hello</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: hello</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.17.1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>2、service-hello.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ cat &lt;&lt; EOF &gt; service-hello.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: service-hello</span><br><span class="line">  labels:</span><br><span class="line">  name: service-hello</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort      #这里代表是NodePort类型的,另外还有ingress,LoadBalancer</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80          </span><br><span class="line">    targetPort: 8080</span><br><span class="line">    protocol: TCP</span><br><span class="line">    nodePort: 31111   # 所有的节点都会开放此端口30000--32767，此端口供外部调用。</span><br><span class="line">  selector:</span><br><span class="line">    run: hello</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>3、查看验证</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f deployment-hello.yaml</span><br><span class="line">$ kubectl apply -f service-hello.yaml</span><br><span class="line"># 查看pod，如果本地没有镜像，可能等待的时候比较长，一定要等到所有pod都在运行中才行。</span><br><span class="line">$ kubectl get pod -o wide|grep hello-*</span><br><span class="line"># 查看service</span><br><span class="line">$ kubectl get service service-hello -o wide</span><br><span class="line"># 查看service详情</span><br><span class="line">$ kubectl describe service service-hello</span><br><span class="line"># 查看pointer</span><br><span class="line">$ kubectl get endpoints service-hello</span><br></pre></td></tr></table></figure><h2 id="六、Service-Endpoints与Pod的关系-1"><a href="#六、Service-Endpoints与Pod的关系-1" class="headerlink" title="六、Service, Endpoints与Pod的关系"></a>六、Service, Endpoints与Pod的关系</h2><p><img src="https://res.yongwang.lu/img/20230322230158.png" alt="Service, Endpoints与Pod的关系"></p><blockquote><p>Kube-proxy进程获取每个Service的Endpoints,实现Service的负载均衡功能。</p></blockquote><p>Service的负载均衡转发规则<br><img src="https://res.yongwang.lu/img/20230322230057.png" alt="Service的负载均衡转发规则 "></p><blockquote><p>访问Service的请求，不论是Cluster IP+TargetPort的方式；还是用Node节点IP+NodePort的方式，都被Node节点的Iptables规则重定向到Kube-proxy监听Service服务代理端口。kube-proxy接收到Service的访问请求后，根据负载策略，转发到后端的Pod。</p></blockquote><h2 id="七、Service的资源清单文件详解-1"><a href="#七、Service的资源清单文件详解-1" class="headerlink" title="七、Service的资源清单文件详解"></a>七、Service的资源清单文件详解</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">#元数据</span><br><span class="line">  name: string</span><br><span class="line">  #Service名称</span><br><span class="line">  namespace: string</span><br><span class="line">  #命名空间，不指定时默认为default命名空间</span><br><span class="line">  labels:</span><br><span class="line">  #自定义标签属性列表     </span><br><span class="line">    - name: string</span><br><span class="line">  annotations:</span><br><span class="line">  #自定义注解属性列表    </span><br><span class="line">    - name: string</span><br><span class="line">spec:</span><br><span class="line">#详细描述    </span><br><span class="line">  selector: []</span><br><span class="line">  #这里选择器一定要选择容器的标签，也就是pod的标签</span><br><span class="line">  #selector:</span><br><span class="line">  #  app: web</span><br><span class="line">  #Label Selector配置，选择具有指定label标签的pod作为管理范围</span><br><span class="line">  type: string</span><br><span class="line">  #service的类型，指定service的访问方式，默认ClusterIP</span><br><span class="line">  #ClusterIP：虚拟的服务ip地址，用于k8s集群内部的pod访问，在Node上kube-porxy通过设置的iptables规则进行转发</span><br><span class="line">  #NodePort：使用宿主机端口，能够访问各Node的外部客户端通过Node的IP和端口就能访问服务器</span><br><span class="line">  #LoadBalancer：使用外部负载均衡器完成到服务器的负载分发，</span><br><span class="line">  #需要在spec.status.loadBalancer字段指定外部负载均衡服务器的IP，并同时定义nodePort和clusterIP用于公有云环境。</span><br><span class="line">  clusterIP: string</span><br><span class="line">  #虚拟服务IP地址，当type=ClusterIP时，如不指定，则系统会自动进行分配，也可以手动指定。当type=loadBalancer，需要指定</span><br><span class="line">  sessionAffinity: string</span><br><span class="line">  #是否支持session，可选值为ClietIP，默认值为空</span><br><span class="line">  #ClientIP表示将同一个客户端(根据客户端IP地址决定)的访问请求都转发到同一个后端Pod</span><br><span class="line">  ports:</span><br><span class="line">  #service需要暴露的端口列表    </span><br><span class="line">  - name: string</span><br><span class="line">    #端口名称</span><br><span class="line">    protocol: string</span><br><span class="line">    #端口协议，支持TCP或UDP，默认TCP</span><br><span class="line">     port: int</span><br><span class="line">    #服务监听的端口号</span><br><span class="line">     targetPort: int</span><br><span class="line">    #需要转发到后端的端口号</span><br><span class="line">     nodePort: int</span><br><span class="line">    #当type=NodePort时，指定映射到物理机的端口号</span><br><span class="line">  status:</span><br><span class="line">  #当type=LoadBalancer时，设置外部负载均衡的地址，用于公有云环境    </span><br><span class="line">    loadBalancer:</span><br><span class="line">    #外部负载均衡器    </span><br><span class="line">      ingress:</span><br><span class="line">      #外部负载均衡器 </span><br><span class="line">      ip: string</span><br><span class="line">      #外部负载均衡器的IP地址</span><br><span class="line">      hostname: string</span><br><span class="line">     #外部负载均衡器的机主机</span><br></pre></td></tr></table></figure><h2 id="八、kubernetes中的四种port-1"><a href="#八、kubernetes中的四种port-1" class="headerlink" title="八、kubernetes中的四种port"></a>八、kubernetes中的四种port</h2><h3 id="1）nodePort-1"><a href="#1）nodePort-1" class="headerlink" title="1）nodePort"></a>1）nodePort</h3><blockquote><p><strong>nodePort是外部访问k8s集群中service的端口</strong>，通过nodeIP: nodePort可以从外部访问到某个service。</p></blockquote><h3 id="2）port-1"><a href="#2）port-1" class="headerlink" title="2）port"></a>2）port</h3><blockquote><p><strong>port是k8s集群内部访问service的端口</strong>，即通过clusterIP: port可以访问到某个service。</p></blockquote><h3 id="3）targetPort-1"><a href="#3）targetPort-1" class="headerlink" title="3）targetPort"></a>3）targetPort</h3><blockquote><p><strong>targetPort是pod的端口</strong>，从port和nodePort来的流量经过kube-proxy流入到后端pod的targetPort上，最后进入容器。</p></blockquote><h3 id="4）containerPort-1"><a href="#4）containerPort-1" class="headerlink" title="4）containerPort"></a>4）containerPort</h3><blockquote><p><strong>containerPort是pod内部容器的端口</strong>，targetPort映射到containerPort。</p></blockquote><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZyHzM6VdDdXcBuy39GptibDPKJMXiacaicepLhblRia9j7k0YRGicmTZW3RSuF4ofq38gHQY7dYOdu10G45lE1nzE7Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><h2 id="九、kubernetes服务发现-1"><a href="#九、kubernetes服务发现-1" class="headerlink" title="九、kubernetes服务发现"></a>九、kubernetes服务发现</h2><blockquote><p>Kubernetes提供了两种方式进行服务发现, 即<strong>环境变量和DNS</strong>, 简单说明如下:</p></blockquote><h3 id="1）环境变量-1"><a href="#1）环境变量-1" class="headerlink" title="1）环境变量"></a>1）环境变量</h3><blockquote><p>当你创建一个Pod的时候，kubelet会在该Pod中注入集群内所有Service的相关环境变量。</p></blockquote><blockquote><p>【注意】要想一个Pod中注入某个Service的环境变量，则<strong>必须Service要比该Pod先创建</strong>。这一点，几乎使得这种方式进行服务发现不可用。比如，一个ServiceName为redis-master的Service，对应的ClusterIP:Port为172.16.50.11:6379，则其对应的环境变量为:</p></blockquote><blockquote><p>REDIS_MASTER_SERVICE_HOST&#x3D;172.16.50.11<br>REDIS_MASTER_SERVICE_PORT&#x3D;6379<br>REDIS_MASTER_PORT&#x3D;tcp:&#x2F;&#x2F;172.16.50.11:6379<br>REDIS_MASTER_PORT_6379_TCP&#x3D;tcp:&#x2F;&#x2F;172.16.50.11:6379<br>REDIS_MASTER_PORT_6379_TCP_PROTO&#x3D;tcp<br>REDIS_MASTER_PORT_6379_TCP_PORT&#x3D;6379<br>REDIS_MASTER_PORT_6379_TCP_ADDR&#x3D;172.16.50.11</p></blockquote><h3 id="2-DNS-1"><a href="#2-DNS-1" class="headerlink" title="2) DNS"></a>2) DNS</h3><blockquote><p>这是k8s官方强烈推荐的方式!!! 可以通过cluster add-on方式轻松的创建KubeDNS来对集群内的Service进行服务发现。</p></blockquote><h2 id="十、Service代理模式-1"><a href="#十、Service代理模式-1" class="headerlink" title="十、Service代理模式"></a>十、Service代理模式</h2><blockquote><p>k8s群集中的每个节点都运行一个kube-proxy的组件，kube-proxy其实是一个代理层负责实现service。</p></blockquote><blockquote><p><strong>Kubernetes v1.2之前默认是userspace，v1.2之后默认是iptables模式</strong>，iptables模式性能和可靠性更好，但是iptables模式依赖健康检查，在没有健康检查的情况下如果一个pod不响应，iptables模式不会切换另一个pod上。</p></blockquote><h3 id="1）userspace模式-1"><a href="#1）userspace模式-1" class="headerlink" title="1）userspace模式"></a>1）userspace模式</h3><blockquote><p>客户端访问ServiceIP(clusterIP)请求会先从用户空间到内核中的iptables，然后回到用户空间kube-proxy，kube-proxy负责代理工作。</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322231436.png" alt="图片"></p><p>缺点：</p><blockquote><p>可见，userspace这种mode最大的问题是，service的请求会先从用户空间进入内核iptables，然后再回到用户空间，由kube-proxy完成后端Endpoints的选择和代理工作，这样流量从用户空间进出内核带来的性能损耗是不可接受的。这也是k8s v1.0及之前版本中对kube-proxy质疑最大的一点，因此社区就开始研究iptables mode。</p></blockquote><p>详细工作流程：</p><blockquote><p>userspace这种模式下，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；对每个 Service，它都为其在本地节点开放一个端口，作为其服务代理端口；发往该端口的请求会采用一定的策略转发给与该服务对应的后端 Pod 实体。kube-proxy 同时会在本地节点设置 iptables 规则，配置一个 Virtual IP，把发往 Virtual IP 的请求重定向到与该 Virtual IP 对应的服务代理端口上。其工作流程大体如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322231458.png" alt="图片"></p><blockquote><p>【分析】该模式请求在到达 iptables 进行处理时就会进入内核，而 kube-proxy 监听则是在用户态, 请求就形成了从用户态到内核态再返回到用户态的传递过程, 一定程度降低了服务性能。</p></blockquote><h3 id="2）iptables模式（默认模式）-1"><a href="#2）iptables模式（默认模式）-1" class="headerlink" title="2）iptables模式（默认模式）"></a>2）iptables模式（默认模式）</h3><blockquote><p>该模式完全利用内核iptables来实现service的代理和LB, 这是K8s在<strong>v1.2及之后版本默认模式</strong>. 工作原理如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322231525.png" alt="图片"></p><blockquote><p><strong>iptables mode因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗</strong>。另外，如果集群中存在上万的Service&#x2F;Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。这也导致目前大部分企业用k8s上生产时，都不会直接用kube-proxy作为服务代理，而是通过自己开发或者通过Ingress Controller来集成HAProxy, Nginx来代替kube-proxy。</p></blockquote><p>详细工作流程：</p><blockquote><p>iptables 模式与 userspace 相同，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；但它并不在本地节点开启反向代理服务，而是把反向代理全部交给 iptables 来实现；即 iptables 直接将对 VIP 的请求转发给后端 Pod，通过 iptables 设置转发策略。其工作流程大体如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322231458.png" alt="图片"></p><blockquote><p>【分析】 该模式相比 userspace 模式，克服了请求在用户态-内核态反复传递的问题，性能上有所提升，但使用 iptables NAT 来完成转发，存在不可忽视的性能损耗，而且在大规模场景下，iptables 规则的条目会十分巨大，性能上还要再打折扣。</p></blockquote><p>示例:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF &gt; mysql-service.yaml</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Service</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    name: mysql</span></span><br><span class="line"><span class="string">    role: service</span></span><br><span class="line"><span class="string">  name: mysql-service</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  ports:</span></span><br><span class="line"><span class="string">    - port: 3306</span></span><br><span class="line"><span class="string">      targetPort: 3306</span></span><br><span class="line"><span class="string">      nodePort: 30964</span></span><br><span class="line"><span class="string">  type: NodePort</span></span><br><span class="line"><span class="string">  selector:</span></span><br><span class="line"><span class="string">    mysql-service: &quot;true&quot;</span></span><br><span class="line"><span class="string">    name: mysql</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">$ kubectl apply -f mysql-service.yaml</span><br><span class="line">$ kubectl get svc</span><br></pre></td></tr></table></figure><h3 id="3）ipvs模型-1"><a href="#3）ipvs模型-1" class="headerlink" title="3）ipvs模型"></a>3）ipvs模型</h3><blockquote><p>在kubernetes 1.8以上的版本中，对于kube-proxy组件增加了除iptables模式和用户模式之外还支持ipvs模式。<strong>kube-proxy ipvs 是基于 NAT 实现的</strong>，通过ipvs的NAT模式，对访问k8s service的请求进行虚IP到POD IP的转发。当创建一个 service 后，kubernetes 会在每个节点上创建一个网卡，同时帮你将 Service IP(VIP) 绑定上，此时相当于每个 Node 都是一个 ds，而其他任何 Node 上的 Pod，甚至是宿主机服务(比如 kube-apiserver 的 6443)都可能成为 rs；</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322230705.png" alt="ipvs-service模型"></p><p>详细工作流程：</p><blockquote><p>与iptables、userspace 模式一样，kube-proxy 依然监听Service以及Endpoints对象的变化, 不过它并不创建反向代理, 也不创建大量的 iptables 规则, 而是通过netlink 创建ipvs规则，并使用k8s Service与Endpoints信息，对所在节点的ipvs规则进行定期同步; netlink 与 iptables 底层都是基于 netfilter 钩子，但是 netlink 由于采用了 hash table 而且直接工作在内核态，在性能上比 iptables 更优。其工作流程大体如下:</p></blockquote><p><img src="https://res.yongwang.lu/img/20230322230748.png" alt="ipvs工作流程"></p><blockquote><p>【分析】ipvs 是目前 kube-proxy 所支持的最新代理模式，相比使用 iptables，使用 ipvs 具有更高的性能。</p></blockquote><hr><h3 id="4）kube-proxy配置-ipvs模式（所有节点）-1"><a href="#4）kube-proxy配置-ipvs模式（所有节点）-1" class="headerlink" title="4）kube-proxy配置 ipvs模式（所有节点）"></a>4）kube-proxy配置 ipvs模式（所有节点）</h3><p>1、加载ip_vs相关内核模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ modprobe -- ip_vs</span><br><span class="line">$ modprobe -- ip_vs_sh</span><br><span class="line">$ modprobe -- ip_vs_rr</span><br><span class="line">$ modprobe -- ip_vs_wrr</span><br><span class="line">$ modprobe -- nf_conntrack_ipv4</span><br></pre></td></tr></table></figure><p>所有节点验证开启了ipvs：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lsmod |grep ip_vs</span><br></pre></td></tr></table></figure><p>2、安装ipvsadm工具</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install ipset ipvsadm -y</span><br></pre></td></tr></table></figure><p>3、编辑kube-proxy配置文件，mode修改成ipvs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl edit  configmap -n kube-system  kube-proxy</span><br></pre></td></tr></table></figure><p>4、重启kube-proxy<br>先查看之前的kube-proxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy</span><br></pre></td></tr></table></figure><p>删掉上面三个kube-proxy，重新拉起新的服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy |awk <span class="string">&#x27;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&#x27;</span></span><br></pre></td></tr></table></figure><p>再查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy</span><br></pre></td></tr></table></figure><p>5、查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ipvsadm -Ln</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system | grep kube-proxy</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）5、工作负载-Deployment</title>
    <link href="https://blog.yongwang.lu/post/c2d6c004.html"/>
    <id>https://blog.yongwang.lu/post/c2d6c004.html</id>
    <published>2023-01-05T06:40:00.000Z</published>
    <updated>2023-01-05T06:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在 kubernetes 的世界里，Pod 是运行应用的载体。 Pod 是由多个容器组成、是 kubernetes 的最小调度单元、Pod 共享底层资源、由 kubernetes 来管理生命周期。</p></blockquote><blockquote><p>一般情况下，我们并不直接创建 Pod，而是通过 Deployment 来创建 Pod，由 Deployment 来负责创建、更新、维护其所管理的所有 Pods。</p></blockquote><blockquote><p>这里注意并不直接管理pod, 而是通过管理replicaset来间接管理pod，即：deployment管理replicaset，replicaset管理pod。所以deployment比replicaset的功能更强大。</p></blockquote><h2 id="一、Deployment介绍与工作原理"><a href="#一、Deployment介绍与工作原理" class="headerlink" title="一、Deployment介绍与工作原理"></a>一、Deployment介绍与工作原理</h2><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h3><p>Deployment是一种更高阶的工作负载资源。其可以视作为对RS的再一次升级。其不仅可以部署应用还可以通过声明的方式升级应用。具体地，Deployment被创建后，其内部先创建相应的RS资源。再利用该RS资源创建相应的Pod</p><p><img src="https://res.yongwang.lu/img/202303222110977.png" alt="deployment介绍"></p><h3 id="2-工作流程"><a href="#2-工作流程" class="headerlink" title="2.工作流程"></a>2.工作流程</h3><p><img src="https://res.yongwang.lu/img/202303222114591.png" alt="工作流程"><br>图片来源书籍: kubernetes in action</p><p>客户端将创建 Deployment 的请求发送给 Apiserver<br>Apiserver 将 Deployment 信息写入 etcd,etcd 将写入结果响应给 Apiserver,Apiserver 将创建结果响应给客户端 (此时未经过 ControllerManager,deployment 的 READY 状态为 0)<br>ControllerManager 通过 Apiserver 的 watch 接口，获取到新增的 Deployment 资源，Deployment controller 向 Apiserver 发送创建 RS 的请求，Apiserver 将 RS 信息写入 etcd。。。<br>ControllerManager 通过 Apiserver 的 watch 接口，获取到新增的 ReplicaSet 资源，ReplicaSet controller 向 Apiserver 发送创建 Pod 的请求，Apiserver 将 Pod 信息写入 etcd。。。<br>Scheduler 通过 Apiserver 的 watch 接口，获取到未调度的 Pod 的通知，根据调度算法选择一个 node 节点，告诉 Apiserver 这个 Pod 应该运行在哪个节点<br>Apiserver 将这个 Pod 和 node 的绑定信息更新到 etcd,etcd 将写入结果响应给 Apiserver<br>Kubelet 通过 Apiserver 的 watch 接口，获取到当前节点有创建 Pod 的通知，Kubelet 调用 docker 创建容器，Kubelet 将 Pod 运行状态发送给 Apiserver<br>Apiserver 将 Pod 状态信息更新到 etcd</p><h2 id="二、Deployment的使用"><a href="#二、Deployment的使用" class="headerlink" title="二、Deployment的使用"></a>二、Deployment的使用</h2><h3 id="1-创建Deployment"><a href="#1-创建Deployment" class="headerlink" title="1.创建Deployment"></a>1.创建Deployment</h3><p>利用 Deployment 也可以创建 Pod，下面来比较一下两种方式创建 Pod 的区别。</p><blockquote><p>方式一：使用kubectl run 创建 Pod</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run my-nginx --image=nginx:1.20 -n hello-world</span><br></pre></td></tr></table></figure><blockquote><p>方式二：使用 kubectl create deployment 创建 Pod</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment my-nginx-dep --image=nginx:1.20 -n hello-world</span><br></pre></td></tr></table></figure><p>如果此时要删除上面创建的 Pod ，对于方式一创建的，很简单，直接执行下面的这行语句即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod my-nginx -n hello-world</span><br></pre></td></tr></table></figure><p>而要删除方式二创建的 Pod ，我们还能用下面的这行语句吗？</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod my-nginx-dep -n hello-world</span><br></pre></td></tr></table></figure><p>这么做，是删不掉的。因为每删除一次，k8s又会自动创建一个新的 my-nginx-dep 的 Pod（Deployment自愈能力的体现）。</p><p>原因在于方式二是通过 Deployment 的方式创建的 Pod（以部署一个应用的方式）。</p><p>如果要删除方式二创建的 Pod，可以通过下面的步骤进行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一步，获取deploy（部署）列表信息</span></span><br><span class="line">kubectl get deploy -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步，通过删除deploy（部署）的方式删除Pod</span></span><br><span class="line">kubectl delete deploy my-tomcat -n hello-world</span><br></pre></td></tr></table></figure><h3 id="2-Deployment的各种能力"><a href="#2-Deployment的各种能力" class="headerlink" title="2.Deployment的各种能力"></a>2.Deployment的各种能力</h3><h4 id="2-1-多副本能力"><a href="#2-1-多副本能力" class="headerlink" title="2.1 多副本能力"></a>2.1 多副本能力</h4><p>副本：可以理解为对于一个应用而言，将它运行在一个 Pod 上就是起一个副本，将它运行在多个 Pod 上，就是起了多个副本。</p><p>下面通过两种方式，来实现多副本的能力。</p><blockquote><p>方式一：命令行方式</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过部署的方式，创建nginx的3个副本</span></span><br><span class="line">kubectl create deployment nginx-base --image=nginx --replicas=3 -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看部署列表信息</span></span><br><span class="line">kubectl get deploy -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除本次部署（即同时删除3个Pod）</span></span><br><span class="line">kubectl delete deploy nginx-base -n hello-world</span><br></pre></td></tr></table></figure><blockquote><p>方式二：yaml文件方式</p></blockquote><p>（1）写一个 yaml 文件，例如 nginx-base.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span> <span class="comment"># 版本号</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span>    <span class="comment"># 资源类型</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx-base</span> <span class="comment"># 标签名称</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-base</span>  <span class="comment"># 本次部署的名字</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">hello-world</span> <span class="comment"># 命名空间</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span> <span class="comment"># 副本数量</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx-base</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx-base</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx:1.20</span> <span class="comment"># 容器的镜像名称</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">nginx</span>  <span class="comment"># 容器名</span></span><br></pre></td></tr></table></figure><p>（2）执行 nginx-base.yaml 文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx-base.yaml</span><br></pre></td></tr></table></figure><h4 id="2-2-扩缩容能力"><a href="#2-2-扩缩容能力" class="headerlink" title="2.2 扩缩容能力"></a>2.2 扩缩容能力</h4><p>扩容：遇到流量高峰，需要增加副本数量，降低当前负载</p><p>缩容：流量高峰过去，不需要这么多副本，减少数量</p><blockquote><p>方式一：命令行方式</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将nginx-base的副本数量扩容到5个</span></span><br><span class="line">kubectl scale --replicas=5 deployment/nginx-base -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将nginx-base的副本数量缩减到2个</span></span><br><span class="line">kubectl scale --replicas=2 deployment/nginx-base -n hello-world</span><br></pre></td></tr></table></figure><blockquote><p>方式二：修改yaml方式</p></blockquote><p>打开对应的 yaml 文件，修改副本发数量，然后保存退出即可。</p><p>（1）打开文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deploy</span><br><span class="line"></span><br><span class="line">kubectl edit deployment nginx-base</span><br></pre></td></tr></table></figure><p>（2）编辑文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1 <span class="comment"># 版本号</span></span><br><span class="line">kind: Deployment    <span class="comment"># 资源类型</span></span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx-base <span class="comment"># 标签名称</span></span><br><span class="line">  name: nginx-base  <span class="comment"># 本次部署的名字</span></span><br><span class="line">  namespace: hello-world <span class="comment"># 命名空间</span></span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 <span class="comment"># 副本数量</span></span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx-base</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-base</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:1.20 <span class="comment"># 容器的镜像名称</span></span><br><span class="line">        name: nginx  <span class="comment"># 容器名</span></span><br></pre></td></tr></table></figure><p>（3）保存退出</p><h4 id="2-3-自愈及故障转移能力"><a href="#2-3-自愈及故障转移能力" class="headerlink" title="2.3 自愈及故障转移能力"></a>2.3 自愈及故障转移能力</h4><p>自愈能力：假设某个 Pod 发生故障，不能正常对外提供服务，此时 k8s 会自动杀死该 Pod ，然后再将该 Pod 重新启动。</p><p>转移能力：假设某个 Pod 所在的结点 node-01 发生了宕机，整个服务器坏掉，不能正常使用，当 k8s 经过一小段时间（可能是5分钟）检测后，发现该服务器确实不能提供服务了，那么 k8s 会自动在其它结点上重新创建 node-01 结点里面的所有 Pod ，以达到正常对外提供服务的目的。</p><h4 id="2-4-滚动更新能力"><a href="#2-4-滚动更新能力" class="headerlink" title="2.4 滚动更新能力"></a>2.4 滚动更新能力</h4><p>k8s 在更新集群应用时，对于每个结点上的 Pod ，会先启动一个新的 Pod ，然后再停掉对应的老的 Pod，如此循环，直至实现每个结点上的 Pod 的更新，才算完成本次集群应用的更新。</p><blockquote><p>方式一：命令行方式</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">set</span> image deployment/nginx-base nginx=nginx:1.16.1 --record -n hello-world</span><br><span class="line"></span><br><span class="line">kubectl rollout status deployment/nginx-base -n hello-world</span><br></pre></td></tr></table></figure><blockquote><p>方式二：修改yaml方式</p></blockquote><p>打开对应的 yaml 文件，修改副本发数量，然后保存退出即可。</p><p>（1）打开文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deploy</span><br><span class="line"></span><br><span class="line">kubectl edit deployment nginx-base</span><br></pre></td></tr></table></figure><p>（2）编辑文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1 <span class="comment"># 版本号</span></span><br><span class="line">kind: Deployment    <span class="comment"># 资源类型</span></span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx-base <span class="comment"># 标签名称</span></span><br><span class="line">  name: nginx-base  <span class="comment"># 本次部署的名字</span></span><br><span class="line">  namespace: hello-world <span class="comment"># 命名空间</span></span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 <span class="comment"># 副本数量</span></span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx-base</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-base</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:1.16.1 <span class="comment"># 容器的镜像名称</span></span><br><span class="line">        name: nginx  <span class="comment"># 容器名</span></span><br></pre></td></tr></table></figure><p>（3）保存退出</p><h4 id="2-5-版本回退能力"><a href="#2-5-版本回退能力" class="headerlink" title="2.5 版本回退能力"></a>2.5 版本回退能力</h4><p>如果对当前这次部署不满意，无法正常提供服务，k8s可以回滚到之前的历史版本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#历史记录</span></span><br><span class="line">kubectl rollout <span class="built_in">history</span> deployment/nginx-base -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看某个历史详情</span></span><br><span class="line">kubectl rollout <span class="built_in">history</span> deployment/nginx-base --revision=2</span><br><span class="line"></span><br><span class="line"><span class="comment">#回滚到上次版本</span></span><br><span class="line">kubectl rollout undo deployment/nginx-base -n hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment">#回滚到指定版本</span></span><br><span class="line">kubectl rollout undo deployment/nginx-base --to-revision=2 -n hello-world</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）4、工作负载</title>
    <link href="https://blog.yongwang.lu/post/c56c5900.html"/>
    <id>https://blog.yongwang.lu/post/c56c5900.html</id>
    <published>2023-01-05T05:40:00.000Z</published>
    <updated>2023-01-05T05:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>参考： <a href="https://kubernetes.io/zh/docs/" title="Kubernetes 文档">Kubernetes 文档</a> &#x2F; <a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/" title="工作负载">工作负载</a></p><h2 id="一、kubernetes中的工作负载"><a href="#一、kubernetes中的工作负载" class="headerlink" title="一、kubernetes中的工作负载"></a>一、kubernetes中的工作负载</h2><h3 id="1、k8s中的工作负载"><a href="#1、k8s中的工作负载" class="headerlink" title="1、k8s中的工作负载"></a>1、k8s中的工作负载</h3><p>Kubernetes中内建了很多controller（控制器），这些相当于一个状态机，用来控制Pod的具体状态和行为</p><ul><li><p><code>Deployment</code>：适合无状态的服务部署</p></li><li><p><code>ReplicaSet</code> : 通俗讲副本的管理, 实际应用中建议使用 Deployment 而不是直接使用 ReplicaSet</p></li><li><p><code>StatefullSet</code>：用来管理有状态应用的工作负载 API 对象。</p></li><li><p><code>DaemonSet</code>：一次部署，所有的node节点都会部署，例如一些典型的应用场景：<br>运行集群存储 daemon，例如在每个Node上运行 glusterd、ceph 在每个Node上运行日志收集 daemon，例如 fluentd、 logstash 在每个Node上运行监控 daemon，例如 Prometheus Node Exporter</p></li><li><p><code>Job</code>：一次性的执行任务</p></li><li><p><code>Cronjob</code>：周期性的执行任务</p></li><li><p><code>ReplicationController</code> 确保在任何时候都有特定数量的 Pod 副本处于运行状态。  现在推荐使用配置 ReplicaSet 的 Deployment 来建立副本管理机制。</p></li></ul><h3 id="2、k8s的控制器图示"><a href="#2、k8s的控制器图示" class="headerlink" title="2、k8s的控制器图示"></a>2、k8s的控制器图示</h3><p><img src="https://res.yongwang.lu/img/202303221916960.png" alt="K8s控制器"></p>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）3、Pod详解</title>
    <link href="https://blog.yongwang.lu/post/3664a83f.html"/>
    <id>https://blog.yongwang.lu/post/3664a83f.html</id>
    <published>2023-01-05T04:40:00.000Z</published>
    <updated>2023-01-05T04:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/">https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/</a></p><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><blockquote><p>在Kubernetes集群中，Pod是所有业务类型的基础，也是K8S管理的<strong>最小单位级</strong>，它是<strong>一个或多个容器的组合</strong>。这些容器<strong>共享存储、网络和命名空间</strong>，以及如何运行的规范。在Pod中，所有容器都被统一安排和调度，并运行在共享的上下文中。对于具体应用而言，Pod是它们的逻辑主机，Pod包含业务相关的多个应用容器。</p></blockquote><h2 id="二、Pod实现机制与设计模式"><a href="#二、Pod实现机制与设计模式" class="headerlink" title="二、Pod实现机制与设计模式"></a>二、Pod实现机制与设计模式</h2><blockquote><p>每个Pod都有一个特殊的被称为”根容器”的Pause 容器（Pause容器，又叫Infrastructure容器）。 Pause容器对应的镜像属于Kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或者多个紧密相关的用户业务容器。</p></blockquote><p><img src="https://res.yongwang.lu/img/202303221607453.png" alt="pod组成"></p><blockquote><p>众所周知，容器之间是通过Namespace隔离的，Pod要想解决上述应用场景，那么就要让Pod里的容器之间高效共享。<br>具体分为两个部分：<strong>网络和存储</strong></p></blockquote><ul><li>共享网络</li></ul><blockquote><p>kubernetes的解法是这样的：会在每个Pod里先启动一个infra container小容器，然后让其他的容器连接进来这个网络命名空间，然后其他容器看到的网络试图就完全一样了，即网络设备、IP地址、Mac地址等，这就是解决网络共享问题。在Pod的IP地址就是infra container的IP地址。</p></blockquote><ul><li>共享存储</li></ul><blockquote><p>比如有两个容器，一个是nginx，另一个是普通的容器，普通容器要想访问nginx里的文件，就需要nginx容器将共享目录通过volume挂载出来，然后让普通容器挂载的这个volume，最后大家看到这个共享目录的内容一样。</p></blockquote><p>例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pod-write-read.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span>  </span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-podspec</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">write</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">centos</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;bash&quot;</span>,<span class="string">&quot;-c&quot;</span>,<span class="string">&quot;for i in &#123;1..100&#125;;do echo $i &gt;&gt; /data/hello;sleep 1;done&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">        <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line"> </span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">read</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">centos</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;bash&quot;</span>,<span class="string">&quot;-c&quot;</span>,<span class="string">&quot;tail -f /data/hello&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">        <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">   </span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">    <span class="attr">emptyDir:</span> &#123;&#125;</span><br></pre></td></tr></table></figure><p>上述示例中有两个容器，write容器负责提供数据，read消费数据，通过数据卷将写入数据的目录和读取数据的目录都放到了该卷中，这样每个容器都能看到该目录。<br>验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f pod-write-read.yaml</span><br><span class="line">$ kubectl logs my-pod -c <span class="built_in">read</span> -f</span><br></pre></td></tr></table></figure><p>在Pod中容器分为以下几个类型：</p><ul><li><p>Infrastructure Container：基础容器，维护整个Pod网络空间，对用户不可见</p></li><li><p>InitContainers：初始化容器，先于业务容器开始执行，一般用于业务容器的初始化工作</p></li><li><p>Containers：业务容器，具体跑应用程序的镜像</p></li></ul><h2 id="三、镜像拉取策略"><a href="#三、镜像拉取策略" class="headerlink" title="三、镜像拉取策略"></a>三、镜像拉取策略</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod001</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">busybox001</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">      <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br></pre></td></tr></table></figure><p>imagePullPolicy 字段有三个可选值：</p><ul><li><p>IfNotPresent：镜像在宿主机上不存在时才拉取</p></li><li><p>Always：<strong>默认值</strong>,每次创建 Pod 都会重新拉取一次镜像</p></li><li><p>Never： Pod 永远不会主动拉取这个镜像</p></li></ul><blockquote><p>注意，这里的重启是指在Pod所在Node上面本地重启，并不会调度到其他Node上去。</p></blockquote><h2 id="四、资源限制"><a href="#四、资源限制" class="headerlink" title="四、资源限制"></a>四、资源限制</h2><p>Pod资源配额有两种：</p><p>申请配额：调度时使用，参考是否有节点满足该配置</p><ul><li><p>spec.containers[].resources.limits.cpu</p></li><li><p>spec.containers[].resources.limits.memory</p></li></ul><p>限制配额：容器能使用的最大配置</p><ul><li><p>spec.containers[].resources.requests.cpu</p></li><li><p>spec.containers[].resources.requests.memory</p></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod002</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">busybox002</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;64Mi&quot;</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;250m&quot;</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;128Mi&quot;</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;500m&quot;</span></span><br></pre></td></tr></table></figure><p>其中cpu值比较抽象，可以这么理解：</p><blockquote><p>1核&#x3D;1000m</p><p>1.5核&#x3D;1500m</p></blockquote><p>那上面限制配置就是1核的二分之一（500m），即该容器最大使用半核CPU。</p><p>该值也可以写成浮点数，更容易理解：</p><blockquote><p>半核&#x3D;0.5</p><p>1核&#x3D;1</p><p>1.5核&#x3D;1.5</p></blockquote><h2 id="五、重启策略"><a href="#五、重启策略" class="headerlink" title="五、重启策略"></a>五、重启策略</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod003</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">busybox003</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br></pre></td></tr></table></figure><p>restartPolicy字段有三个可选值：</p><ul><li><p>Always：当容器终止退出后，总是重启容器，<strong>默认策略</strong>。</p></li><li><p>OnFailure：当容器异常退出（退出状态码非0）时，才重启容器。适于job</p></li><li><p>Never：当容器终止退出，从不重启容器。适于job</p></li></ul><h2 id="六、-健康检查"><a href="#六、-健康检查" class="headerlink" title="六、 健康检查"></a>六、 健康检查</h2><blockquote><p>默认情况下，kubelet 根据容器状态作为健康依据，但不能容器中应用程序状态，例如程序假死。这就会导致无法提供服务，丢失流量。因此引入健康检查机制确保容器健康存活。</p></blockquote><p>健康检查有两种类型：</p><ul><li>livenessProbe</li></ul><blockquote><p>如果检查失败，将杀死容器，根据Pod的restartPolicy来操作。</p></blockquote><ul><li>readinessProbe</li></ul><blockquote><p>如果检查失败，Kubernetes会把Pod从service endpoints中剔除。</p></blockquote><p>这两种类型支持三种检查方法：</p><ul><li>httpGet</li></ul><blockquote><p>发送HTTP请求，返回200-400范围状态码为成功。</p></blockquote><ul><li>exec</li></ul><blockquote><p>执行Shell命令返回状态码是0为成功。</p></blockquote><ul><li>tcpSocket</li></ul><blockquote><p>发起TCP Socket建立成功。</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># health-check.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">test:</span> <span class="string">liveness</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-exec</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">liveness</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">touch</span> <span class="string">/tmp/healthy;</span> <span class="string">sleep</span> <span class="number">30</span><span class="string">;</span> <span class="string">rm</span> <span class="string">-rf</span> <span class="string">/tmp/healthy;</span> <span class="string">sleep</span> <span class="number">60</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">exec:</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">cat</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/tmp/healthy</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">5</span></span><br></pre></td></tr></table></figure><p>上述示例：启动容器第一件事在容器内创建文件，停止30s，删除该文件，再停止60s，确保容器还在运行中。</p><p>验证现象：容器启动正常，30s后异常，会restartPolicy策略自动重建，容器继续正常，反复现象。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f health-check.yaml</span><br><span class="line">$ kubectl describe pod liveness-exec</span><br></pre></td></tr></table></figure><h2 id="七、调度策略"><a href="#七、调度策略" class="headerlink" title="七、调度策略"></a>七、调度策略</h2><blockquote><p>先看下创建一个Pod的工作流程： create pod -&gt; apiserver -&gt; write etcd -&gt; scheduler -&gt; bind pod to node -&gt; write etcd -&gt; kubelet( apiserver get pod) -&gt; dcoekr api,create container -&gt; apiserver -&gt; update pod status to etcd -&gt; kubectl get pods</p></blockquote><p><img src="https://res.yongwang.lu/img/202303221620812.png" alt="图片"></p><p>Pod根据调度器默认算法将Pod分配到合适的节点上，一般是比较空闲的节点。但有些情况我们希望将Pod分配到指定节点，该怎么做呢？</p><blockquote><p>这里给你介绍调度策略：<strong>nodeName、nodeSelector和污点</strong></p></blockquote><h3 id="1）nodeName"><a href="#1）nodeName" class="headerlink" title="1）nodeName"></a>1）nodeName</h3><p>nodeName用于将Pod调度到指定的Node名称上。<br>例如：下面示例会绕过调度系统，直接分配到k8s-node1节点。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SchedulePolicy-nodeName.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">busyboxnn</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">k8s-node1</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">bs</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;ping&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;baidu.com&quot;</span></span><br></pre></td></tr></table></figure><p>执行&amp;查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f SchedulePolicy-nodeName.yaml</span><br><span class="line">$ kubectl get pod busyboxnn -o wide</span><br></pre></td></tr></table></figure><h3 id="2）nodeSelector"><a href="#2）nodeSelector" class="headerlink" title="2）nodeSelector"></a>2）nodeSelector</h3><blockquote><p>nodeSelector用于将Pod调度到匹配Label的Node上。先给规划node用途，然后打标签，例如将两台node划分给不同团队使用：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl label nodes k8s-node1 team=a</span><br><span class="line">$ kubectl label nodes k8s-node2 team=b</span><br></pre></td></tr></table></figure><p>后在创建Pod只会被调度到含有team&#x3D;a标签的节点上。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SchedulePolicy-nodeSelector.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">busyboxsn</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">team:</span> <span class="string">b</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">bs</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;ping&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;baidu.com&quot;</span></span><br></pre></td></tr></table></figure><p>执行&amp;查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f SchedulePolicy-nodeSelector.yaml</span><br><span class="line">$ kubectl get pod busyboxsn -o wide</span><br></pre></td></tr></table></figure><h3 id="3）taint（污点）与tolerations（容忍）"><a href="#3）taint（污点）与tolerations（容忍）" class="headerlink" title="3）taint（污点）与tolerations（容忍）"></a>3）taint（污点）与tolerations（容忍）</h3><ul><li>污点应用场景：节点独占，例如具有特殊硬件设备的节点，如GPU<br>设置污点命令：<br>kubectl taint node [node] key&#x3D;value[effect]</li></ul><p>其中[effect] 可取值：</p><ul><li><p>NoSchedule ：一定不能被调度。</p></li><li><p>PreferNoSchedule：尽量不要调度。</p></li><li><p>NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。</p></li></ul><p>示例：</p><p>先给节点设置污点，说明这个节点不是谁都可以调度过来的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint node k8s-node1  abc=123:NoSchedule</span><br></pre></td></tr></table></figure><p>查看污点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe node k8s-node1 |grep Taints</span><br></pre></td></tr></table></figure><blockquote><p>然后在创建Pod<strong>只有声明了容忍污点（tolerations），才允许被调度到abc&#x3D;123污点节点上</strong>。</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SchedulePolicy-tolerations.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">busybox3</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;abc&quot;</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">&quot;Equal&quot;</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;123&quot;</span></span><br><span class="line">    <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">bs</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;ping&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;baidu.com&quot;</span></span><br></pre></td></tr></table></figure><p>如果不配置容忍污点，则永远不会调度到k8s-node1。（也可以叫做<strong>反亲和性</strong>）<br>去掉污点：</p><blockquote><p>kubectl taint node k8s-node1 abc&#x3D;123:NoSchedule</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint node k8s-node1 abc:NoSchedule-</span><br></pre></td></tr></table></figure><p>master节点默认是打了污点标记，不调度的，去掉污点标记</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#添加 尽量不调度 PreferNoSchedule </span></span><br><span class="line">kubectl taint nodes k8s-master node-role.kubernetes.io/master:PreferNoSchedule</span><br><span class="line"><span class="comment">#去除污点NoSchedule，最后一个&quot;-&quot;代表删除</span></span><br><span class="line">kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule-</span><br></pre></td></tr></table></figure><h2 id="八、Pod状态"><a href="#八、Pod状态" class="headerlink" title="八、Pod状态"></a>八、Pod状态</h2><h3 id="1）Pod常见状态"><a href="#1）Pod常见状态" class="headerlink" title="1）Pod常见状态"></a>1）Pod常见状态</h3><p>1、Pending：等待中</p><blockquote><p>Pod已经被创建，但还没有完成调度，或者说有一个或多个镜像正处于从远程仓库下载的过程。处在这个阶段的Pod可能正在写数据到etcd中、调度、pull镜像或启动容器。</p></blockquote><p>2、Running：运行中</p><blockquote><p>该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。</p></blockquote><p>3、Succeeded：正常终止</p><blockquote><p>Pod中的所有的容器已经正常的执行后退出，并且不会自动重启，一般会是在部署job的时候会出现。</p></blockquote><p>4、Failed：异常停止</p><blockquote><p>Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。</p></blockquote><p>5、Terminating 或 Unknown 状态</p><blockquote><p>从 v1.5 开始，Kubernetes 不会因为 Node 失联而删除其上正在运行的 Pod，而是将其标记为 Terminating 或 Unknown 状态。</p></blockquote><p>6、Error 状态</p><blockquote><p>通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括：</p></blockquote><ul><li><p>依赖的 ConfigMap、Secret 或者 PV 等不存在</p></li><li><p>请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等</p></li><li><p>违反集群的安全策略，比如违反了 PodSecurityPolicy 等</p></li><li><p>容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定。</p></li></ul><p>7、Completed状态</p><blockquote><p>状态由ContainerCreating变为Completed再变为CrashLoopBackOff，原因是command: 或args 参数错误无法正常执行。</p></blockquote><p>用一张图来表示Pod的各个状态</p><p><img src="https://res.yongwang.lu/img/202303221614525.png" alt="图片"></p><h3 id="2）Pod-其它状态详细说明"><a href="#2）Pod-其它状态详细说明" class="headerlink" title="2）Pod 其它状态详细说明"></a>2）Pod 其它状态详细说明</h3><table><thead><tr><th>状态</th><th>描述</th></tr></thead><tbody><tr><td>ContainerCreating</td><td>容器创建中</td></tr><tr><td>PodInitializing pod</td><td>初始化中</td></tr><tr><td>CrashLoopBackOff</td><td>容器曾经启动了，但可能又异常退出了，kubelet正在将它重启</td></tr><tr><td>InvalidImageName</td><td>无法解析镜像名称</td></tr><tr><td>ImageInspectError</td><td>无法校验镜像</td></tr><tr><td>ErrImageNeverPull</td><td>策略禁止拉取镜像</td></tr><tr><td>ImagePullBackOff</td><td>正在重试拉取</td></tr><tr><td>RegistryUnavailable</td><td>连接不到镜像中心</td></tr><tr><td>ErrImagePull</td><td>通用的拉取镜像出错</td></tr><tr><td>CreateContainerConfigError</td><td>不能创建kubelet使用的容器配置</td></tr><tr><td>CreateContainerError</td><td>创建容器失败</td></tr><tr><td>m.internalLifecycle.PreStartContainer</td><td>执行hook报错</td></tr><tr><td>RunContainerError</td><td>启动容器失败</td></tr><tr><td>PostStartHookError</td><td>执行hook报错</td></tr><tr><td>ContainersNotInitialized</td><td>容器没有初始化完毕</td></tr><tr><td>ContainersNotRead</td><td>容器没有准备完毕</td></tr><tr><td>DockerDaemonNotReady</td><td>docker还没有完全启动</td></tr><tr><td>NetworkPluginNotReady</td><td>网络插件还没有完全启动</td></tr></tbody></table><p>pod启动后停止问题总结（ContainerCreating-》Completed-》CrashLoopBackOff）：</p><blockquote><p>pod 是否能持续运行，是由执行命令决定的，执行命令如果一执行就停止，控制台也停止持续输出，pod生命周期就结束，pod状态就会变成CrashLoopBackOff。</p></blockquote>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）2、集群搭建(containerd)</title>
    <link href="https://blog.yongwang.lu/post/9866a7eb.html"/>
    <id>https://blog.yongwang.lu/post/9866a7eb.html</id>
    <published>2023-01-05T03:40:00.000Z</published>
    <updated>2023-01-05T03:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h1><p>参考： <a href="https://kubernetes.io/zh/docs/" title="Kubernetes 文档">Kubernetes 文档</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/" title="入门">入门</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/production-environment/" title="生产环境">生产环境</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/" title="使用部署工具安装 Kubernetes">使用部署工具安装 Kubernetes</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/" title="使用 kubeadm 引导集群">使用 kubeadm 引导集群</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" title="安装 kubeadm">安装 kubeadm</a></p><p>流程图</p><p><img src="https://res.yongwang.lu/img/image_rFtNPTs_40.png"></p><h2 id="准备开始"><a href="#准备开始" class="headerlink" title="准备开始"></a>准备开始</h2><ul><li>一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令</li><li>每台机器 <code>2 GB</code> 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)</li><li><code>2 CPU 核</code>或更多</li><li>集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)</li><li>节点之中<code>不可以有重复的</code>主机名、MAC 地址或 product_uuid。请参见<a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#verify-mac-address" title="这里">这里</a>了解更多详细信息。</li><li>开启机器上的某些端口。请参见<a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports" title="这里">这里</a> 了解更多详细信息。</li><li>禁用交换分区。为了保证 kubelet 正常工作，你 <strong>必须</strong> <code>禁用交换分区</code>。</li></ul><h2 id="U-确保每个节点上-MAC-地址和-product-uuid-的唯一性-x20"><a href="#U-确保每个节点上-MAC-地址和-product-uuid-的唯一性-x20" class="headerlink" title="U. 确保每个节点上 MAC 地址和 product_uuid 的唯一性&#x20;"></a>U. 确保每个节点上 MAC 地址和 product_uuid 的唯一性&#x20;</h2><ul><li>你可以使用命令 <code>ip link</code> 或 <code>ifconfig -a</code> 来获取网络接口的 MAC 地址</li><li>可以使用 <code>sudo cat /sys/class/dmi/id/product_uuid</code> 命令对 product_uuid 校验</li></ul><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p><strong>[k8s-master|k8s-worker1|k8s-worker2]$</strong></p><ol><li>设置当前用户sudo免密[选做]<blockquote><p>不想每次都输入密码 - 加速</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 缓存 sudo 密码</span></span><br><span class="line"><span class="built_in">echo</span> ubuntu | sudo -v -S </span><br><span class="line"></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">sudo <span class="built_in">tee</span> /etc/sudoers.d/<span class="variable">$USER</span> &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">$USER ALL=(ALL) NOPASSWD: ALL</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li>使用国内镜像仓库[选做]<blockquote><p>软件安装 - 加速</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更换阿里云加速</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cp</span> /etc/apt/sources.list /etc/apt/sources.list_bak</span><br><span class="line">vim /etc/apt/sources.list</span><br><span class="line"><span class="comment"># 查看/etc/apt/sources.list中的URL是archive.ubuntu还是cn.archive.ubuntu </span></span><br><span class="line"><span class="comment"># 然后再执行：</span></span><br><span class="line">sudo sed -i <span class="string">&#x27;s/cn.archive.ubuntu.com/mirrors.aliyun.com/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line"><span class="comment"># 或 </span></span><br><span class="line">sudo sed -i <span class="string">&#x27;s/archive.ubuntu.com/mirrors.aliyun.com/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>编辑 hosts&lt;必做&gt; <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">tee</span> -a /etc/hosts &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">192.168.147.131 k8s-master</span></span><br><span class="line"><span class="string">192.168.147.132 k8s-worker1</span></span><br><span class="line"><span class="string">192.168.147.133 k8s-worker2</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li></ol><p><strong>[k8s-master|k8s-worker1|k8s-worker2]$</strong></p><ol><li><p>禁用 swap&lt;必做&gt;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交换文件</span></span><br><span class="line">SWAPF=$(awk <span class="string">&#x27;/swap/ &#123;print $1&#125;&#x27;</span> /etc/fstab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即禁用</span></span><br><span class="line">sudo swapoff <span class="variable">$SWAPF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 永久禁用</span></span><br><span class="line">sudo sed -i <span class="string">&#x27;/swap/d&#x27;</span> /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除交换文件</span></span><br><span class="line">sudo <span class="built_in">rm</span> <span class="variable">$SWAPF</span></span><br></pre></td></tr></table></figure></li><li><p>模块支持&lt;必做&gt;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">sudo apt -y install bridge-utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即生效</span></span><br><span class="line">sudo modprobe br_netfilter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 内核支持</span></span><br><span class="line">sudo <span class="built_in">tee</span> /etc/sysctl.d/k8s.conf &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">net.ipv4.ip_forward=1</span></span><br><span class="line"><span class="string">vm.swappiness=0</span></span><br><span class="line"><span class="string">vm.overcommit_memory=1</span></span><br><span class="line"><span class="string">vm.panic_on_oom=0</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即生效</span></span><br><span class="line">sudo sysctl -p /etc/sysctl.d/k8s.conf</span><br></pre></td></tr></table></figure></li><li><p>安装运行时&lt;必做&gt;&#x20;</p><p>这里不再采用docker作为k8s的运行时. 因为K8s自1.24 对docker支持改为 安装指定CRI才能访问.&#x20;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 containerd https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/</span></span><br><span class="line"><span class="comment"># 下载最新版本 containerd 因为在k8s后续版本废弃1.5.x 安装对应的deb版本</span></span><br><span class="line">sudo apt install -y containerd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锁定版本</span></span><br><span class="line">sudo apt-mark hold containerd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line">sudo <span class="built_in">mkdir</span> /etc/containerd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成默认配置文件</span></span><br><span class="line">containerd config default | \</span><br><span class="line">sudo <span class="built_in">tee</span> /etc/containerd/config.toml &gt;/dev/null</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改配置文件 加速</span></span><br><span class="line">sudo sed -i \</span><br><span class="line">-e <span class="string">&#x27;/sandbox_image/s?k8s.gcr.io?registry.aliyuncs.com/google_containers?&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/SystemdCgroup/s?false?true?&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/registry.mirrors/a\        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/registry.mirrors/a\          endpoint = [&quot;https://docker.nju.edu.cn/&quot;]&#x27;</span> /etc/containerd/config.toml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 服务重启</span></span><br><span class="line">sudo systemctl restart containerd</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行时配置私服</span></span><br><span class="line">[plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry]</span><br><span class="line">    [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors]</span><br><span class="line">        [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors.<span class="string">&quot;docker.io&quot;</span>]</span><br><span class="line">          endpoint = [<span class="string">&quot;https://------.mirror.aliyuncs.com&quot;</span>, <span class="string">&quot;https://registry-1.docker.io&quot;</span>]</span><br><span class="line"><span class="comment"># 运行时配置登录</span></span><br><span class="line">[plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry]</span><br><span class="line">   [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors]</span><br><span class="line">       [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors.<span class="string">&quot;docker.io&quot;</span>]</span><br><span class="line">          endpoint = [<span class="string">&quot;https://registry-1.docker.io&quot;</span>]</span><br><span class="line">       [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors.<span class="string">&quot;registry.cn-hangzhou.aliyuncs.com&quot;</span>]</span><br><span class="line">          endpoint = [<span class="string">&quot;https://registry.cn-hangzhou.aliyuncs.com&quot;</span>]</span><br><span class="line">      [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.configs]</span><br><span class="line">        [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.configs.<span class="string">&quot;registry.cn-hangzhou.aliyuncs.com&quot;</span>.tls]</span><br><span class="line">          insecure_skip_verify = <span class="literal">true</span></span><br><span class="line">        [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.configs.<span class="string">&quot;registry.cn-hangzhou.aliyuncs.com&quot;</span>.auth]</span><br><span class="line">          username = <span class="string">&quot;阿里云账户，类似xxx@aliyun.com&quot;</span></span><br><span class="line">          password = <span class="string">&quot;上一步设置的固定密码&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h2 id="安装-K8s"><a href="#安装-K8s" class="headerlink" title="安装 K8s"></a>安装 K8s</h2><p><strong>[kiosk@k8s-master|k8s-worker1|k8s-worker2]$</strong></p><ol><li>安装 kubeadm、kubelet 和 kubectl<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新 apt 包索引并安装使用 Kubernetes apt 仓库所需要的包</span></span><br><span class="line">sudo apt -y install apt-transport-https ca-certificates curl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 Google Cloud 公开签名秘钥</span></span><br><span class="line">curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 Kubernetes apt 仓库</span></span><br><span class="line">MIRROR_URL=https://mirrors.aliyun.com/kubernetes/apt/</span><br><span class="line">sudo <span class="built_in">tee</span> /etc/apt/sources.list.d/kubernetes.list &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">deb $MIRROR_URL kubernetes-xenial main</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新 apt 包索引</span></span><br><span class="line">sudo <span class="built_in">cp</span> /etc/apt/trusted.gpg /etc/apt/trusted.gpg.d</span><br><span class="line">sudo apt update -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询指定版本</span></span><br><span class="line">sudo apt-cache madison kubelet | grep 1.25</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 kubelet、kubeadm 和 kubectl 指定版本</span></span><br><span class="line">sudo apt install -y kubelet=1.25.1-00 kubeadm=1.25.1-00 kubectl=1.25.1-00</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锁定版本</span></span><br><span class="line">sudo apt-mark hold kubelet kubeadm kubectl</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装完K8s以后会自带crictl</span></span><br><span class="line"><span class="comment"># crictl 配置文件</span></span><br><span class="line">sudo <span class="built_in">tee</span> /etc/crictl.yaml &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">runtime-endpoint: unix:///run/containerd/containerd.sock</span></span><br><span class="line"><span class="string">image-endpoint: unix:///run/containerd/containerd.sock</span></span><br><span class="line"><span class="string">timeout: 10</span></span><br><span class="line"><span class="string">debug: false</span></span><br><span class="line"><span class="string">pull-image-on-create: true</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li>k8s 支持<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加 k8s 支持</span></span><br><span class="line">sudo sed -i <span class="string">&#x27;/ExecStart=\//s|$| --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock --cgroup-driver=systemd|&#x27;</span> \</span><br><span class="line">  /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启 kubelet 服务</span></span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart kubelet</span><br></pre></td></tr></table></figure></li></ol><p><strong>[k8s-master]$</strong></p><ol><li>初始化<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成初始文件</span></span><br><span class="line">sudo kubeadm config <span class="built_in">print</span> init-defaults &gt; kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改文件</span></span><br><span class="line">NICP=$(ip a | awk <span class="string">&#x27;/inet / &#123;print $2&#125;&#x27;</span> | grep -v ^127 | sed <span class="string">&#x27;s+/24++&#x27;</span>)</span><br><span class="line">sudo sed -i \</span><br><span class="line">  -e <span class="string">&quot;/advertiseAddress/s?:.*?: <span class="variable">$NICP</span>?&quot;</span> \</span><br><span class="line">  -e <span class="string">&quot;/name/s?:.*?: <span class="subst">$(hostname -s)</span>?&quot;</span> \</span><br><span class="line">  -e <span class="string">&quot;/clusterName/s?:.*?: k8s?&quot;</span> \</span><br><span class="line">  -e <span class="string">&quot;/imageRepository/s?:.*?: registry.aliyuncs.com/google_containers?&quot;</span> kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用初始文件，初始化集群</span></span><br><span class="line">sudo kubeadm init --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><blockquote><p>Your Kubernetes control-plane has initialized <code>successfully</code>!<br>PS: 普通用户管理集群</p><p>To start using your cluster, you need to run the following as a regular user:<br>bash<br>mkdir -p $HOME&#x2F;.kube<br>sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config<br>sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</p><p>PS：root 用户管理集群</p><p>Alternatively, if you are the root user, you can run:</p><p>bash<br>export KUBECONFIG&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;admin.conf</p><p>You should now deploy a pod network to the cluster.Run “<code>kubectl apply -f [podnetwork].yaml</code>“ with one of the options listed at:<a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/" title="https://kubernetes.io/docs/concepts/cluster-administration/addons/">https://kubernetes.io/docs/concepts/cluster-administration/addons/</a><br>Then you can join any number of worker nodes by running the following on each as root:<br>bash<br>kubeadm join 192.168.147.128:6443 –token abcdef.0123456789abcdef –discovery-token-ca-cert-hash sha256:c4781194de65ebb47984fc5e7e64d4897875410825ce4d18df81da1a298afa1f</p></blockquote></li><li>配置文件 - Client<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line"></span><br><span class="line"><span class="comment"># user 复制配置文件</span></span><br><span class="line">sudo \<span class="built_in">cp</span> /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># root 变量</span></span><br><span class="line">sudo <span class="built_in">tee</span> -a ~root/.bashrc &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">export KUBECONFIG=/etc/kubernetes/admin.conf</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li>创建网络<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</span><br></pre></td></tr></table></figure></li><li>命令补全 - Client[建议]<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl completion --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即生效</span></span><br><span class="line"><span class="built_in">source</span> &lt;(kubectl completion bash)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 永久生效</span></span><br><span class="line"><span class="built_in">mkdir</span> ~/.kube 2&gt;/dev/null</span><br><span class="line">kubectl completion bash &gt; ~/.kube/completion.bash.inc</span><br><span class="line"><span class="built_in">printf</span> <span class="string">&quot;</span></span><br><span class="line"><span class="string"># Kubectl shell completion</span></span><br><span class="line"><span class="string">source &#x27;<span class="variable">$HOME</span>/.kube/completion.bash.inc&#x27;</span></span><br><span class="line"><span class="string">&quot;</span> &gt;&gt; <span class="variable">$HOME</span>/.bashrc</span><br><span class="line"><span class="built_in">source</span> <span class="variable">$HOME</span>/.bashrc</span><br></pre></td></tr></table></figure></li><li>命令别名 - Client[建议]<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网址 https://kubernetes.io/zh-cn/docs/reference/kubectl/cheatsheet/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 永久生效</span></span><br><span class="line"><span class="built_in">tee</span> -a <span class="variable">$HOME</span>/.bashrc &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">alias k=&#x27;kubectl&#x27;</span></span><br><span class="line"><span class="string">complete -F __start_kubectl k</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即生效</span></span><br><span class="line"><span class="built_in">source</span> <span class="variable">$HOME</span>/.bashrc</span><br></pre></td></tr></table></figure></li></ol><p><strong>[k8s-worker1|k8s-worker2]$</strong></p><ol><li>加入集群<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo \</span><br><span class="line">  kubeadm <span class="built_in">join</span> 192.168.147.128:6443 \</span><br><span class="line">  --token abcdef.0123456789abcdef \</span><br><span class="line">  --discovery-token-ca-cert-hash sha256:c4781194de65ebb47984fc5e7e64d4897875410825ce4d18df81da1a298afa1f</span><br></pre></td></tr></table></figure></li></ol><h2 id="C-确认环境正常"><a href="#C-确认环境正常" class="headerlink" title="C. 确认环境正常"></a>C. 确认环境正常</h2><p><strong>[k8s-master]</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME          STATUS     ROLES           AGE     VERSION</span><br><span class="line">k8s-master   `Ready`     control-plane   9m17s   `v1.25.1`</span><br><span class="line">k8s-worker1  `Ready`     &lt;none&gt;          90s     `v1.25.1`</span><br><span class="line">k8s-worker2  `Ready`     &lt;none&gt;          51s     `v1.25.1`</span><br><span class="line"></span><br><span class="line">$ kubectl get componentstatuses</span><br><span class="line">Warning: v1 ComponentStatus is deprecated <span class="keyword">in</span> v1.19+</span><br><span class="line">NAME                 STATUS    MESSAGE                         ERROR</span><br><span class="line">scheduler           `Healthy`  ok</span><br><span class="line">controller-manager  `Healthy`  ok</span><br><span class="line">etcd-0              `Healthy`  &#123;<span class="string">&quot;health&quot;</span>:<span class="string">&quot;true&quot;</span>,<span class="string">&quot;reason&quot;</span>:<span class="string">&quot;&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">$ kubectl -n kube-system get pod -w</span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">calico-kube-controllers-798cc86c47-dkdjl   1/1     Running   0          4m5s</span><br><span class="line">calico-node-ftwk8                          1/1     Running   0          4m5s</span><br><span class="line">calico-node-hstcg                          1/1     Running   0          109s</span><br><span class="line">calico-node-lcnw6                          1/1     Running   0          2m28s</span><br><span class="line">coredns-c676cc86f-mxpb8                    1/1     Running   0          10m</span><br><span class="line">coredns-c676cc86f-vhzzh                    1/1     Running   0          10m</span><br><span class="line">etcd-k8s-master                            1/1     Running   0          10m</span><br><span class="line">kube-apiserver-k8s-master                  1/1     Running   0          10m</span><br><span class="line">kube-controller-manager-k8s-master         1/1     Running   0          10m</span><br><span class="line">kube-proxy-g2tz9                           1/1     Running   0          109s</span><br><span class="line">kube-proxy-j4fgc                           1/1     Running   0          10m</span><br><span class="line">kube-proxy-nz8vj                           1/1     Running   0          2m28s</span><br><span class="line">kube-scheduler-k8s-master                  1/1     Running   0          10m</span><br><span class="line">&lt;Ctrl-C&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes（k8s）1、基本介绍和功能架构</title>
    <link href="https://blog.yongwang.lu/post/700b7316.html"/>
    <id>https://blog.yongwang.lu/post/700b7316.html</id>
    <published>2023-01-05T02:40:00.000Z</published>
    <updated>2023-01-05T02:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>参考： <a href="https://kubernetes.io/zh/docs/" title="Kubernetes 文档">Kubernetes 文档</a> &#x2F; <a href="https://kubernetes.io/zh/docs/setup/" title="入门">入门</a> &#x2F; <a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/" title="K8s架构">K8s架构</a></p><h2 id="1、kubernetes-概述"><a href="#1、kubernetes-概述" class="headerlink" title="1、kubernetes 概述"></a>1、kubernetes 概述</h2><p>kubernetes，简称 <a href="https://so.csdn.net/so/search?q=K8s&spm=1001.2101.3001.7020">K8s</a>，是用 8 代替 8 个字符“ubernete”而成的缩写。是一个开源 的，用于管理云平台中多个主机上的容器化的应用，Kubernetes 的目标是让部署容器化的 应用简单并且高效(powerful),Kubernetes 提供了应用部署，规划，更新，维护的一种 机制。</p><p>传统的应用部署方式是通过插件或脚本来安装应用。这样做的缺点是应用的运行、配 置、管理、所有生存周期将与当前操作系统绑定，这样做并不利于应用的升级更新&#x2F;回滚等 操作，当然也可以通过<a href="https://so.csdn.net/so/search?q=%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA&spm=1001.2101.3001.7020">创建虚拟机</a>的方式来实现某些功能，但是虚拟机非常重，并不利于 可移植性。</p><p>新的方式是通过部署容器方式实现，每个容器之间互相隔离，每个容器有自己的文件 系统 ，容器之间进程不会相互影响，能区分计算资源。相对于虚拟机，容器能快速部署， 由于容器与底层设施、机器文件系统解耦的，所以它能在不同云、不同版本操作系统间进 行迁移。</p><p>容器占用资源少、部署快，每个应用可以被打包成一个容器镜像，每个应用与容器间 成一对一关系也使容器有更大优势，使用容器可以在 build 或 release 的阶段，为应用创 建容器镜像，因为每个应用不需要与其余的应用堆栈组合，也不依赖于生产环境基础结构， 这使得从研发到测试、生产能提供一致环境。类似地，容器比虚拟机轻量、更“透明”， 这更便于监控和管理。</p><p>Kubernetes 是 Google 开源的一个容器编排引擎，它支持自动化部署、大规模可伸缩、 应用容器化管理。在生产环境中部署一个应用程序时，通常要部署该应用的多个实例以便 对应用请求进行负载均衡。</p><p>在 Kubernetes 中，我们可以创建多个容器，每个容器里面运行一个应用实例，然后通 过内置的负载均衡策略，实现对这一组应用实例的管理、发现、访问，而这些细节都不需 要运维人员去进行复杂的手工配置和处理。</p><h2 id="2、kubernetes-功能和架构"><a href="#2、kubernetes-功能和架构" class="headerlink" title="2、kubernetes 功能和架构"></a>2、kubernetes 功能和架构</h2><h3 id="2-1、概述"><a href="#2-1、概述" class="headerlink" title="2.1、概述"></a>2.1、概述</h3><p>Kubernetes 是一个轻便的和可扩展的开源平台，用于管理容器化应用和服务。通过 Kubernetes 能够进行应用的自动化部署和扩缩容。在 Kubernetes 中，会将组成应用的容 器组合成一个逻辑单元以更易管理和发现。Kubernetes 积累了作为 Google 生产环境运行 工作负载 15 年的经验，并吸收了来自于社区的最佳想法和实践。</p><h3 id="2-2、K8s-特性"><a href="#2-2、K8s-特性" class="headerlink" title="2.2、K8s 特性"></a>2.2、K8s 特性</h3><ul><li>自动化装箱：在不牺牲可用性的条件下，基于容器对资源的要求和约束自动部署容器。同时，为了提高利用率和节省更多资源，将关键和最佳工作量结合在一起。</li><li>自愈能力：当容器失败时，会对容器进行重启；当所部署的Node节点有问题时，会对容器进行重新部署和重新调度；当容器未通过监控检查时，会关闭此容器；直到容器正常运行时，才会对外提供服务。</li><li>水平扩容：通过简单的命令、用户界面或基于CPU的使用情况，能够对应用进行扩容和缩容。</li><li>服务发现和负载均衡：开发者不需要使用额外的服务发现机制，就能够基于Kubernetes进行服务发现和负载均衡。</li><li>自动发布和回滚：Kubernetes能够程序化的发布应用和相关的配置。如果发布有问题，Kubernetes将能够回归发生的变更。</li><li>保密和配置管理：在不需要重新构建镜像的情况下，可以部署和更新保密和应用配置。</li><li>存储编排：自动挂接存储系统，这些存储系统可以来自于本地、公共云提供商（例如：GCP和AWS）、网络存储(例如：NFS、iSCSI、Gluster、Ceph、Cinder和Floker等)。</li></ul><h3 id="2-3、k8s组件"><a href="#2-3、k8s组件" class="headerlink" title="2.3、k8s组件"></a>2.3、k8s组件</h3><p><img src="https://res.yongwang.lu/img/202303221859050.png" alt="k8s组件架构"></p><p>集群中总体分Master节点和Worker节点</p><p>Master节点对应上图控制平面组件（Control Plane Components）相当于组织部长，负责调度分配，不干具体的事。</p><p>Worker节点对应Node，就是实际干活的人</p><p>2.4、master节点下组件</p><ul><li>kube-apiserver</li></ul><p>kube-apiserver是整个集群的入口，他就像个网关，要访问集群中的组件，必须通过他。</p><ul><li>etcd</li></ul><p>一致且高度可用的键值存储，用作 Kubernetes 的所有集群数据的后台数据库。</p><ul><li>kube-scheduler</li></ul><p>负责监视新创建的、未指定运行<a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/" title="节点（node）">节点（node）</a>的 <a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/" title="Pods">Pods</a>， 并选择节点来让 Pod 在上面运行。</p><ul><li>kube-controller-manager</li></ul><p>处理集群中常规后台任务，一个资源对应一个控制器</p><p>2.5、worker节点下组件</p><ul><li>kubelet</li></ul><p><code>kubelet</code>会在集群中每个<a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/" title="节点（node）">节点（node）</a>上运行。 它保证<a href="https://kubernetes.io/zh-cn/docs/concepts/overview/what-is-kubernetes/#why-containers" title="容器（containers）">容器（containers）</a>都运行在 <a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/" title="Pod">Pod</a> 中。kubelet 接收一组通过各类机制提供给它的 PodSpecs， 确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。</p><p>说白了就是节点的控制器，负责管理本节点容器。</p><ul><li>kube-proxy</li></ul><p><a href="https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-proxy/" title="kube-proxy">kube-proxy</a> 是集群中每个<a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/" title="节点（node）">节点（node）</a>上所运行的网络代理， 实现 Kubernetes <a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/" title="服务（Service）">服务（Service）</a> 概念的一部分。kube-proxy 维护节点上的一些网络规则， 这些网络规则会允许从集群内部或外部的网络会话与 Pod 进行网络通信。</p><p>至此，我相信朋友们对kubernetes有了一个大致的了解，下篇我们就开始搭建kubernetes集群</p>]]></content>
    
    
    <summary type="html">kubernetes1.26</summary>
    
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://blog.yongwang.lu/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Containerd ctr 和 crictl 客户端命令介绍与实战操作（nerdctl ）</title>
    <link href="https://blog.yongwang.lu/post/a657fa03.html"/>
    <id>https://blog.yongwang.lu/post/a657fa03.html</id>
    <published>2023-01-02T05:05:21.000Z</published>
    <updated>2023-01-02T05:05:21.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><blockquote><p>作为接替Docker运行时的<a href="https://containerd.io/">Containerd</a>在早在Kubernetes1.7时就能直接与Kubelet集成使用，只是大部分时候我们因熟悉Docker，在部署集群时采用了默认的dockershim。在<code>V1.24</code>起的版本的kubelet就彻底移除了<code>dockershim</code>，改为默认使用<code>Containerd</code>了，当然也可以使用 <code>cri-dockerd</code> 适配器来将 <code>Docker Engine</code> 与 Kubernetes 集成。可以参考官方文档。</p></blockquote><p><img src="https://res.yongwang.lu/img/5da250511cf72797f2e2e992bd47240f-2.png" alt="图片"></p><h2 id="二、Containerd-常见命令操作"><a href="#二、Containerd-常见命令操作" class="headerlink" title="二、Containerd 常见命令操作"></a>二、Containerd 常见命令操作</h2><blockquote><p>更换Containerd后，以往我们常用的docker命令也不再使用，取而代之的分别是 <code>crictl</code> 和 <code>ctr</code> 两个命令客户端。</p></blockquote><ul><li><p><code>crictl</code> 是遵循CRI接口规范的一个命令行工具，通常用它来检查和管理<code>kubelet</code>节点上的容器运行时和镜像。</p></li><li><p><code>ctr</code> 是 <code>containerd</code> 的一个客户端工具。</p></li><li><p><code>ctr -v</code> 输出的是 <code>containerd</code> 的版本，<code>crictl -v</code> 输出的是当前 k8s 的版本，从结果显而易见你可以认为 <code>crictl</code> 是用于 <code>k8s</code> 的。</p></li><li><p>一般来说你某个主机安装了 k8s 后，命令行才会有 crictl 命令。而 ctr 是跟 k8s 无关的，你主机安装了 containerd 服务后就可以操作 ctr 命令。</p></li></ul><p>使用<code>crictl</code>命令之前，需要先配置<code>/etc/crictl.yaml</code>如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runtime-endpoint: unix:///run/containerd/containerd.sockimage-endpoint: unix:///run/containerd/containerd.socktimeout: 10debug: false</span><br></pre></td></tr></table></figure><p>也可以通过命令进行设置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl config runtime-endpoint unix:///run/containerd/containerd.sockcrictl config image-endpoint unix:///run/containerd/containerd.sock</span><br></pre></td></tr></table></figure><table><thead><tr><th>命令</th><th>docker</th><th>ctr（containerd）</th><th>crictl（kubernetes）</th></tr></thead><tbody><tr><td>查看运行的容器</td><td>docker ps</td><td>ctr task ls&#x2F;ctr container ls</td><td>crictl ps</td></tr><tr><td>查看镜像</td><td>docker images</td><td>ctr image ls</td><td>crictl images</td></tr><tr><td>查看容器日志</td><td>docker logs</td><td>无</td><td>crictl logs</td></tr><tr><td>查看容器数据信息</td><td>docker inspect</td><td>ctr container info</td><td>crictl inspect</td></tr><tr><td>查看容器资源</td><td>docker stats</td><td>无</td><td>crictl stats</td></tr><tr><td>启动&#x2F;关闭已有的容器</td><td>docker start&#x2F;stop</td><td>ctr task start&#x2F;kill</td><td>crictl start&#x2F;stop</td></tr><tr><td>运行一个新的容器</td><td>docker run</td><td>ctr run</td><td>无（最小单元为pod）</td></tr><tr><td>打标签</td><td>docker tag</td><td>ctr image tag</td><td>无</td></tr><tr><td>创建一个新的容器</td><td>docker create</td><td>ctr container create</td><td>crictl create</td></tr><tr><td>导入镜像</td><td>docker load</td><td>ctr image import</td><td>无</td></tr><tr><td>导出镜像</td><td>docker save</td><td>ctr image export</td><td>无</td></tr><tr><td>删除容器</td><td>docker rm</td><td>ctr container rm</td><td>crictl rm</td></tr><tr><td>删除镜像</td><td>docker rmi</td><td>ctr image rm</td><td>crictl rmi</td></tr><tr><td>拉取镜像</td><td>docker pull</td><td>ctr image pull</td><td>ctictl pull</td></tr><tr><td>推送镜像</td><td>docker push</td><td>ctr image push</td><td>无</td></tr><tr><td>登录或在容器内部执行命令</td><td>docker exec</td><td>无</td><td>crictl exec</td></tr><tr><td>清空不用的容器</td><td>docker image prune</td><td>无</td><td>crictl rmi –prune</td></tr></tbody></table><p>更多命令操作，可以直接在命令行输入命令查看帮助。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker --help</span><br><span class="line">ctr --help</span><br><span class="line">crictl --help</span><br></pre></td></tr></table></figure><p>由于Containerd也有namespaces的概念，对于上层编排系统的支持，<code>ctr</code> 客户端 主要区分了3个命名空间分别是<code>k8s.io</code>、<code>moby</code>和<code>default</code>，以上我们用<code>crictl</code>操作的均在<code>k8s.io</code>命名空间，使用<code>ctr</code> 看镜像列表就需要加上-n参数。crictl是只有一个<code>k8s.io</code>命名空间，但是没有-n参数。</p><blockquote><p>【温馨提示】ctr images pull 拉取的镜像默认放在<code>default</code>，而crictl pull 和 kubelet 默认拉取的镜像都在k8s.io命名空间下。所以通过<code>ctr</code>导入镜像的时候特别注意一点，最好指定命名空间。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 注意-n不能放在命令最后面，下面几行查看的镜像是一样的</span><br><span class="line">ctr -n=k8s.io image ls</span><br><span class="line">ctr -n k8s.io image ls</span><br><span class="line"></span><br><span class="line"># crictl 没有-n参数，操作都在`k8s.io`命名空间下。</span><br><span class="line">crictl image ls</span><br><span class="line">crictl images</span><br><span class="line"># crictl image list = ctr -n=k8s.io image list</span><br><span class="line"># crictl image ls = ctr -n=k8s.io image ls</span><br><span class="line"># crictl images = ctr -n=k8s.io image list</span><br><span class="line"># crictl images = ctr -n=k8s.io image ls</span><br><span class="line"></span><br><span class="line"># 使用ctr命令指定命名空间导入镜像</span><br><span class="line">ctr -n=k8s.io image import dashboard.tar</span><br><span class="line"></span><br><span class="line">#查看镜像，可以看到可以查询到了</span><br><span class="line">crictl images</span><br></pre></td></tr></table></figure><h2 id="三、container-客户端工具-nerdctl"><a href="#三、container-客户端工具-nerdctl" class="headerlink" title="三、container 客户端工具 nerdctl"></a>三、container 客户端工具 nerdctl</h2><p>推荐使用nerdctl，使用效果与docker命令的语法一致<br>github下载链接：<a href="https://github.com/containerd/nerdctl/releases">https://github.com/containerd/nerdctl/releases</a></p><ul><li><p>精简 (nerdctl-</p><p>-linux-amd64.tar.gz): 只包含nerdctl</p></li><li><p>完整 (nerdctl-full-</p><p>-linux-amd64.tar.gz): 包含 containerd, runc, and CNI等依赖</p></li></ul><blockquote><p><code>nerdctl</code> 的目标并不是单纯地复制 docker 的功能，它还实现了很多 docker 不具备的功能，例如延迟拉取镜像（lazy-pulling）、镜像加密（imgcrypt）等。具体看nerdctl。</p></blockquote><h3 id="1）安装-nerdctl（精简版）"><a href="#1）安装-nerdctl（精简版）" class="headerlink" title="1）安装 nerdctl（精简版）"></a>1）安装 nerdctl（精简版）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/containerd/nerdctl/releases/download/v0.22.2/nerdctl-0.22.2-linux-amd64.tar.gz</span><br><span class="line"><span class="comment"># 解压</span></span><br><span class="line">tar -xf nerdctl-0.22.2-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="built_in">ln</span> -s /opt/k8s/nerdctl/nerdctl /usr/local/bin/nerdctl</span><br></pre></td></tr></table></figure><h3 id="2）安装-nerdctl（完整版，这里不装）"><a href="#2）安装-nerdctl（完整版，这里不装）" class="headerlink" title="2）安装 nerdctl（完整版，这里不装）"></a>2）安装 nerdctl（完整版，这里不装）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/containerd/nerdctl/releases/download/v0.22.2/nerdctl-full-0.22.2-linux-amd64.tar.gz</span><br><span class="line">tar -xf nerdctl-full-0.16.0-linux-amd64.tar.gz -C /usr/local/</span><br><span class="line"></span><br><span class="line"><span class="built_in">cp</span> /usr/local/lib/systemd/system/*.service /etc/systemd/system/</span><br></pre></td></tr></table></figure><p>启动服务buildkit</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span>  buildkit containerd --now</span><br><span class="line">systemctl status buildkit containerd</span><br></pre></td></tr></table></figure><h3 id="3）安装-buildkit-支持构建镜像"><a href="#3）安装-buildkit-支持构建镜像" class="headerlink" title="3）安装 buildkit 支持构建镜像"></a>3）安装 buildkit 支持构建镜像</h3><p>buildkit GitHub地址：<a href="https://github.com/moby/buildkit">https://github.com/moby/buildkit</a></p><blockquote><p>使用<strong>精简版 nerdctl无法直接通过containerd构建镜像</strong>，需要与buildkit组全使用以实现镜像构建。当然你也可以安装上面的完整nerdctl；buildkit项目是Docker公司开源出来的一个构建工具包，支持OCI标准的镜像构建。它主要包含以下部分:</p></blockquote><ul><li><p>服务端buildkitd，当前支持runc和containerd作为worker，默认是runc；</p></li><li><p>客户端buildctl，负责解析Dockerfile，并向服务端buildkitd发出构建请求。</p></li></ul><p>buildkit是典型的<strong>C&#x2F;S架构</strong>，client和server可以不在一台服务器上。而nerdctl在构建镜像方面也可以作为buildkitd的客户端。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/moby/buildkit/releases</span></span><br><span class="line">wget https://github.com/moby/buildkit/releases/download/v0.10.4/buildkit-v0.10.4.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">tar -xf buildkit-v0.10.4.linux-amd64.tar.gz  -C /usr/local/</span><br></pre></td></tr></table></figure><p>配置buildkit的启动文件，可以从这里下载：<a href="https://github.com/moby/buildkit/tree/master/examples/systemd">https://github.com/moby/buildkit/tree/master/examples/systemd</a><br>buildkit需要配置两个文件</p><ul><li><code>/usr/lib/systemd/system/buildkit.socket</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /usr/lib/systemd/system/buildkit.socket &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">[Unit]</span></span><br><span class="line"><span class="string">Description=BuildKit</span></span><br><span class="line"><span class="string">Documentation=https://github.com/moby/buildkit</span></span><br><span class="line"><span class="string">[Socket]</span></span><br><span class="line"><span class="string">ListenStream=%t/buildkit/buildkitd.sock</span></span><br><span class="line"><span class="string">SocketMode=0660</span></span><br><span class="line"><span class="string">[Install]</span></span><br><span class="line"><span class="string">WantedBy=sockets.target</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><ul><li><code>/usr/lib/systemd/system/buildkit.service</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /usr/lib/systemd/system/buildkit.service &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">[Unit]</span></span><br><span class="line"><span class="string">Description=BuildKit</span></span><br><span class="line"><span class="string">Requires=buildkit.socket</span></span><br><span class="line"><span class="string">After=buildkit.socket</span></span><br><span class="line"><span class="string">Documentation=https://github.com/moby/buildkit</span></span><br><span class="line"><span class="string">[Service]</span></span><br><span class="line"><span class="string"># Replace runc builds with containerd builds  </span></span><br><span class="line"><span class="string">ExecStart=/usr/local/bin/buildkitd --addr fd://</span></span><br><span class="line"><span class="string">[Install]</span></span><br><span class="line"><span class="string">WantedBy=multi-user.target</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>启动buildkit</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl <span class="built_in">enable</span> buildkit --now</span><br></pre></td></tr></table></figure><h2 id="四、实战操作"><a href="#四、实战操作" class="headerlink" title="四、实战操作"></a>四、实战操作</h2><h3 id="1）修改containerd配置文件"><a href="#1）修改containerd配置文件" class="headerlink" title="1）修改containerd配置文件"></a>1）修改containerd配置文件</h3><p>可以参考我之前的文章：【云原生.大数据】镜像仓库Harbor对接MinIO对象存储</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">containerd config default &gt; /etc/containerd/config.toml</span><br></pre></td></tr></table></figure><p>配置如下：</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]</span></span><br><span class="line">      <span class="attr">config_path</span> = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">      <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.auths]</span></span><br><span class="line"></span><br><span class="line">      <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs]</span></span><br><span class="line">        <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;myharbor-minio.com&quot;.tls]</span></span><br><span class="line">          <span class="attr">insecure_skip_verify</span> = <span class="literal">true</span>  <span class="comment">#跳过认证</span></span><br><span class="line">          <span class="attr">ca_file</span> = <span class="string">&quot;/etc/containerd/myharbor-minio.com/ca.crt&quot;</span></span><br><span class="line">        <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;myharbor-minio.com&quot;.auth]</span></span><br><span class="line">          <span class="attr">username</span> = <span class="string">&quot;admin&quot;</span></span><br><span class="line">          <span class="attr">password</span> = <span class="string">&quot;Harbor12345&quot;</span></span><br><span class="line"></span><br><span class="line">      <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.headers]</span></span><br><span class="line"></span><br><span class="line">      <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]</span></span><br><span class="line">        <span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;myharbor-minio.com&quot;]</span></span><br><span class="line">          <span class="attr">endpoint</span> = [<span class="string">&quot;https://myharbor-minio.com&quot;</span>]</span><br></pre></td></tr></table></figure><p>重启containerd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重新加载配置</span></span><br><span class="line">systemctl daemon-reload</span><br><span class="line"><span class="comment">#重启containerd</span></span><br><span class="line">systemctl restart containerd</span><br></pre></td></tr></table></figure><blockquote><p>注意：这个配置文件是给<code>crictl</code>和<code>kubelet</code>使用，<code>ctr</code>是不可以用这个配置文件的，ctr 不使用 CRI，因此它不读取plugins.”io.containerd.grpc.v1.cri”配置。</p></blockquote><h3 id="2）ctr-拉取推送镜像"><a href="#2）ctr-拉取推送镜像" class="headerlink" title="2）ctr 拉取推送镜像"></a>2）ctr 拉取推送镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 推送镜像到harbor</span></span><br><span class="line">ctr --namespace=k8s.io images push myharbor-minio.com/bigdata/minio:2022.8.22-debian-11-r0 --skip-verify --user admin:Harbor12345</span><br><span class="line"></span><br><span class="line"><span class="comment"># --namespace=k8s.io 指定命名空间，不是必须，根据环境而定</span></span><br><span class="line"><span class="comment"># --skip-verify 跳过认证</span></span><br><span class="line"><span class="comment"># --user 指定harbor用户名及密码</span></span><br><span class="line"></span><br><span class="line">ctr  images pull --user admin:Harbor12345  --tlscacert=/etc/containerd/myharbor-minio.com/ca.crt myharbor-minio.com/bigdata/minio:2022.8.22-debian-11-r0</span><br></pre></td></tr></table></figure><p>不想-u user:password每次必须使用 ctr pull&#x2F;ctr push， 可以使用<code>nerdctl</code> 。</p><h3 id="3）镜像构建"><a href="#3）镜像构建" class="headerlink" title="3）镜像构建"></a>3）镜像构建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; Dockerfile &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">FROM nginx:alpine</span></span><br><span class="line"><span class="string">RUN echo &#x27;Hello Nerdctl From Containerd&#x27; &gt; /usr/share/nginx/html/index.html</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>然后在文件所在目录执行镜像构建命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不加-n指定命名空间，crictl看不到，kubelet也不能使用它，默认在default命名空间下</span></span><br><span class="line">nerdctl -n k8s.io build -t nginx:nerctl -f ./Dockerfile . </span><br><span class="line"><span class="comment">### 参数解释</span></span><br><span class="line"><span class="comment"># -t：指定镜像名称</span></span><br><span class="line"><span class="comment"># . ：当前目录Dockerfile</span></span><br><span class="line"><span class="comment"># -f：指定Dockerfile路径</span></span><br><span class="line"><span class="comment">#  --no-cache：不缓存</span></span><br></pre></td></tr></table></figure><h3 id="4）打标签-tag"><a href="#4）打标签-tag" class="headerlink" title="4）打标签 tag"></a>4）打标签 tag</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># crictl没有tag命令，只能使用nerdctl和ctr，必须指定命名空间，要不然kubelet无法使用。</span></span><br><span class="line">ctr -n k8s.io i tag </span><br><span class="line">nerdctl -n k8s.io tag nginx:nerctl myharbor-minio.com/bigdata/nginx:nerctl</span><br><span class="line"><span class="comment"># ctr -n k8s.io tag nginx:nerctl myharbor-minio.com/bigdata/nginx:nerctl</span></span><br><span class="line"><span class="comment"># 查看镜像</span></span><br><span class="line">nerdctl  -n k8s.io  images myharbor-minio.com/bigdata/nginx:nerctl</span><br></pre></td></tr></table></figure><h3 id="5）将镜像推送到-Harbor"><a href="#5）将镜像推送到-Harbor" class="headerlink" title="5）将镜像推送到 Harbor"></a>5）将镜像推送到 Harbor</h3><p>第一种情况：<code>http</code>方式，配置如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下两个哪个都可以</span></span><br><span class="line"><span class="comment"># mkdir -p /etc/docker/certs.d/myharbor-minio.com:443</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /etc/containerd/certs.d/myharbor-minio.com:443</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> &gt; /etc/containerd/certs.d/myharbor-minio.com\:443/hosts.toml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">server = &quot;https://docker.io&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[host.&quot;http://myharbor-minio.com:80&quot;]</span></span><br><span class="line"><span class="string">  capabilities = [&quot;pull&quot;, &quot;resolve&quot;,&quot;push&quot;]</span></span><br><span class="line"><span class="string">  #skip_verify = true</span></span><br><span class="line"><span class="string">  #ca = &quot;ca.crt&quot;   #相对路径</span></span><br><span class="line"><span class="string">  #ca = &quot;/opt/auth/ca.crt&quot;  #绝对路径</span></span><br><span class="line"><span class="string">  #ca = [&quot;/opt/auth/ca.crt&quot;]</span></span><br><span class="line"><span class="string">  #ca = [&quot;ca.crt&quot;]</span></span><br><span class="line"><span class="string">  #client = [[&quot;/opt/auth/nginx.cclinux.cn.crt&quot;, &quot;/opt/auth/nginx.cclinux.cn.key&quot;]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>第一种情况：<code>https</code>方式，配置如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下两个哪个都可以</span></span><br><span class="line"><span class="comment"># mkdir -p /etc/docker/certs.d/myharbor-minio.com:443</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /etc/containerd/certs.d/myharbor-minio.com:443</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> &gt; /etc/containerd/certs.d/myharbor-minio.com\:443/hosts.toml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">server = &quot;https://docker.io&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[host.&quot;https://myharbor-minio.com:443&quot;]</span></span><br><span class="line"><span class="string">  capabilities = [&quot;pull&quot;, &quot;resolve&quot;,&quot;push&quot;]</span></span><br><span class="line"><span class="string">  skip_verify = true</span></span><br><span class="line"><span class="string">  #ca = &quot;ca.crt&quot;   #相对路径</span></span><br><span class="line"><span class="string">  #ca = &quot;/opt/auth/ca.crt&quot;  #绝对路径</span></span><br><span class="line"><span class="string">  #ca = [&quot;/opt/auth/ca.crt&quot;]</span></span><br><span class="line"><span class="string">  ca = [&quot;/etc/containerd/myharbor-minio.com/ca.crt&quot;]</span></span><br><span class="line"><span class="string">  #client = [[&quot;/opt/auth/nginx.cclinux.cn.crt&quot;, &quot;/opt/auth/nginx.cclinux.cn.key&quot;]]</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>通过 nerdctl 登录 harbor</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> Harbor12345 | nerdctl login --username <span class="string">&quot;admin&quot;</span> --password-stdin  myharbor-minio.com:443</span><br><span class="line"></span><br><span class="line">$ nerdctl login --username <span class="string">&quot;admin&quot;</span> --password Harbor12345 myharbor-minio.com:443</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登出</span></span><br><span class="line">$ nerdctl <span class="built_in">logout</span></span><br></pre></td></tr></table></figure><p>开始将镜像推送到harbor</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 推送到Harbor</span></span><br><span class="line"><span class="comment"># --insecure-registry        skips verifying HTTPS certs, and allows falling back to plain HTTP</span></span><br><span class="line">nerdctl --insecure-registry --namespace=k8s.io push myharbor-minio.com/bigdata/nginx:nerctl</span><br><span class="line"><span class="comment"># ctr --namespace=k8s.io images push myharbor-minio.com/bigdata/nginx:nerctl --skip-verify --user admin:Harbor12345</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --namespace=k8s.io 指定命名空间，跟-n一样，不是必须，根据环境而定</span></span><br><span class="line"><span class="comment"># --skip-verify 跳过认证</span></span><br><span class="line"><span class="comment"># --user 指定harbor用户名及密码</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">containerd</summary>
    
    
    
    <category term="containerd" scheme="https://blog.yongwang.lu/categories/containerd/"/>
    
    
    <category term="containerd" scheme="https://blog.yongwang.lu/tags/containerd/"/>
    
  </entry>
  
  <entry>
    <title>Containerd 入门到尝试</title>
    <link href="https://blog.yongwang.lu/post/964191ef.html"/>
    <id>https://blog.yongwang.lu/post/964191ef.html</id>
    <published>2023-01-02T03:45:41.000Z</published>
    <updated>2023-01-02T03:45:41.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Kubernetes v1.24 之前的版本直接集成了 Docker Engine 的一个组件，名为 <strong>dockershim</strong> [用于调用Docker]。 这种特殊的直接整合不再是 Kubernetes 的一部分 （这次删除被作为 v1.20 发行版本的一部分宣布）。 这意味Kubernetes从版本1.24开始就弃用Docker作为容器运行时，取而代之的是更加轻量级的Containerd。</p></blockquote><p><img src="https://res.yongwang.lu/img/5da250511cf72797f2e2e992bd47240f-2.png" alt="K8s-CRI流程更改"></p><blockquote><p>containerd可用作 Linux 和 Windows 的守护进程。它管理其主机系统的完整容器生命周期，从图像传输和存储到容器执行和监督，再到低级存储到网络附件等等。</p></blockquote><h2 id="一、Containerd介绍"><a href="#一、Containerd介绍" class="headerlink" title="一、Containerd介绍"></a>一、Containerd介绍</h2><h3 id="1、Containerd的由来"><a href="#1、Containerd的由来" class="headerlink" title="1、Containerd的由来"></a>1、Containerd的由来</h3><p>【Docker名噪一时，捐出runC】2013年docker公司在推出docker产品后，由于其对全球技术产生了一定的影响力，Google公司明显感觉到自己公司内部所使用的Brog系统江湖地位受到的威胁，希望Docker公司能够与自己联合打造一款开源的容器运行时作为Docker核心依赖，但Docker公司拒绝了；接着Google公司联合RedHat、IBM等公司说服Docker公司把其容器核心技术<code>libcontainer</code>捐给OCI（Open Container Intiative），并更名为<code>runC</code>。</p><p>【CNCF成立，kubernetes被迫开源】为了进一步遏制Docker在未来技术市场影响力，避免在容器市场上Docker一家独大，Google公司带领RedHat、IBM等成立了CNCF（Cloud Native Computing Fundation）基金会，即<strong>云原生计算基金会</strong>。CNCF的目标很明确，既然在容器应用领域无法与Docker相抗衡，那就做Google更有经验的技术市场——大规模容器编排应用场景。Google公司把自己内部使用的Brog系统开源——Kubernetes，也就是我们今天所说的云原生技术生态。</p><p>【Docker妥协，贡献出Containerd】2016年Docker公司推出了Docker Swarm，意在一统Docker生态，让Docker既可以实现容器应用管理，也可以实现大规模容器编排，经过近1年左右时间的市场验证后，发现在容器编排方面无法独立抗衡kubernetes，所以Docker公司于2017年正式宣布原生支持Kubernetes。至此，Docker在大规模容器编排应用市场败下阵来，但是Docker依然不甘心失败，把Docker核心依赖Containerd捐给了CNCF，依此说明Docker依旧是一个PaaS平台。</p><p>【k8s宣布不支持Docker，Containerd成为CRI主角】2020年CNCF基金会宣布Kubernetes 1.20版本将不再仅支持Docker容器管理工具，此事的起因主要也与Docker捐给CNCF基金会的Containerd有关，早期为了实现Kubernetes能够使用Docker实现容器管理，专门在Kubernetes组件中集成一个<code>shim</code>技术，用来将Kubernetes <strong>容器运行时接口</strong>（CRI，Container Runntime Interface）调用翻译成Docker的API，这样就可以很好地使用Docker了。但是随着Kubernetes在全球技术市场的广泛应用，有更多的容器管理工具的出现，它们都想能够借助于Kubernetes被用户所使用，所以就提出标准化容器运行时接口，只要适配了这个接口就可以集成到Kubernetes生态当中，所以Kubernetes取消了对shim的维护，并且由于Containerd技术的成功，可以实现无缝对接Kubernetes，所以接下来Kubernetes容器运行时的主角是Containerd。</p><h3 id="2、Containerd概念"><a href="#2、Containerd概念" class="headerlink" title="2、Containerd概念"></a>2、Containerd概念</h3><p>早在2016年3月，Docker 1.11的Docker Engine里就包含了containerd，而现在则是把containerd从Docker Engine里彻底剥离出来，作为一个独立的开源项目独立发展，目标是提供一个更加开放、稳定的容器运行基础设施。</p><p>和原先包含在Docker Engine里containerd相比，独立的containerd将具有更多的功能，可以涵盖整个容器运行时管理的所有需求。另外独立之后containerd的特性演进可以和Docker Engine分开，专注容器运行时管理，可以更稳定。</p><p>Containerd是一个工业标准的容器运行时，重点是它简洁，健壮，便携，在Linux和window上可以作为一个守护进程运行，它可以管理主机系统上容器的完整的生命周期：镜像传输和存储，容器的执行和监控，低级别的存储和网络。</p><p><strong>每个containerd只负责一台机器</strong>，Pull镜像，对容器的操作（启动、停止等），网络，存储都是由containerd完成。<strong>具体运行容器由runC负责</strong>，实际上只要是符合OCI规范的容器都可以支持。</p><p>Containerd和docker不同，containerd重点是集成在大规模的系统中，例如kubernetes、Swarm、Mesos等【对于容器编排服务来说，运行时只需要使用containerd+runC，更加轻量，容易管理。】。<strong>Containerd 被设计成嵌入到一个更大的系统中，而不是直接由开发人员或终端用户使用</strong>。</p><p>Containerd的特点：</p><ul><li>简洁的基于 gRPC 的 API 和 client library。</li><li>完整的 OCI 支持(runtime 和 image spec)。</li><li>同时具备稳定性和高性能的定义良好的容器核心功能。</li><li>一个解耦的系统(让 image、filesystem、runtime 解耦合)，实现插件式的扩展和重用。</li></ul><p>Containerd的作用：</p><ul><li>管理容器的生命周期(从创建容器到销毁容器)。</li><li>拉取&#x2F;推送容器镜像。</li><li>存储管理(管理镜像及容器数据的存储)。</li><li>调用 runC 运行容器(与 runC 等容器运行时交互)。</li><li>管理容器网络接口及网络。</li></ul><p>使用 bucketbench 对 Docker、crio 和 Containerd 的性能测试结果，包括启动、停止和删除容器，以比较它们所耗的时间，可以发现Containerd 在各个方面都表现良好，总体性能优于 Docker 和 crio。</p><h3 id="3、Containerd架构"><a href="#3、Containerd架构" class="headerlink" title="3、Containerd架构"></a>3、Containerd架构</h3><p>Containerd 采用标准的 C&#x2F;S 架构：服务端通过 GRPC 协议提供稳定的 API；客户端通过调用服务端的 API 进行高级的操作。</p><p>为了实现解耦，Containerd 将不同的职责划分给不同的组件，每个组件就相当于一个子系统（subsystem）。连接不同子系统的组件被称为模块。</p><p>Containerd 被分为三个大块： Storage 、 Metadata 和 Runtime。</p><p>Containerd 两大子系统为：<br>Bundle : 在 Containerd 中，Bundle 包含了配置、元数据和根文件系统数据，你可以理解为 容器的文件系统。而 Bundle 子系统允许用户从镜像中提取和打包 Bundles。<br>Runtime : Runtime 子系统用来执行 Bundles，比如创建容器。其中，每一个子系统的行为都由一个或多个模块协作完成（架构图中的 Core 部分）。</p><p><img src="https://containerd.io/img/architecture.png"></p><h3 id="4、几个概念区分"><a href="#4、几个概念区分" class="headerlink" title="4、几个概念区分"></a>4、几个概念区分</h3><p><code>containerd</code> 是一个高级容器运行时，又名容器管理器。简单来说，它是一个守护进程，在单个主机上管理完整的容器生命周期：创建、启动、停止容器、拉取和存储镜像、配置挂载、网络等。</p><p><code>ctr</code> 是作为 containerd 项目的一部分提供的命令行客户端。该<code>ctr</code>界面 与 Docker CLI不兼容，乍一看，可能看起来不太用户友好。因为它的主要受众是测试守护进程的容器开发人员。<strong>ctr + containerd比docker + dockerd更接近实际的容器。</strong></p><p><code>nerdctl</code> 是一个相对较新的containerd命令行客户端。与ctr不同，nerdctl的目标是用户友好和docker兼容。在某种程度上，<strong>nerdctl + containerd可以无缝地替代docker + dockerd。</strong></p><p><code>crictl</code> 是一个命令行客户端，用于 [kubernetes] CRI兼容的容器运行时。引入 Kubernetes 容器运行时接口 (CRI)以使 Kubernetes 容器运行时不可知。Kubernetes节点代理kubelet实现了 CRI客户端 API，可以使用任何实现 CRI 服务器 API的容器运行时来管理其节点上的容器和 Pod。</p><h2 id="二、Containerd安装"><a href="#二、Containerd安装" class="headerlink" title="二、Containerd安装"></a>二、Containerd安装</h2><p>containerd官网：<a href="https://containerd.io/" title="https://containerd.io/">https://containerd.io/</a></p><p>containerd官方安装步骤：<a href="https://github.com/containerd/containerd/blob/main/docs/getting-started.md" title="https://github.com/containerd/containerd/blob/main/docs/getting-started.md">https://github.com/containerd/containerd/blob/main/docs/getting-started.md</a></p><h3 id="step1：安装Containerd"><a href="#step1：安装Containerd" class="headerlink" title="step1：安装Containerd"></a>step1：安装Containerd</h3><p>因后续版本Kubernetes 对containerd运行时有版本要求这里安装1.6.x版本的. Ubuntu源是1.5.9版本. 这里 直接下载deb安装.&#x20;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载Deb</span></span><br><span class="line"><span class="comment"># Docker官方源 https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/</span></span><br><span class="line"><span class="comment"># Tencent源 https://mirrors.cloud.tencent.com/docker-ce/linux/ubuntu/dists/jammy/pool/stable/amd64/</span></span><br><span class="line">$ wget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/containerd.io_1.6.14-1_amd64.deb</span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">$ dpkg -i containerd.io_1.6.14-1_amd64.deb</span><br></pre></td></tr></table></figure><p>将containerd服务设置卫开机启动。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl enable --now containerd</span><br><span class="line"># 验证</span><br><span class="line">$ ctr version</span><br><span class="line">Client:</span><br><span class="line">  Version:  v1.6.14</span><br><span class="line">  Revision: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6</span><br><span class="line">  Go version: go1.17.13</span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line">  Version:  v1.6.14</span><br><span class="line">  Revision: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6</span><br><span class="line">  UUID: 87222662-9eb1-44cb-998a-689c9efce638</span><br></pre></td></tr></table></figure><p>至此，containerd安装完成。</p><h3 id="step2：修改配置"><a href="#step2：修改配置" class="headerlink" title="step2：修改配置"></a>step2：修改配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line"><span class="built_in">mkdir</span> /etc/containerd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成默认配置文件</span></span><br><span class="line">containerd config default | \</span><br><span class="line"><span class="built_in">tee</span> /etc/containerd/config.toml &gt;/dev/null</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改配置文件 加速</span></span><br><span class="line">sed -i \</span><br><span class="line">-e <span class="string">&#x27;/sandbox_image/s?k8s.gcr.io?registry.aliyuncs.com/google_containers?&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/SystemdCgroup/s?false?true?&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/registry.mirrors/a\        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/registry.mirrors/a\          endpoint = [&quot;https://docker.nju.edu.cn/&quot;]&#x27;</span> /etc/containerd/config.toml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 服务重启</span></span><br><span class="line">systemctl restart containerd</span><br></pre></td></tr></table></figure><h3 id="step3：安装cni插件和cni工具包"><a href="#step3：安装cni插件和cni工具包" class="headerlink" title="step3：安装cni插件和cni工具包"></a>step3：安装cni插件和cni工具包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">mkdir</span> -p /home/cni-plugins</span><br><span class="line">$ tar Cxzvf /home/cni-plugins cni-plugins-linux-amd64-v1.0.0.tgz</span><br><span class="line"><span class="comment">## 该二进制文件是静态构建的，可以在任何 Linux 发行版上运行</span></span><br><span class="line"><span class="comment"># cni工具1.1.2兼容cni插件1.0.0</span></span><br><span class="line">$ <span class="built_in">mkdir</span> -p /home/cni-tools</span><br><span class="line">$ tar Cxzvf /home/cni-tools cni-1.1.2.tar.gz</span><br></pre></td></tr></table></figure><p>完成。</p><h2 id="三、ctr命令行操作"><a href="#三、ctr命令行操作" class="headerlink" title="三、ctr命令行操作"></a>三、ctr命令行操作</h2><p><strong>ctr是作为 containerd 项目的一部分提供的命令行客户端。</strong></p><p>该ctr界面 [显然] 与 Docker CLI不兼容，乍一看，可能看起来不太用户友好。显然，它的主要受众是测试守护进程的容器开发人员。但是，由于它最接近实际的 containerd API，因此它可以作为一种很好的探索手段——通过检查可用命令，可以大致了解 containerd可以做什么和不可以做什么。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">$ ctr --<span class="built_in">help</span></span><br><span class="line">NAME:</span><br><span class="line">   ctr - </span><br><span class="line">        __</span><br><span class="line">  _____/ /______</span><br><span class="line"> / ___/ __/ ___/</span><br><span class="line">/ /__/ /_/ /</span><br><span class="line">\___/\__/_/</span><br><span class="line"></span><br><span class="line">containerd CLI</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">USAGE:</span><br><span class="line">   ctr [global options] <span class="built_in">command</span> [<span class="built_in">command</span> options] [arguments...]</span><br><span class="line"></span><br><span class="line">VERSION:</span><br><span class="line">   v1.6.8</span><br><span class="line"></span><br><span class="line">DESCRIPTION:</span><br><span class="line">   </span><br><span class="line">ctr is an unsupported debug and administrative client <span class="keyword">for</span> interacting</span><br><span class="line">with the containerd daemon. Because it is unsupported, the commands,</span><br><span class="line">options, and operations are not guaranteed to be backward compatible or</span><br><span class="line">stable from release to release of the containerd project.</span><br><span class="line"></span><br><span class="line">COMMANDS:</span><br><span class="line">   plugins, plugin            provides information about containerd plugins</span><br><span class="line">   version                    <span class="built_in">print</span> the client and server versions</span><br><span class="line">   containers, c, container   manage containers</span><br><span class="line">   content                    manage content</span><br><span class="line">   events, event              display containerd events</span><br><span class="line">   images, image, i           manage images</span><br><span class="line">   leases                     manage leases</span><br><span class="line">   namespaces, namespace, ns  manage namespaces</span><br><span class="line">   pprof                      provide golang pprof outputs <span class="keyword">for</span> containerd</span><br><span class="line">   run                        run a container</span><br><span class="line">   snapshots, snapshot        manage snapshots</span><br><span class="line">   tasks, t, task             manage tasks</span><br><span class="line">   install                    install a new package</span><br><span class="line">   oci                        OCI tools</span><br><span class="line">   shim                       interact with a shim directly</span><br><span class="line">   <span class="built_in">help</span>, h                    Shows a list of commands or <span class="built_in">help</span> <span class="keyword">for</span> one <span class="built_in">command</span></span><br><span class="line"></span><br><span class="line">GLOBAL OPTIONS:</span><br><span class="line">   --debug                      <span class="built_in">enable</span> debug output <span class="keyword">in</span> logs</span><br><span class="line">   --address value, -a value    address <span class="keyword">for</span> containerd<span class="string">&#x27;s GRPC server (default: &quot;/run/containerd/containerd.sock&quot;) [$CONTAINERD_ADDRESS]</span></span><br><span class="line"><span class="string">   --timeout value              total timeout for ctr commands (default: 0s)</span></span><br><span class="line"><span class="string">   --connect-timeout value      timeout for connecting to containerd (default: 0s)</span></span><br><span class="line"><span class="string">   --namespace value, -n value  namespace to use with commands (default: &quot;default&quot;) [$CONTAINERD_NAMESPACE]</span></span><br><span class="line"><span class="string">   --help, -h                   show help</span></span><br><span class="line"><span class="string">   --version, -v                print the version</span></span><br></pre></td></tr></table></figure><h3 id="1、Containerd镜像管理"><a href="#1、Containerd镜像管理" class="headerlink" title="1、Containerd镜像管理"></a>1、Containerd镜像管理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拉取镜像  拉取镜像，完全合格的参考似乎是必需的，所以不能忽略镜像仓库或标签部分。</span></span><br><span class="line">$ ctr images pull --all-platforms docker.io/library/nginx:alpine</span><br><span class="line"><span class="comment"># --all-platforms  指定所有平台镜像  </span></span><br><span class="line"><span class="comment"># --platform       指定系统平台</span></span><br><span class="line">$ ctr images pull --platform linux/amd64 docker.io/library/nginx:latest</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看镜像</span></span><br><span class="line">$ ctr images <span class="built_in">ls</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂载镜像</span></span><br><span class="line">$ ctr images mount docker.io/library/nginx:alpine /mnt</span><br><span class="line">$ <span class="built_in">ls</span> /mnt</span><br><span class="line">$ umount /mnt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像导出，并保存为nginx.img文件</span></span><br><span class="line">$ ctr i <span class="built_in">export</span> --all-platforms nginx.img docker.io/library/nginx:alpine</span><br><span class="line">$ <span class="built_in">ls</span> -l</span><br><span class="line">-rw-r--r-- 1 root root 70138368 Sep  2 16:33 nginx.img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像导入</span></span><br><span class="line">$ ctr images import nginx.img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像删除</span></span><br><span class="line">$ ctr image <span class="built_in">rm</span> docker.io/library/nginx:alpine</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像打tag</span></span><br><span class="line">$ ctr images tag docker.io/library/nginx:alpine nginx:alpine</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像对比</span></span><br><span class="line">$ ctr images check</span><br></pre></td></tr></table></figure><h3 id="2、Containerd容器管理"><a href="#2、Containerd容器管理" class="headerlink" title="2、Containerd容器管理"></a>2、Containerd容器管理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建【静态】容器</span></span><br><span class="line"><span class="comment"># 该命令创建容器后，容器并没有处于运行状态，其只是一个静态的容器。这个container对象只是包含了运行一个容器所需的资源及配置的数据结构，例如： namespaces、rootfs 和容器的配置都已经初始化成功了，只是用户进程(本案例为nginx)还没有启动。需要使用`ctr tasks`命令才能获取一个动态容器。</span></span><br><span class="line">$ ctr container create docker.io/library/nginx:alpine testnginx1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动【动态】容器  创建容器和任务子命令分离</span></span><br><span class="line">$ ctr task start -d testnginx1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接运行一个动态容器【容器的IP就是宿主机的IP】</span></span><br><span class="line"><span class="comment"># ctr运行命令实际上是ctr容器创建+ ctr任务启动的快捷方式</span></span><br><span class="line">$ ctr run -d --net-host docker.io/library/nginx:alpine testnginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看【静态】容器</span></span><br><span class="line">$ ctr container <span class="built_in">ls</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看【动态】容器</span></span><br><span class="line">$ ctr task ps testnginx1</span><br><span class="line">PID      INFO</span><br><span class="line">64574    -</span><br><span class="line">64613    -</span><br><span class="line">64614    -</span><br><span class="line">64615    -</span><br><span class="line">64616    -</span><br><span class="line"><span class="comment"># 物理机上查看</span></span><br><span class="line">$ ps -ef | grep nginx</span><br><span class="line">root      64574  64553  0 16:41 ?        00:00:00 nginx: master process nginx -g daemon off;</span><br><span class="line">101       64613  64574  0 16:41 ?        00:00:00 nginx: worker process</span><br><span class="line">101       64614  64574  0 16:41 ?        00:00:00 nginx: worker process</span><br><span class="line">101       64615  64574  0 16:41 ?        00:00:00 nginx: worker process</span><br><span class="line">101       64616  64574  0 16:41 ?        00:00:00 nginx: worker process</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">$ ctr task <span class="built_in">exec</span> --exec-id 1 testnginx2 sh</span><br><span class="line"><span class="comment"># 查看网卡</span></span><br><span class="line">ifconfig  </span><br><span class="line">ens33     Link encap:Ethernet  HWaddr 00:0C:29:32:CC:B0  </span><br><span class="line">          inet addr:192.168.168.201  Bcast:192.168.168.255  Mask:255.255.255.0</span><br><span class="line">          inet6 addr: fe80::63d5:da66:d3db:fa1c/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:476940 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:595381 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:175687016 (167.5 MiB)  TX bytes:149770479 (142.8 MiB)</span><br><span class="line">lo        Link encap:Local Loopback  </span><br><span class="line">          inet addr:127.0.0.1  Mask:255.0.0.0</span><br><span class="line">          inet6 addr: ::1/128 Scope:Host</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:65536  Metric:1</span><br><span class="line">          RX packets:0 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)</span><br><span class="line"><span class="comment"># 为容器中运行的网站添加网站文件</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;hello nginx2&quot;</span> &gt; /usr/share/nginx/html/index.html</span><br><span class="line"><span class="comment"># 在容器里访问</span></span><br><span class="line">curl 127.0.0.1</span><br><span class="line"><span class="comment"># 退出容器          </span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在宿主机上访问</span></span><br><span class="line">$ curl 192.168.168.201</span><br><span class="line">hello nginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暂停容器</span></span><br><span class="line">$ ctr task pause testnginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复容器</span></span><br><span class="line">$ ctr task resume testnginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止容器</span></span><br><span class="line">$ ctr task <span class="built_in">kill</span> testnginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除动态容器【必须先停止】</span></span><br><span class="line">$ ctr task delete testnginx2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除静态容器</span></span><br><span class="line">$ ctr container delete testnginx2</span><br></pre></td></tr></table></figure><h3 id="3、Containerd命名空间管理"><a href="#3、Containerd命名空间管理" class="headerlink" title="3、Containerd命名空间管理"></a>3、Containerd命名空间管理</h3><p><strong>containerd中namespace的作用为：隔离运行的容器，可以实现运行多个容器。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拉取镜像</span></span><br><span class="line">$ ctr namespace --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建命名空间</span></span><br><span class="line">$ ctr ns create testns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看命名空间</span></span><br><span class="line">$ ctr ns <span class="built_in">ls</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在命名空间中进行镜像、容器等相关操作</span></span><br><span class="line">$ ctr -n testns images pull docker.io/library/nginx:latest</span><br><span class="line">$ ctr -n testns images <span class="built_in">ls</span></span><br><span class="line">$ ctr -n testns container create docker.io/library/nginx:latest</span><br><span class="line">$ ctr -n testns container <span class="built_in">ls</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除命名空间</span></span><br><span class="line">$ ctr ns <span class="built_in">rm</span> testns</span><br></pre></td></tr></table></figure><h3 id="4、Containerd网络管理"><a href="#4、Containerd网络管理" class="headerlink" title="4、Containerd网络管理"></a>4、Containerd网络管理</h3><p><strong>默认Containerd管理的容器仅有lo网络，无法访问容器之外的网络，可以为其添加网络插件，使用容器可以连接外网，CNI（Container Network Interface）</strong></p><p>cni插件和cni工具包已经在前面第二章节已经部署完毕。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cni插件目录</span></span><br><span class="line">$ <span class="built_in">cd</span> /home/cni-plugins</span><br><span class="line"><span class="comment"># cni工具包目录</span></span><br><span class="line">$ <span class="built_in">cd</span> /home/cni-tools/cni-1.1.2</span><br><span class="line"><span class="comment"># cni工具1.1.2兼容cni插件1.0.0</span></span><br></pre></td></tr></table></figure><ul><li><p>下面先开始准备容器网络配置文件，用于为容器提供网关、IP地址等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/cni/net.d/10-mynet.conf</span><br><span class="line">$ vim /etc/cni/net.d/99-loopback.conf</span><br><span class="line">$ <span class="built_in">cat</span> /etc/cni/net.d/10-mynet.conf</span><br><span class="line">&#123;</span><br><span class="line">        <span class="string">&quot;cniVersion&quot;</span>: <span class="string">&quot;1.0.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;mynet&quot;</span>,</span><br><span class="line">        <span class="string">&quot;type&quot;</span>: <span class="string">&quot;bridge&quot;</span>,</span><br><span class="line">        <span class="string">&quot;bridge&quot;</span>: <span class="string">&quot;cni0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;isGateway&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">        <span class="string">&quot;ipMasq&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">        <span class="string">&quot;ipam&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;host-local&quot;</span>,</span><br><span class="line">            <span class="string">&quot;subnet&quot;</span>: <span class="string">&quot;10.66.0.0/16&quot;</span>,</span><br><span class="line">            <span class="string">&quot;routes&quot;</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;dst&quot;</span>: <span class="string">&quot;0.0.0.0/0&quot;</span></span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">$ <span class="built_in">cat</span> /etc/cni/net.d/99-loopback.conf</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;cniVerion&quot;</span>: <span class="string">&quot;1.0.0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;lo&quot;</span>,</span><br><span class="line">    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;loopback&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>生成cni网络</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取epel源</span></span><br><span class="line">$ wget -O /etc/yum.repos.d/epel.repo [http://mirrors.aliyun.com/repo/epel-7.repo](http://mirrors.aliyun.com/repo/epel-7.repo <span class="string">&quot;http://mirrors.aliyun.com/repo/epel-7.repo&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装jq【jq是一个&#x27;出色&#x27;的&#x27;针对--&gt;JSON处理器&#x27;的命令行】</span></span><br><span class="line">$ yum -y install jq</span><br><span class="line"><span class="comment"># 进入cni工具目录</span></span><br><span class="line">$ <span class="built_in">cd</span> /home/cni-tools/cni-1.1.2</span><br><span class="line"><span class="comment"># 执行脚本文件，基于/etc/cni/net.d/目录中的 \*.conf配置文件生成容器网络</span></span><br><span class="line"><span class="comment"># CNI\_PATH是cni插件安装的目录</span></span><br><span class="line">$ CNI\_PATH=/home/cni-plugins ./priv-net-run.sh <span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line"><span class="comment"># cni插件和cni工具包版本要兼容，否则可能会报错如下：</span></span><br><span class="line">mynet : error executing ADD: &#123;</span><br><span class="line">    <span class="string">&quot;code&quot;</span>: 1,</span><br><span class="line">    <span class="string">&quot;msg&quot;</span>: <span class="string">&quot;incompatible CNI versions&quot;</span>,</span><br><span class="line">    <span class="string">&quot;details&quot;</span>: <span class="string">&quot;config is &quot;</span>1.0.0<span class="string">&quot;, plugin supports \[&quot;</span>0.1.0<span class="string">&quot; &quot;</span>0.2.0<span class="string">&quot; &quot;</span>0.3.0<span class="string">&quot; &quot;</span>0.3.1<span class="string">&quot; &quot;</span>0.4.0<span class="string">&quot;]&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装成功后，在宿主机上查看是否生成容器网络名为cni0的网桥</span></span><br><span class="line">$ ip a s</span><br><span class="line">    1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">    valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">    valid_lft forever preferred_lft forever</span><br><span class="line">    ...</span><br><span class="line">    7: cni0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/ether 6a:32:bc:eb:0f:23 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.66.0.1/16 brd 10.66.255.255 scope global cni0</span><br><span class="line">    valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::6832:bcff:feeb:f23/64 scope <span class="built_in">link</span></span><br><span class="line">    valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></li><li><p>创建容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ ctr images pull docker.io/library/busybox:latest</span><br><span class="line">$ ctr run -d docker.io/library/busybox:latest busybox</span><br><span class="line">$ ctr tasks <span class="built_in">exec</span> --exec-id<span class="variable">$RANDOM</span> -t busybox sh ip a s</span><br><span class="line">    1: lo: \&lt;LOOPBACK,UP,LOWER\_UP&gt; mtu 65536 qdisc noqueue qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    2: tunl0\@NONE:  mtu 1480 qdisc noop qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line"><span class="comment">#获取容器进程ID及其网络命名空间</span></span><br><span class="line">$ pid= $(ctr tasks <span class="built_in">ls</span> | grep busybox | awk <span class="string">&#x27;&#123;print $ 2&#125;&#x27;</span>) &amp;&amp; <span class="built_in">echo</span> <span class="variable">$pid</span></span><br><span class="line">39287</span><br><span class="line">$ netnspath=/proc/ <span class="variable">$pid</span>/ns/net &amp;&amp; <span class="built_in">echo</span> <span class="variable">$netnspath</span>/proc/39287/ns/net</span><br><span class="line"><span class="comment"># 进入目录为指定容器添加网络配置</span></span><br><span class="line"><span class="variable">$cd</span> /home/cni-tools/cni-1.1.2/scripts/$ CNI\_PATH=/home/cni-plugins ./exec-plugins.sh add $ pid  <span class="variable">$netnspath</span></span><br></pre></td></tr></table></figure></li><li><p>验证容器网络与宿主机网络的互访功能</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证</span></span><br><span class="line">    <span class="comment"># 进入容器确认是否添加网卡信息</span></span><br><span class="line">    <span class="variable">$ctr</span> tasks <span class="built_in">exec</span> --exec-id<span class="variable">$RANDOM</span> -t busybox sh</span><br><span class="line">    $ ip a s</span><br><span class="line">    1: lo: \&lt;LOOPBACK,UP,LOWER\_UP&gt; mtu 65536 qdisc noqueue qlen 1000</span><br><span class="line">    <span class="built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    2: tunl0\@NONE:&amp;<span class="comment">#x20;</span></span><br><span class="line"></span><br><span class="line">    &amp;<span class="comment">#x20;mtu 1480 qdisc noop qlen 1000</span></span><br><span class="line">    <span class="built_in">link</span>/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">    3: eth0\@if9: \&lt;BROADCAST,MULTICAST,UP,LOWER\_UP,M-DOWN&gt; mtu 1500 qdisc noqueue</span><br><span class="line">    <span class="built_in">link</span>/ether 5a:36:90:c1:33:b1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.66.0.4/16 brd 10.66.255.255 scope global eth0</span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    inet6 fe80::5836:90ff:fec1:33b1/64 scope <span class="built_in">link</span></span><br><span class="line">    valid\_lft forever preferred\_lft forever</span><br><span class="line">    <span class="comment"># 在容器中ping容器宿主机IP地址</span></span><br><span class="line">    $ ping -c 2 192.168.168.201</span><br><span class="line">    PING 192.168.168.201 (192.168.168.201): 56 data bytes</span><br><span class="line">    64 bytes from 192.168.168.201: <span class="built_in">seq</span>=0 ttl=64 time=0.067 ms</span><br><span class="line">    64 bytes from 192.168.168.201: <span class="built_in">seq</span>=1 ttl=64 time=0.070 ms</span><br><span class="line"></span><br><span class="line">    \--- 192.168.168.201 ping statistics ---</span><br><span class="line">    2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">    round-trip min/avg/max = 0.067/0.068/0.070 ms</span><br><span class="line">    <span class="comment"># 在容器中ping容器宿主机IP地址</span></span><br><span class="line">    $ ping -c 2 192.168.168.2</span><br><span class="line">    PING 192.168.168.2 (192.168.168.2): 56 data bytes</span><br><span class="line">    64 bytes from 192.168.168.2: <span class="built_in">seq</span>=0 ttl=127 time=0.141 ms</span><br><span class="line">    64 bytes from 192.168.168.2: <span class="built_in">seq</span>=1 ttl=127 time=0.324 ms</span><br><span class="line"></span><br><span class="line">    \--- 192.168.168.2 ping statistics ---</span><br><span class="line">    2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">    round-trip min/avg/max = 0.141/0.232/0.324 ms</span><br><span class="line">    <span class="comment"># 在容器中ping容器宿主机IP地址</span></span><br><span class="line">    $ ping -c 2 192.168.168.202</span><br><span class="line">    PING 192.168.168.201 (192.168.168.202): 56 data bytes</span><br><span class="line">    64 bytes from 192.168.168.202: <span class="built_in">seq</span>=0 ttl=64 time=0.067 ms</span><br><span class="line">    64 bytes from 192.168.168.202: <span class="built_in">seq</span>=1 ttl=64 time=0.070 ms</span><br><span class="line"></span><br><span class="line">    \--- 192.168.168.202 ping statistics ---</span><br><span class="line">    2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">    round-trip min/avg/max = 0.067/0.068/0.070 ms</span><br><span class="line">    <span class="comment"># 在容器中开启httpd服务</span></span><br><span class="line">    $ <span class="built_in">echo</span> <span class="string">&quot;containerd net web test&quot;</span> &gt; /tmp/index.html</span><br><span class="line">    $ httpd -h /tmp</span><br><span class="line">    $ wget -O - -q 127.0.0.1</span><br><span class="line">    containerd net web <span class="built_in">test</span></span><br><span class="line">    $ <span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">    \<span class="comment">#在宿主机访问容器提供的httpd服务</span></span><br><span class="line">    \$ curl 10.66.0.4</span><br><span class="line">    containerd net web <span class="built_in">test</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="5、Containerd数据持久化"><a href="#5、Containerd数据持久化" class="headerlink" title="5、Containerd数据持久化"></a>5、Containerd数据持久化</h3><p>实现把宿主机目录挂载至Containerd容器中，<strong>实现容器数据持久化存储</strong>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个静态容器，实现宿主机目录与容器挂载，src=/tmp 为宿主机目录 dst=/hostdir 为容器中目录</span></span><br><span class="line">$ ctr container create docker.io/library/busybox:latest busybox3 --mount <span class="built_in">type</span>=<span class="built_in">bind</span>,src=/tmp,dst=/hostdir,options=rbind:rw</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行用户进程</span></span><br><span class="line">$ ctr tasks start -d busybox3 bash</span><br><span class="line">$ ctr t <span class="built_in">ls</span></span><br><span class="line">TASK        PID       STATUS    </span><br><span class="line">busybox     39287     RUNNING</span><br><span class="line">busybox3    105003    RUNNING</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入容器，查看是否挂载成功</span></span><br><span class="line">$ ctr tasks <span class="built_in">exec</span> --exec-id <span class="variable">$RANDOM</span> -t busybox3 sh</span><br><span class="line">$ <span class="built_in">ls</span> /hostdir/</span><br><span class="line">ks-script-GSRMK8                                                         vmware-root_6389-1958486695</span><br><span class="line">systemd-private-cc74dd8802f0428490924757e690c750-chronyd.service-9TDICh  vmware-root_6511-1689719547</span><br><span class="line">vmware-root_6359-1949639453                                              yum.log</span><br><span class="line"><span class="comment"># 向容器中挂载目录中添加文件</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;hello world&quot;</span> &gt; /hostdir/test.txt</span><br><span class="line">$ <span class="built_in">ls</span> /hostdir/</span><br><span class="line">ks-script-GSRMK8                                                         vmware-root_6389-1958486695</span><br><span class="line">systemd-private-cc74dd8802f0428490924757e690c750-chronyd.service-9TDICh  vmware-root_6511-1689719547</span><br><span class="line">test.txt                                                                 yum.log</span><br><span class="line">vmware-root_6359-1949639453</span><br><span class="line"><span class="comment"># 在宿主机上查看被容器挂载的目录中是否添加了新的文件，已添加表明被容器挂载成功，并可以读写此目录中内容。</span></span><br><span class="line">$ <span class="built_in">cat</span> /tmp/test.txt </span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h3 id="6、Docker集成Container容器管理"><a href="#6、Docker集成Container容器管理" class="headerlink" title="6、Docker集成Container容器管理"></a>6、Docker集成Container容器管理</h3><p>目前Containerd主要任务还在于解决容器运行时的问题，对于其周边生态还不完善，所以有时需要借助：Docker结合Containerd来实现Docker完整的功能应用。</p><p>Docker安装与使用教程：<a href="https://blog.csdn.net/qq_41822345/article/details/107123094" title="https://blog.csdn.net/qq_41822345/article/details/107123094">https://blog.csdn.net/qq_41822345&#x2F;article&#x2F;details&#x2F;107123094</a></p><p>docker运行的容器默认在moby命名空间下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装docker</span></span><br><span class="line">$ yum install docker</span><br><span class="line"><span class="comment"># 启动docker</span></span><br><span class="line">$ systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行一个容器</span></span><br><span class="line">$ docker run -d nginx:latest</span><br><span class="line">$ docker ps</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看ctr命令下的namespace，发现多了一个moby命名空间。moby即为docker使用的命名空间。</span></span><br><span class="line">$ ctr namespace <span class="built_in">ls</span></span><br><span class="line">NAME    LABELS </span><br><span class="line">default        </span><br><span class="line">k8s.io         </span><br><span class="line">moby</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看moby命名空间下的容器和任务</span></span><br><span class="line">$ ctr -n moby container <span class="built_in">ls</span></span><br><span class="line">$ ctr -n moby tasks <span class="built_in">ls</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">containerd</summary>
    
    
    
    <category term="containerd" scheme="https://blog.yongwang.lu/categories/containerd/"/>
    
    
    <category term="containerd" scheme="https://blog.yongwang.lu/tags/containerd/"/>
    
  </entry>
  
</feed>
